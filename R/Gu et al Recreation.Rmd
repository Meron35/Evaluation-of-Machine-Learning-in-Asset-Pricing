---
title: "Simulation"
author: "Ze Yu Zhong"
date: "21 March 2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
################
##Load Libraries
################

library(glmnet)
library(tidyverse)
library(keras)
library(ggplot2)
library(caret)
library(forecast)
library(rlist)
library(plm)
library(Metrics)
library(ranger)

set.seed(27935248)
```

```{r global_options}
#####################################
##Simulation
#####################################

#Number of stocks
N <- 20
#Number of characteristics that underly true model
P_c <- 50
#Number of Periods
Time <- 180

```



```{r function_simulate_characteristics}
###################
##characteristics C_bar
###################

#######
##Function to Generate C_bar
#######

gen_C_bar <- function(){
  #empty matrix
  elm <- matrix(
    data = 0, nrow = N, ncol = P_c
  )
  
  C_bar <- rep(list(elm), Time + 2)
  
  for (t in 1:(Time+1)) {
    for (j in 1:P_c) {
      #Gu et al set rho to runif 0.9, 1
      rho <- runif(1, 0.9, 1)
      #Gu et al set error term here to be standard normal
      C_bar[[t+1]][, j] <- (C_bar[[t]][, j]*rho + rnorm(N, 0, 1))
    }  
  }
  
  ##Delete first period full of zeroes
  C_bar[[1]] <- NULL
  
  return(C_bar)
}

##################################
##Generate final "observed" C
##################################

#Remember you need to generate C_bar and C_hat first
#This function "observes" characteristics by normalizing them within (-1, 1) via the rank transformation

gen_C <- function(C_matrix){
  elm <- matrix(
    data = 0, nrow = N, ncol = P_c
  )
  
  C <- rep(list(elm), Time+1)

  for (t in 1:(Time+1)) {
    C[[t]] <- (2/(N*P_c+1))*
      matrix(rank(C_matrix[[t]]), nrow = N, ncol = P_c) - matrix(
        data = 1, nrow = N, ncol = P_c
        )
  }
  
  return(C)
}
```

```{r generate_characteristics}
C_bar <- gen_C_bar()
#C_hat <- gen_C_hat()
#Gu et al do NOT build in cross sectional correlation
C <- gen_C(C_bar)
```

```{r gu_gen_xt}

###########################################
## GU ET AL UNIVARIATE SPECIFICATION
###########################################
gen_gu_xt <- function(){
  xt <- c(0:Time)
  p <- 0.95
  
  for (i in 1:(Time+1)) {
  #Gu et al set these to N(0, 1-p^2); p^2 = 0.95
    ut <- rnorm(1, mean = 0, sd = (1-p^2))
    xt[1+i] <- p*xt[i] + ut
  }
  #Remove Initial
  xt <- xt[-1]
  return(xt)
}

#Test, working
gu_xt <- gen_gu_xt()

acf(gu_xt)

```

```{r gu_g_functions}

#######################
#g1
#######################

#Default theta, 1x3 dimensions
theta <- matrix(c(0.02, 0.02, 0.02), nrow = 1)

gu_g1 <- function(C, x, i, t, theta){
  
  matrix(c(
    C[[t]][i,1], C[[t]][i,2], C[[t]][i,3] * x[t]
    ), nrow = 1) %*% t(theta)
}

#test if working, working
gu_g1(C, gu_xt, 1, 1, theta)

#######################
#g2
#######################

#Default theta, 1x3 dimensions
theta <- matrix(c(0.04, 0.035, 0.01), nrow = 1)

gu_g2 <- function(C, x, i, t, theta){
  matrix(c(
    C[[t]][i, 1]^2, 
    C[[t]][i, 1]*C[[t]][i, 2],
    sign(
      C[[t]][i, 3] * x[t]
      )
  ), nrow = 1) %*% t(theta)
}

#test if working
gu_g2(C, gu_xt, 1, 1, theta)

#Both Working, Hooray

```


```{r function_return_equation}
##########################
##Functions for elements in return equation
##########################

###########
##Function to generate Beta, i,t
###########

#Beta is the the first 3 rows of the specified characteristics vector

gen_Beta <- function(C){
  
  #Empty Matrix Init
  eln <- matrix(0, nrow = N, ncol = 3)

  Beta <- rep(list(eln), Time+1)
  
  for (t in 1:(Time+1)) {
    Beta[[t]] <- C[[t]][, 1:3]
  }
  return(Beta)
}

#Check
Beta <- gen_Beta(C)

###
##v error term
###
#This is is trivariate N(0, 0.05^2) series, constant across all stocks

v_sd <- 0.05

gu_gen_v <- function(v_sd){
  
  #Empty Matrix Init
  elo <- matrix(0, nrow = 3, ncol = 1)
  
  v <- rep(list(elo), Time+1)
  
  for (t in 1:(Time+1)) {
    v[[t]] <- matrix(
      data = rnorm(3, 0, v_sd),
      nrow = 3, ncol = 1
    )
  }
  return(v)
}

#No SV error strucure for Gu et al, yay!

###
##Epsilon Term is N x 1 vector of idiosyncatic errors, distributed at students t with 5 dof (0, 0.05^2)
###

ep_sd <- 0.05

gu_gen_error <- function(ep_sd){
  
  error <- rep(
    list(
      matrix(
        0, nrow = N, ncol = 1
      )
    ), Time+1
  )
  
  for (t in 1:Time+1){
    for (i in 1:N) {
      #This is how you build in a specified variance for student t
      error[[t]][i] <- rt(1, 5)*sqrt(ep_sd^2 * (5-2)/5)
    }
  }
  
  return(error)
}
```


```{r gu_function_generate_return}
############################################################################
##Function to generate return series + its residuals given specification of g function
############################################################################

gu_gen_rt_resid <- function(v_sd, ep_sd){
  elp <- matrix(0, nrow = N, ncol = 1)
  
  resid <- rep(list(elp), Time+1)
  
  ##############################
  Beta <- gen_Beta(C)
  v <- gu_gen_v(v_sd)
  error <- gu_gen_error(ep_sd)
  ##############################
  
  for (t in 1:(Time)) {
    for (i in 1:N){
      resid[[t+1]][i] <- Beta[[t+1]][i, ] %*% v[[t+1]] + error[[t+1]][i]
    }
  }
  return(resid)
}

gu_gen_g <- function(g){
  elp <- matrix(0, nrow = N, ncol = 1)
  
  rt <- rep(list(elp), Time+1)
  
  #########################
  gu_xt <- gen_gu_xt()
  #########################
  
  for (t in 1:(Time)) {
    for (i in 1:N){
      rt[[t+1]][i] <- g(C, gu_xt, i, t, theta)
    }
  }
  
  return(rt)
}

```

```{r gu_tune_rsquared_skeleton, eval = FALSE}
#Tuning Cross Sectional R Squared SKELETON CODE

#We want inidividual r squared for each stock to be 50%
#We want inidividual annualized volatility to be 30%
#We want to cross sectional R squared to be 25%
#We want the predictive R squared to be 5%

#Generate paramaters first

#Tune v_sd for inidividual r square first
#Gu et al had 0.05 for their v sd
v_sd <- 0.05

ep_sd <- 0.05

#Tune theta for cross sectional r square
#theta

theta <- matrix(c(0.02, 0.02, 0.02), nrow = 1)

resid <- gu_gen_rt_resid(v_sd, ep_sd)

g <- gu_gen_g(gu_g1)

rt_cross_tune_panel <- data.frame(
  g = rep(0, N*Time),
  resid = rep(0, N*Time),
  time = rep(0, N*Time),
  stock = rep(0, N*Time)
)

for (i in 1:N){

  rt_cross_tune_df <- data.frame(
    resid = rep(0, Time+1),
    g = rep(0, Time+1),
    time = rep(0, Time+1),
    stock = rep(i, Time+1)
  )
  
  for (t in 1:(Time+1)){
    rt_cross_tune_df$resid[t] <- resid[[t]][i]
    rt_cross_tune_df$g[t] <- g[[t]][i]
    rt_cross_tune_df$time <- t
  }
  
  #Cbind returns, id, time and predictors
  rt_cross_tune_panel[((i-1)*(Time+1)+1):(i*(Time+1)), ] <- rt_cross_tune_df
  
}
  
#Remove 1st row because returns data only starts from t = 2
rt_cross_tune_panel <- rt_cross_tune_panel %>%
  filter(time != 1) %>%
  select(-time) %>%
  mutate(return = resid + g)

#Individual return R squared

rt_id_tune_df <- data.frame(
  stock = c(1:N),
  rsquare = c(1:N)
)

for (i in 1:N) {
  rt_id_tune_panel <- rt_cross_tune_panel %>%
    filter(stock == i)
  SSR <- t(rt_id_tune_panel$return - rt_id_tune_panel$g) %*% (rt_id_tune_panel$return - rt_id_tune_panel$g)
  SST <- t(rt_id_tune_panel$return - mean(rt_id_tune_panel$return)) %*% (rt_id_tune_panel$return - mean(rt_id_tune_panel$return))
  #SST <- t(rt_id_tune_panel$return) %*% (rt_id_tune_panel$return)
  rt_id_tune_df$rsquare[i] <- 1 - SSR/SST
}

#Sanity check
summary((rt_id_tune_panel$return - rt_id_tune_panel$g) - rt_id_tune_panel$resid)
#Makes sure that the right returns, residuals etc are lined up correctly
#Looks good (not exactly zero due to rounding)

#Return mean of each individual rsquared
mean(rt_id_tune_df$rsquare)

#Calculate Annualized Volatility
#Annualized volatility = calculate volatility (via standard deviation) for the monthly returns, then annualize it by multiplying it by sqrt(12) for 12 months in a year

vol_tune_df <- data.frame(
  stock = c(1:N),
  annual_vol = c(1:N)
)

for (i in 1:N) {
  vol_tune_panel <- rt_cross_tune_panel %>%
    filter(stock == i)
  vol_tune_df$annual_vol[i] <- sd(vol_tune_panel$return) * sqrt(12)
}

#Return the mean annualized volatility
#Want this aorund 30%
mean(vol_tune_df$annual_vol)

#Cross sectional r squared

SSR <- t(rt_cross_tune_panel$resid) %*% (rt_cross_tune_panel$resid)
SST <- t(rt_cross_tune_panel$return - mean(rt_cross_tune_panel$return)) %*% (rt_cross_tune_panel$return - mean(rt_cross_tune_panel$return))

#SST <- t(rt_cross_tune_panel$return) %*% (rt_cross_tune_panel$return)

1 - SSR/SST

#This is quite off from Gu et al's specification and results. NOT GOOD
```

```{r tune_rsquared_skeleton}
#Same as before, but now with grid set up

#Tuning Cross Sectional R Squared SKELETON CODE

#We want inidividual r squared for each stock to be 50%
#We want inidividual annualized volatility to be 30%
#We want to cross sectional R squared to be 25%
#We want the predictive R squared to be 5%

#Big Problem, errors are random so individual r squared values are highly unstable

#Set up Grids

individual_rsquare_grid <- expand.grid(
  v_sd = seq(0.01, 0.05, 0.01),
  omega = -0.736,
  gamma = 0.9,
  w = sqrt(0.363)
)

individual_rsquare_grid <- cbind(individual_rsquare_grid, rsquare = rep(0, nrow(individual_rsquare_grid)), vol = rep(0, nrow(individual_rsquare_grid)))

#Generate paramaters first

###START ACTUAL CACULATIONS

for (grid in 1:nrow(individual_rsquare_grid)) {
  
resid <- gen_rt_resid(
  omega = individual_rsquare_grid$omega[grid], 
  gamma = individual_rsquare_grid$gamma[grid], 
  w = individual_rsquare_grid$w[grid], 
  v_sd = individual_rsquare_grid$v_sd[grid]
)

#Tune theta for cross sectional r square
#theta

theta <- matrix(c(0.02, 0.02, 0.02), nrow = 1)

rt <- gen_rt(A1, g1, resid)

rt_cross_tune_panel <- data.frame(
  return = rep(0, 36000),
  resid = rep(0, 36000),
  time = rep(0, 36000),
  stock = rep(0, 36000)
)

for (i in 1:N){

  rt_cross_tune_df <- data.frame(
    return = rep(0, Time+1),
    resid = rep(0, Time+1),
    time = rep(0, Time+1)
  )
  
  for (t in 1:(Time+1)){
    rt_cross_tune_df$return[t] <- rt[[t]][i]
    rt_cross_tune_df$resid[t] <- resid[[t]][i]
  }
  
  #Cbind returns, id, time and predictors
  panel <- cbind(rt_cross_tune_df$return, rt_cross_tune_df$resid, c(1:(Time+1)), c(rep(i, (Time+1))))
  rt_cross_tune_panel[((i-1)*(Time+1)+1):(i*(Time+1)), ] <- panel
  
}
  
#Sort by time
rt_cross_tune_panel <- rt_cross_tune_panel %>%
  arrange(time)
  
#Remove 1st row because returns data only starts from t = 2
rt_cross_tune_panel <- rt_cross_tune_panel %>%
  filter(time != 1) %>%
  select(-time)

#Individual return R squared

rt_id_tune_df <- data.frame(
  stock = c(1:200),
  rsquare = c(1:200)
)

for (i in 1:200) {
  rt_id_tune_panel <- rt_cross_tune_panel %>%
    filter(stock == i)
  SSR <- t(rt_id_tune_panel$resid) %*% rt_id_tune_panel$resid
  SST <- t(rt_id_tune_panel$return - mean(rt_id_tune_panel$return)) %*% (rt_id_tune_panel$return - mean(rt_id_tune_panel$return))
  rt_id_tune_df$rsquare[i] <- 1 - SSR/SST
}

#Return mean of each individual rsquared
individual_rsquare_grid$rsquare[grid] <-  mean(rt_id_tune_df$rsquare)

#Calculate Annualized Volatility
#Annualized volatility = calculate volatility (via standard deviation) for the monthly returns, then annualize it by multiplying it by sqrt(12) for 12 months in a year

vol_tune_df <- data.frame(
  stock = c(1:200),
  annual_vol = c(1:200)
)

for (i in 1:200) {
  vol_tune_panel <- rt_cross_tune_panel %>%
    filter(stock == i)
  vol_tune_df$annual_vol[i] <- sd(vol_tune_panel$return) * sqrt(12)
}

#Return the mean annualized volatility
#Want this aorund 30%

individual_rsquare_grid$vol[grid] <- mean(vol_tune_df$annual_vol)

#Cross sectional r squared

SSR <- t(rt_cross_tune_panel$resid) %*% (rt_cross_tune_panel$resid)
SST <- t(rt_cross_tune_panel$return - mean(rt_cross_tune_panel$return)) %*% (rt_cross_tune_panel$return - mean(rt_cross_tune_panel$return))

1 - SSR/SST
  
}



```


```{r generate_returns}
##############################
##Generate Return Series
##############################

##############################
##g1
##############################

rt_A1_g1 <- gen_rt(A1, g1, gen_rt_resid(omega, gamma, w))
#rt_A2_g1 <- gen_rt(A2, g1)
#rt_A3_g1 <- gen_rt(A3, g1)

##############################
##g2
##############################

#rt_A1_g2 <- gen_rt(A1, g2)
#rt_A2_g2 <- gen_rt(A2, g2)
#rt_A3_g2 <- gen_rt(A3, g2)

##############################
##g3
##############################

#rt_A1_g3 <- gen_rt(A1, g3)
#rt_A2_g3 <- gen_rt(A2, g3)
#rt_A3_g3 <- gen_rt(A3, g3)


##############################
##g4
##############################

#rt_A1_g4 <- gen_rt(A1, g4)
#rt_A2_g4 <- gen_rt(A2, g4)
#rt_A3_g4 <- gen_rt(A3, g4)
```

```{r function_build_predictor}
##############################################################################
##Function to build predictor set for each i individual stock, and each time t
##############################################################################

###Note that the format of the list/matrix is different from previously so that it can be fed into algorithms more easily
###Instead of t denoting the list, it is now the row number of the matrix containing all predictors
###This should make it very easy to reconcile with the return series

#Our P_x is 1 constant + 3 multivariate time series = 4
#Hence, ncol = 4*P_c

gen_z <- function(){
  null_matrix <- matrix(0, nrow = Time+1, ncol = 4*P_c)

  z <- rep(list(null_matrix), N)
  
  xt <- gen_xt(A1)
  
  for (i in 1:N) {
    for (t in 1:(Time+1)){
      z[[i]][t, ] <- t(
        kronecker(t(cbind(c(rep(1, Time+1)), xt)[t, ]), C[[t]][i, ])
        )
    }
  }
  return(z)
}

############################################################################
##Function to cbind the corresponding return series and the predictor set together, and return it in dataframe form
############################################################################

gen_pooled_panel <- function(){
  
  #Initialize entire dataset dimensions
  #There should be 181*200 = 36,200 rows
  #There should be 1 return column + 1 id column + 1 time column + 400 predictors = 403 columns
  
  pooled_panel <- matrix(data = 0, nrow = (181*200), ncol = (1+1+1+400))
  
  for (i in 1:N){
    #Extract return series for ith stock from the return series list first
    #Initialize empty matrix
    r <- matrix(0, nrow = Time+1, ncol = 1)
    
    for (t in 1:(Time+1)){
        r[t] <- rt_A1_g1[[t]][i]
    }
    
    #Cbind returns, id, time and predictors
    panel <- cbind(r, c(rep(i, (Time+1))), c(1:(Time+1)), z[[i]])
    pooled_panel[((i-1)*(Time+1)+1):(i*(Time+1)), ] <- panel
  }
  
  #Pass this through to data frame format with column titles so you know which column is which
  pooled_panel <- data.frame(return = pooled_panel[, 1], stock = pooled_panel[, 2], time = pooled_panel[, 3], pooled_panel[, 4:403])
  
  #Sort by time
  pooled_panel <- pooled_panel %>%
  arrange(time)
  
  #Remove 1st row because returns data only starts from t = 2
  pooled_panel <- pooled_panel %>%
    filter(time != 1)
  
  #Return
  return(pooled_panel)
  
}
```

```{r geenrate_panel_dataset}
z <- gen_z()

pooled_panel <- gen_pooled_panel()

pooled_panel_y <- pooled_panel$return

pooled_panel_x <- pooled_panel %>%
  select(-return, -stock, -time)


```

```{r train_valid_test}
#Create Training + Test Sets

#$T = 180$ monthly periods corresponds to 15 years. The training sample was set to start from $T = 108$ or 9 years, a validation set 1 year in length. The last 3 years were reserved as a test set never to be used for validation or training.

#This means that there are only 3 sample periods to train on, yay

#Caret and other packages are not so good at dealing with custom training/validation/test sets. Have to specify resampling, tuning and testing manually (ugh)

time_slices <- createTimeSlices(1:(Time-36), initialWindow = 108, horizon = 12, fixedWindow = FALSE, skip = 11)

true_test <- list(c(121:180), c(133:180), c(145:180))

time_slices <- list.append(time_slices, true_test)

names(time_slices) <- c("train", "validation", "test")

```

```{r pooled_ols}
#POLS

#Pooled OLS does not have anything hyperparameters to tune, easy example to start off with

#Expand formula manually this way because PLM is dumb and gets confused by y ~ .

plm_formula <- function(panel){
  colnames <- colnames(pooled_panel)
  colnames <- colnames[-c(1:3)]
  f <- as.formula(c("return ~ ", paste(colnames, collapse = "+")))
  return(f)
}

f <- plm_formula(pooled_panel)

#Initialize Loss Function Statistics

POLS_stats <- data.frame(
  set = c(1:3),
  #Training
  train_MAE = c(1:3),
  train_MSE = c(1:3),
  train_RMSE = c(1:3),
  train_R2 = c(1:3),
  #Validation
  valid_MAE = c(1:3),
  valid_MSE = c(1:3),
  valid_RMSE = c(1:3),
  valid_R2 = c(1:3),
  #Test
  test_MAE = c(1:3),
  test_MSE = c(1:3),
  test_RMSE = c(1:3),
  test_R2 = c(1:3)
)

#Initialize Forecasts

POLS_forecasts <- rep(list(0), 3)

#Initialize Models

POLS_models <- rep(list(0), 3)

for (set in 1:3) {
  #Load training, validation and test sets
    train <- pooled_panel %>%
      filter(time %in% unlist(time_slices$train[set]))
    validation <- pooled_panel %>%
      filter(time %in% unlist(time_slices$validation[set]))
    test <- pooled_panel %>%
      filter(time %in% unlist(time_slices$test[set]))
    
  #Initialize forecasts
    POLS_forecasts[[set]] <- data.frame(
      time = test$time,
      forecast = rep(0, length(test$time)),
      forecast_error = rep(0, length(test$time))
    )

  #Train model on training set
  
    pols <- plm(f, data = train, model = "pooling", index = c("time", "stock"))
    POLS_models[[set]] <- pols
    
  #Usually Tuning via Validation set would be here, but POLS does not need tuning
  
  #Training Set Statistics
    POLS_stats$train_MAE[set] <- mae(train$return, predict(pols))
    POLS_stats$train_MSE[set] <- mse(train$return, predict(pols))
    POLS_stats$train_RMSE[set] <- rmse(train$return, predict(pols))
    error <- (train$return - predict(pols))
    SSR <- t(error) %*% (error)
    SST <- t(train$return - mean(train$return)) %*% (train$return - mean(train$return))
    POLS_stats$train_R2[set] <- (1 - SSR/SST)
    
  #Validation Set Statistics
    POLS_stats$valid_MAE[set] <- mae(validation$return, predict(pols, newdata = validation))
    POLS_stats$valid_MSE[set] <- mse(validation$return, predict(pols, newdata = validation))
    POLS_stats$valid_RMSE[set] <- rmse(validation$return, predict(pols, newdata = validation))
    
    error <- (validation$return - predict(pols, newdata = validation))
    SSR <- t(error) %*% (error)
    SST <- t(validation$return - mean(validation$return)) %*% (validation$return - mean(validation$return))
    POLS_stats$valid_R2[set] <- (1 - SSR/SST)
  
  #Test Set Statistics
    POLS_stats$test_MAE[set] <- mae(test$return, predict(pols, newdata = test))
    POLS_stats$test_MSE[set] <- mse(test$return, predict(pols, newdata = test))
    POLS_stats$test_RMSE[set] <- rmse(test$return, predict(pols, newdata = test))
    
    error <- (test$return - predict(pols, newdata = test))
    SSR <- t(error) %*% (error)
    SST <- t(test$return - mean(test$return)) %*% (test$return - mean(test$return))
    POLS_stats$test_R2[set] <- (1 - SSR/SST)
    
  #Forecasts
    POLS_forecasts[[set]]$forecast <- predict(pols, newdata = test)
    POLS_forecasts[[set]]$forecast_error <- test$return - predict(pols, newdata = test)
}

POLS_stats

summary(POLS_models[[3]])


```

```{r elastic_net}
#Elasticnet

#Elasticnet only has alpha to tune, thank god

elasticnet_grid <- expand.grid(
  alpha = seq(0, 1, 0.01)
)

#Initialize Loss Function Statistics

elasticnet_stats <- data.frame(
  set = c(1:3),
  #Training
  train_MAE = c(1:3),
  train_MSE = c(1:3),
  train_RMSE = c(1:3),
  train_R2 = c(1:3),
  #Validation
  valid_MAE = c(1:3),
  valid_MSE = c(1:3),
  valid_RMSE = c(1:3),
  valid_R2 = c(1:3),
  #Test
  test_MAE = c(1:3),
  test_MSE = c(1:3),
  test_RMSE = c(1:3),
  test_R2 = c(1:3)
)

#Initialize Forecasts

elasticnet_forecasts <- rep(list(0), 3)

#Initialize Models

elasticnet_models <- rep(list(0), 3)

for (set in 1:3) {
  #Load training, validation and test sets
    train <- pooled_panel %>%
      filter(time %in% unlist(time_slices$train[set]))
    train_x <- train %>%
      select(-return, -time, -stock)
    train_y <- train %>%
      select(return)
    
    validation <- pooled_panel %>%
      filter(time %in% unlist(time_slices$validation[set]))
    validation_x <- validation %>%
      select(-return, -time, -stock)
    validation_y <- validation %>%
      select(return)
    
    test <- pooled_panel %>%
      filter(time %in% unlist(time_slices$test[set]))
    test_x <- test %>%
      select(-return, -time, -stock)
    test_y <- test %>%
      select(return)
    
  #Initialize forecasts
    elasticnet_forecasts[[set]] <- data.frame(
      time = test$time,
      forecast = rep(0, length(test$time)),
      forecast_error = rep(0, length(test$time))
    )

  #Train model on training set
  
    elasticnet <- glmnet(x = train_x, y = train_y)
    POLS_models[[set]] <- pols
    
  #Usually Tuning via Validation set would be here, but POLS does not need tuning
  
  #Training Set Statistics
    POLS_stats$train_MAE[set] <- mae(train$return, predict(pols))
    POLS_stats$train_MSE[set] <- mse(train$return, predict(pols))
    POLS_stats$train_RMSE[set] <- rmse(train$return, predict(pols))
    error <- (train$return - predict(pols))
    SSR <- t(error) %*% (error)
    SST <- t(train$return - mean(train$return)) %*% (train$return - mean(train$return))
    POLS_stats$train_R2[set] <- (1 - SSR/SST)
    
  #Validation Set Statistics
    POLS_stats$valid_MAE[set] <- mae(validation$return, predict(pols, newdata = validation))
    POLS_stats$valid_MSE[set] <- mse(validation$return, predict(pols, newdata = validation))
    POLS_stats$valid_RMSE[set] <- rmse(validation$return, predict(pols, newdata = validation))
    
    error <- (validation$return - predict(pols, newdata = validation))
    SSR <- t(error) %*% (error)
    SST <- t(validation$return - mean(validation$return)) %*% (validation$return - mean(validation$return))
    POLS_stats$valid_R2[set] <- (1 - SSR/SST)
  
  #Test Set Statistics
    POLS_stats$test_MAE[set] <- mae(test$return, predict(pols, newdata = test))
    POLS_stats$test_MSE[set] <- mse(test$return, predict(pols, newdata = test))
    POLS_stats$test_RMSE[set] <- rmse(test$return, predict(pols, newdata = test))
    
    error <- (test$return - predict(pols, newdata = test))
    SSR <- t(error) %*% (error)
    SST <- t(test$return - mean(test$return)) %*% (test$return - mean(test$return))
    POLS_stats$test_R2[set] <- (1 - SSR/SST)
    
  #Forecasts
    POLS_forecasts[[set]]$forecast <- predict(pols, newdata = test)
    POLS_forecasts[[set]]$forecast_error <- test$return - predict(pols, newdata = test)
}

```

```{r random_forest}
#Random Forest

#Random Forest should also be fairly straightforward, only two tuning parameters mtry and ntree (usually)

#The vanilla randomForest package is very basic and too slow
#We will use the ranger package, which is apparently a C++ implementation of the random forest algorithm and should be faster
#Another alternative is to use XGBoost (which is very very fast) and set boosting rounds = 1, because in this case it should be a standard random forest

#COmpute power is a real struggle, and the aim is just to get the code basis working for the moment. Very conservative grids are therefore used for now

#Default ntree = 500
#Default mtry = count(predictors/3) = 400/3 = 133

random_forest_grid <- expand.grid(
  ntree = seq(10, 50, 10),
  mtry = seq(10, 100, 10)
)

#Generate formula, actually same code from PLM earlier

rf_formula <- function(panel){
  colnames <- colnames(pooled_panel)
  colnames <- colnames[-c(1:3)]
  f <- as.formula(c("return ~ ", paste(colnames, collapse = "+")))
  return(f)
}

f <- rf_formula(pooled_panel)

#Initialize Loss Function Statistics

RF_stats <- data.frame(
  set = c(1:3),
  #Training
  train_MAE = c(1:3),
  train_MSE = c(1:3),
  train_RMSE = c(1:3),
  train_R2 = c(1:3),
  #Validation
  valid_MAE = c(1:3),
  valid_MSE = c(1:3),
  valid_RMSE = c(1:3),
  valid_R2 = c(1:3),
  #Test
  test_MAE = c(1:3),
  test_MSE = c(1:3),
  test_RMSE = c(1:3),
  test_R2 = c(1:3)
)

#Initialize Forecasts

RF_forecasts <- rep(list(0), 3)

#Initialize Models

RF_models <- rep(list(0), 3)

for (set in 1:3) {
  #Load training, validation and test sets
    train <- pooled_panel %>%
      filter(time %in% unlist(time_slices$train[set]))
    validation <- pooled_panel %>%
      filter(time %in% unlist(time_slices$validation[set]))
    test <- pooled_panel %>%
      filter(time %in% unlist(time_slices$test[set]))
    
  #Initialize forecasts
    RF_forecasts[[set]] <- data.frame(
      time = test$time,
      forecast = rep(0, length(test$time)),
      forecast_error = rep(0, length(test$time))
    )

  #Train model on training set
  
    rf <- ranger(f, data = train, model = "pooling", index = c("stock", "time"))
    RF_models[[set]] <- rf
    
  #Usually Tuning via Validation set would be here, but POLS does not need tuning
  
  #Training Set Statistics
    RF_stats$train_MAE[set] <- mae(train$return, predict(rf))
    RF_stats$train_MSE[set] <- mse(train$return, predict(rf))
    RF_stats$train_RMSE[set] <- rmse(train$return, predict(rf))
    #R squared still pending, haven't figured out how to do it yet fully
    
  #Validation Set Statistics
    RF_stats$valid_MAE[set] <- mae(validation$return, predict(rf, newdata = validation))
    RF_stats$valid_MSE[set] <- mse(validation$return, predict(rf, newdata = validation))
    RF_stats$valid_RMSE[set] <- rmse(validation$return, predict(rf, newdata = validation))
    #R squared still pending, haven't figured out how to do it yet fully
  
  #Test Set Statistics
    RF_stats$test_MAE[set] <- mae(test$return, predict(rf, newdata = test))
    RF_stats$test_MSE[set] <- mse(test$return, predict(rf, newdata = test))
    RF_stats$test_RMSE[set] <- rmse(test$return, predict(rf, newdata = test))
    #R squared still pending, haven't figured out how to do it yet fully
    
  #Forecasts
    RF_forecasts[[set]]$forecast <- predict(rf, newdata = test)
    RF_forecasts[[set]]$forecast_error <- test$return - predict(rf, newdata = test)
    
}

```

```{r random_forest_tune}
#Train RF Model + tune
rf <- ranger(f, data = train, 
             #Hyperparameters
             
             #Tuning parameters
              num.trees = 10,
              mtry = 10,
             
             #No Tuning
              #Default Min node size
              min.node.size = 5,
             
             #Other
             write.forest = TRUE,
             importance = "impurity"
             )

importance(rf)

```


```{r neural_networks}
#Neural Network

```

