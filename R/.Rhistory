library(keras)
library(ggplot2)
library(forecast)
library(rlist)
library(Metrics)
library(tensorflow)
library(quantreg)
library(randomForestSRC)
library(hqreg)
library(caret)
library(tsfeatures)
## Vector arima/garch packages
library(kernlab)
library(xts)
library(tsne)
library(rmgarch)
library(marima)
library(xgboost)
#Parallel Computing
library(foreach)
library(doFuture)
#Registering
registerDoFuture()
plan(multisession)
error_softmax_obj <- function(preds, dtrain) {
labels <- xgboost::getinfo(dtrain, "label")
errors <- attr(dtrain, "errors")
preds <- exp(preds)
sp <- rowSums(preds)
preds <- preds / replicate(ncol(preds), sp)
rowsumerrors <- replicate(ncol(preds), rowSums(preds * errors))
grad <- preds*(errors - rowsumerrors)
hess <- errors*preds*(1.0-preds) - grad*preds
#hess <- grad*(1.0 - 2.0*preds)
#hess <- pmax(hess, 1e-16)
#the true hessian should be grad*(1.0 - 2.0*preds) but it produces numerical problems
#what we use here is a upper bound
#print(mean(rowSums(preds*errors)))
return(list(grad = t(grad), hess = t(hess)))
}
param <- list(booster = "gbtree",
max_depth = 6, eta = 0.3, nthread = 12,
num_class = ncol(errors),
objective = error_softmax_obj,
subsample = 1,
colsample_bytree = 1)
dtrain <- xgb.DMatrix(data = as.matrix(train_list$data),
label = train_list$labels[, 1])
attr(dtrain, "errors") <- train_list$errors
bst <- xgboost::xgb.train(param, dtrain, 94,
verbose = 1)
softmax_transform <- function(x) {
exp(x) / sum(exp(x))
}
error_softmax_obj <- function(preds, dtrain) {
labels <- xgboost::getinfo(dtrain, "label")
errors <- attr(dtrain, "errors")
preds <- exp(preds)
sp <- rowSums(preds)
preds <- preds / replicate(ncol(preds), sp)
rowsumerrors <- replicate(ncol(preds), rowSums(preds * errors))
grad <- preds*(errors - rowsumerrors)
hess <- errors*preds*(1.0-preds) - grad*preds
#hess <- grad*(1.0 - 2.0*preds)
#hess <- pmax(hess, 1e-16)
#the true hessian should be grad*(1.0 - 2.0*preds) but it produces numerical problems
#what we use here is a upper bound
#print(mean(rowSums(preds*errors)))
return(list(grad = t(grad), hess = t(hess)))
}
param <- list(booster = "gbtree",
max_depth = 6, eta = 0.3, nthread = 12,
num_class = ncol(errors),
objective = error_softmax_obj,
subsample = 1,
colsample_bytree = 1)
dtrain <- xgb.DMatrix(data = as.matrix(train_list$data),
label = train_list$labels[, 1])
attr(dtrain, "errors") <- train_list$errors
bst <- xgboost::xgb.train(param, dtrain, 94,
verbose = 1)
matrix(1, 2, 2)
x <- matrix(1, 2, 2)
exp(x)
?replicate
x*x
x <- matrix(2, 2, 2)
x*x
################
##Load Libraries
################
library(speedglm)
library(tidyverse)
library(keras)
library(ggplot2)
library(forecast)
library(rlist)
library(Metrics)
library(tensorflow)
library(quantreg)
library(randomForestSRC)
library(hqreg)
library(caret)
library(tsfeatures)
## Vector arima/garch packages
library(kernlab)
library(xts)
library(tsne)
library(rmgarch)
library(marima)
library(xgboost)
#Parallel Computing
library(foreach)
library(doFuture)
#Registering
registerDoFuture()
plan(multisession)
g1_A1_sv_0.1 <- readRDS("~/GitHub/Evaluation-of-Machine-Learning-in-Asset-Pricing/R/Simulated_dataset/g1_A1_sv_0.1.RDS")
pooled_panel <- g1_A1_sv_0.1[[1]]$panel
fforma_features <- function(time_series) {
cbind(
# Series length
length = length(time_series),
# nperiods, seasonal_period, trend, seasonality, linearity, curvature, spikiness e_acr1 and e_acf10
tsfeatures(time_series, features = "stl_features"),
# Stability
tsfeatures(time_series, features = "stability"),
# Lumpiness
tsfeatures(time_series, features = "lumpiness"),
# Hurst
tsfeatures(time_series, features = "hurst"),
# Nonlinearity
tsfeatures(time_series, features = "nonlinearity"),
# Holt alpha and beta
tsfeatures(time_series, features = "holt_parameters"),
# HW alpha and beta, only applies for seasonal time series
#tsfeatures(time_series, features = "hw_parameters"),
# Unitroot statistics
tsfeatures(time_series, features = c("unitroot_kpss", "unitroot_pp")),
# ACF features
tsfeatures(time_series, features = "acf_features"),
# PACF features
tsfeatures(time_series, features = "pacf_features"),
# Crossing point
tsfeatures(time_series, features = "crossing_points"),
# flat_spots
tsfeatures(time_series, features = "flat_spots"),
# arch_lm
tsfeatures(time_series, features = "arch_stat"),
# Heterogeneity
tsfeatures(time_series, features = "heterogeneity")
)
}
### Constituent models, and their corresponding losses over the test set
## Note that we can specify any loss function we want
## Going ahead with mse for now
# Naive
naive_model <- naive(univariate_time_series, h = 12)
mse(as.vector(univariate_time_series_test), predict(naive_model)$mean)
# Random walk with drift
random_walk_model <- rwf(univariate_time_series, h = 12, drift = TRUE)
mse(as.vector(univariate_time_series_test), predict(random_walk_model)$mean)
# Seasonal Naive, this ends up being the same as naive
seasonal_naive_model <- snaive(univariate_time_series, h = 12)
mse(as.vector(univariate_time_series_test), predict(seasonal_naive_model)$mean)
# Theta Method
theta_model <- thetaf(univariate_time_series, h = 12)
mse(as.vector(univariate_time_series_test), predict(theta_model)$mean)
## Auto arima
auto_arima_model <- auto.arima(univariate_time_series)
mse(as.vector(univariate_time_series_test), forecast(auto_arima_model, h = 12)$mean)
# Auto ETS
auto_ets_model <- ets(univariate_time_series)
mse(as.vector(univariate_time_series_test), forecast(auto_ets_model, h = 12)$mean)
# TBATS
tbats_model <- tbats(univariate_time_series)
mse(as.vector(univariate_time_series_test), forecast(tbats_model, h = 12)$mean)
# STLM-AR, does not work because our data is not seasonal
#stlm_ar_model <- stlm(univariate_time_series, modelfunction = "ar")
# Neural Network time series forecasts
nnetar_model <- nnetar(univariate_time_series)
mse(as.vector(univariate_time_series_test), forecast(nnetar_model, h = 12)$mean)
## Other constituent models that may perform well
# Naive model with 0 forecasts
# Neat idea, but practically speaking ends up being the sam eas auto arima's 0, 0, 0 model most of the time
# naive0_model <- rep(0, 12)
# mse(as.vector(univariate_time_series_test), naive0_model)
# ARMA 1, 1 with GARCH 1, 1 gaussian errors
ugarch_spec <- ugarchspec(variance.model = list(model = "sGARCH",
garchOrder = c(1, 1),
submodel = NULL,
external.regressors = NULL,
variance.targeting = FALSE),
mean.model     = list(armaOrder = c(1, 1),
include.mean = TRUE,
external.regressors = NULL),
distribution.model = "norm")
ugarch_spec_fit <- ugarchfit(ugarch_spec, data = univariate_time_series, out.sample = 12)
ugarch_forecast <- ugarchforecast(ugarch_spec_fit, n.ahead = 12, n.roll = 11)
fitted(ugarch_forecast)[1, ]
plot(ugarch_forecast, which = 2)
# ARMA 1, 1 with GARCH 1, 1 student t errors
ugarch_spec_t <- ugarchspec(variance.model = list(model = "sGARCH",
garchOrder = c(1, 1),
submodel = NULL,
external.regressors = NULL,
variance.targeting = FALSE),
mean.model     = list(armaOrder = c(1, 1),
include.mean = TRUE,
external.regressors = NULL),
distribution.model = "std")
ugarch_spec_t_fit <- ugarchfit(ugarch_spec_t, data = univariate_time_series, out.sample = 12)
ugarch_t_forecast <- ugarchforecast(ugarch_spec_t_fit, n.ahead = 12, n.roll = 11)
fitted(ugarch_t_forecast)[1, ]
plot(ugarch_t_forecast, which = 2)
## Stochastic volatility model
stochvol_model <- svlsample(univariate_time_series, draws = 10000, burnin = 1000, designmatrix = "ar1",
priormu = c(0, 100), priorphi = c(5, 1.5), priorsigma = 1,
priorrho = c(4, 4), priorbeta = c(0, 10000), thinpara = 1,
thinlatent = 1, thintime = NULL, keeptime = "all", quiet = FALSE)
### Data matrix
# Just use fforma_features function from before, easy
data <- foreach(i = 1:length(unique(pooled_panel$stock)), .combine = "rbind") %dopar% {
stock_id <- unique(pooled_panel$stock)
univariate_time_series <- pooled_panel %>%
filter(stock == stock_id[i]) %>%
filter(time %in% timeSlices[[1]]$train | time %in% timeSlices[[1]]$validation) %>%
select(rt) %>%
ts()
univariate_time_series_test <- pooled_panel %>%
filter(stock == stock_id[i]) %>%
filter(time %in% timeSlices[[1]]$test) %>%
select(rt) %>%
ts()
fforma_features(univariate_time_series)
}
### Errors
# This is trickier
errors <- data.frame(naive = mse(as.vector(univariate_time_series_test), predict(naive_model)$mean),
random_walk = mse(as.vector(univariate_time_series_test), predict(random_walk_model)$mean),
theta = mse(as.vector(univariate_time_series_test), predict(theta_model)$mean),
auto_arima = mse(as.vector(univariate_time_series_test),
forecast(auto_arima_model, h = 12)$mean),
ets = mse(as.vector(univariate_time_series_test), forecast(auto_ets_model, h = 12)$mean),
tbats = mse(as.vector(univariate_time_series_test), forecast(tbats_model, h = 12)$mean),
nnetar = mse(as.vector(univariate_time_series_test), forecast(nnetar_model, h = 12)$mean),
garch_11 = mse(as.vector(univariate_time_series_test), fitted(ugarch_forecast)[1, ]),
garch_11_t = mse(as.vector(univariate_time_series_test), fitted(ugarch_t_forecast)[1, ]),
stochvol = mse(as.vector(univariate_time_series_test),
apply(X = predict(stochvol_model, steps = 12)$y, MARGIN = 2, FUN = mean)))
### Errors
# This is trickier
errors <- data.frame(naive = mse(as.vector(univariate_time_series_test), predict(naive_model)$mean),
random_walk = mse(as.vector(univariate_time_series_test), predict(random_walk_model)$mean),
theta = mse(as.vector(univariate_time_series_test), predict(theta_model)$mean),
auto_arima = mse(as.vector(univariate_time_series_test),
forecast(auto_arima_model, h = 12)$mean),
ets = mse(as.vector(univariate_time_series_test), forecast(auto_ets_model, h = 12)$mean),
tbats = mse(as.vector(univariate_time_series_test), forecast(tbats_model, h = 12)$mean),
nnetar = mse(as.vector(univariate_time_series_test), forecast(nnetar_model, h = 12)$mean),
garch_11 = mse(as.vector(univariate_time_series_test), fitted(ugarch_forecast)[1, ]),
garch_11_t = mse(as.vector(univariate_time_series_test), fitted(ugarch_t_forecast)[1, ]),
stochvol = mse(as.vector(univariate_time_series_test),
apply(X = predict(stochvol_model, steps = 12)$y, MARGIN = 2, FUN = mean)))
library(stochvol)
stochvol_model <- svlsample(univariate_time_series, draws = 10000, burnin = 1000, designmatrix = "ar1",
priormu = c(0, 100), priorphi = c(5, 1.5), priorsigma = 1,
priorrho = c(4, 4), priorbeta = c(0, 10000), thinpara = 1,
thinlatent = 1, thintime = NULL, keeptime = "all", quiet = FALSE)
### Data matrix
# Just use fforma_features function from before, easy
data <- foreach(i = 1:length(unique(pooled_panel$stock)), .combine = "rbind") %dopar% {
stock_id <- unique(pooled_panel$stock)
univariate_time_series <- pooled_panel %>%
filter(stock == stock_id[i]) %>%
filter(time %in% timeSlices[[1]]$train | time %in% timeSlices[[1]]$validation) %>%
select(rt) %>%
ts()
univariate_time_series_test <- pooled_panel %>%
filter(stock == stock_id[i]) %>%
filter(time %in% timeSlices[[1]]$test) %>%
select(rt) %>%
ts()
fforma_features(univariate_time_series)
}
### Data matrix
# Just use fforma_features function from before, easy
data <- foreach(i = 1:length(unique(pooled_panel$stock)), .combine = "rbind") %dopar% {
stock_id <- unique(pooled_panel$stock)
univariate_time_series <- pooled_panel %>%
filter(stock == stock_id[i]) %>%
filter(time %in% timeSlices[[1]]$train | time %in% timeSlices[[1]]$validation) %>%
select(rt) %>%
ts()
univariate_time_series_test <- pooled_panel %>%
filter(stock == stock_id[i]) %>%
filter(time %in% timeSlices[[1]]$test) %>%
select(rt) %>%
ts()
fforma_features(univariate_time_series)
}
### Errors
# This is trickier
errors <- data.frame(naive = mse(as.vector(univariate_time_series_test), predict(naive_model)$mean),
random_walk = mse(as.vector(univariate_time_series_test), predict(random_walk_model)$mean),
theta = mse(as.vector(univariate_time_series_test), predict(theta_model)$mean),
auto_arima = mse(as.vector(univariate_time_series_test),
forecast(auto_arima_model, h = 12)$mean),
ets = mse(as.vector(univariate_time_series_test), forecast(auto_ets_model, h = 12)$mean),
tbats = mse(as.vector(univariate_time_series_test), forecast(tbats_model, h = 12)$mean),
nnetar = mse(as.vector(univariate_time_series_test), forecast(nnetar_model, h = 12)$mean),
garch_11 = mse(as.vector(univariate_time_series_test), fitted(ugarch_forecast)[1, ]),
garch_11_t = mse(as.vector(univariate_time_series_test), fitted(ugarch_t_forecast)[1, ]),
stochvol = mse(as.vector(univariate_time_series_test),
apply(X = predict(stochvol_model, steps = 12)$y, MARGIN = 2, FUN = mean)))
### Labels
# Easy, but requires the setup of the error matrix earlier
labels <- data.frame(label = which.min(errors))
errors <- foreach(i = 1:length(unique(pooled_panel$stock)), .combine = "rbind") %dopar% {
## Filter Data first
stock_id <- unique(pooled_panel$stock)
univariate_time_series <- pooled_panel %>%
filter(stock == stock_id[i]) %>%
filter(time %in% timeSlices[[1]]$train | time %in% timeSlices[[1]]$validation) %>%
select(rt) %>%
ts()
univariate_time_series_test <- pooled_panel %>%
filter(stock == stock_id[i]) %>%
filter(time %in% timeSlices[[1]]$test) %>%
select(rt) %>%
ts()
## Estimation of consituent models
# Naive
naive_model <- naive(univariate_time_series, h = 12)
# Random walk with drift
random_walk_model <- rwf(univariate_time_series, h = 12, drift = TRUE)
# Seasonal Naive, this ends up being the same as naive
seasonal_naive_model <- snaive(univariate_time_series, h = 12)
# Theta Method
theta_model <- thetaf(univariate_time_series, h = 12)
## Auto arima
auto_arima_model <- auto.arima(univariate_time_series)
# Auto ETS
auto_ets_model <- ets(univariate_time_series)
# TBATS
tbats_model <- tbats(univariate_time_series)
# Neural Network time series forecasts
nnetar_model <- nnetar(univariate_time_series)
# Naive model that forecasts 0
# Neat idea, but practically speaking ends up being the sam eas auto arima's 0, 0, 0 model most of the time
# naive0_model <- rep(0, 12)
# mse(as.vector(univariate_time_series_test), naive0_model)
# ARMA 1, 1 with GARCH 1, 1 gaussian errors
ugarch_spec <- ugarchspec(variance.model = list(model = "sGARCH",
garchOrder = c(1, 1),
submodel = NULL,
external.regressors = NULL,
variance.targeting = FALSE),
mean.model     = list(armaOrder = c(1, 1),
include.mean = TRUE,
external.regressors = NULL),
distribution.model = "norm")
ugarch_spec_fit <- ugarchfit(ugarch_spec, data = univariate_time_series, out.sample = 12,
solver = "hybrid")
ugarch_forecast <- ugarchforecast(ugarch_spec_fit, n.ahead = 12, n.roll = 11)
## Stochastic volatility model
## Theoretically the most correctly specified model, but very computatioally intensive, and may be infeasible
## Takes ~ 14 seconds to estimate
stochvol_model <- svlsample(univariate_time_series, draws = 10000, burnin = 1000, designmatrix = "ar1",
priormu = c(0, 100), priorphi = c(5, 1.5), priorsigma = 1,
priorrho = c(4, 4), priorbeta = c(0, 10000), thinpara = 1,
thinlatent = 1, thintime = NULL, keeptime = "all", quiet = TRUE)
## Put errors into a single row
errors <- data.frame(naive = mse(as.vector(univariate_time_series_test), predict(naive_model)$mean),
random_walk = mse(as.vector(univariate_time_series_test), predict(random_walk_model)$mean),
theta = mse(as.vector(univariate_time_series_test), predict(theta_model)$mean),
auto_arima = mse(as.vector(univariate_time_series_test),
forecast(auto_arima_model, h = 12)$mean),
ets = mse(as.vector(univariate_time_series_test), forecast(auto_ets_model, h = 12)$mean),
tbats = mse(as.vector(univariate_time_series_test), forecast(tbats_model, h = 12)$mean),
nnetar = mse(as.vector(univariate_time_series_test), forecast(nnetar_model, h = 12)$mean),
garch_11 = mse(as.vector(univariate_time_series_test), fitted(ugarch_forecast)[1, ]),
stochvol = mse(as.vector(univariate_time_series_test),
apply(X = predict(stochvol_model, steps = 12)$y, MARGIN = 2, FUN = mean)))
errors
}
errors <- foreach(i = 1:length(unique(pooled_panel$stock)), .combine = "rbind") %dopar% {
## Filter Data first
stock_id <- unique(pooled_panel$stock)
univariate_time_series <- pooled_panel %>%
filter(stock == stock_id[i]) %>%
filter(time %in% timeSlices[[1]]$train | time %in% timeSlices[[1]]$validation) %>%
select(rt) %>%
ts()
univariate_time_series_test <- pooled_panel %>%
filter(stock == stock_id[i]) %>%
filter(time %in% timeSlices[[1]]$test) %>%
select(rt) %>%
ts()
## Estimation of consituent models
# Naive
naive_model <- naive(univariate_time_series, h = 12)
# Random walk with drift
random_walk_model <- rwf(univariate_time_series, h = 12, drift = TRUE)
# Seasonal Naive, this ends up being the same as naive
seasonal_naive_model <- snaive(univariate_time_series, h = 12)
# Theta Method
theta_model <- thetaf(univariate_time_series, h = 12)
## Auto arima
auto_arima_model <- auto.arima(univariate_time_series)
# Auto ETS
auto_ets_model <- ets(univariate_time_series)
# TBATS
tbats_model <- tbats(univariate_time_series)
# Neural Network time series forecasts
nnetar_model <- nnetar(univariate_time_series)
# Naive model that forecasts 0
# Neat idea, but practically speaking ends up being the sam eas auto arima's 0, 0, 0 model most of the time
# naive0_model <- rep(0, 12)
# mse(as.vector(univariate_time_series_test), naive0_model)
# ARMA 1, 1 with GARCH 1, 1 gaussian errors
ugarch_spec <- ugarchspec(variance.model = list(model = "sGARCH",
garchOrder = c(1, 1),
submodel = NULL,
external.regressors = NULL,
variance.targeting = FALSE),
mean.model     = list(armaOrder = c(1, 1),
include.mean = TRUE,
external.regressors = NULL),
distribution.model = "norm")
ugarch_spec_fit <- ugarchfit(ugarch_spec, data = univariate_time_series, out.sample = 12,
solver = "hybrid")
ugarch_forecast <- ugarchforecast(ugarch_spec_fit, n.ahead = 12, n.roll = 11)
## Stochastic volatility model
## Theoretically the most correctly specified model, but very computatioally intensive, and may be infeasible
## Takes ~ 14 seconds to estimate
stochvol_model <- svlsample(univariate_time_series, draws = 10000, burnin = 1000, designmatrix = "ar1",
priormu = c(0, 100), priorphi = c(5, 1.5), priorsigma = 1,
priorrho = c(4, 4), priorbeta = c(0, 10000), thinpara = 1,
thinlatent = 1, thintime = NULL, keeptime = "all", quiet = TRUE)
## Put errors into a single row
errors <- data.frame(naive = mse(as.vector(univariate_time_series_test), predict(naive_model)$mean),
random_walk = mse(as.vector(univariate_time_series_test), predict(random_walk_model)$mean),
theta = mse(as.vector(univariate_time_series_test), predict(theta_model)$mean),
auto_arima = mse(as.vector(univariate_time_series_test),
forecast(auto_arima_model, h = 12)$mean),
ets = mse(as.vector(univariate_time_series_test), forecast(auto_ets_model, h = 12)$mean),
tbats = mse(as.vector(univariate_time_series_test), forecast(tbats_model, h = 12)$mean),
nnetar = mse(as.vector(univariate_time_series_test), forecast(nnetar_model, h = 12)$mean),
garch_11 = mse(as.vector(univariate_time_series_test), fitted(ugarch_forecast)[1, ]),
stochvol = mse(as.vector(univariate_time_series_test),
apply(X = predict(stochvol_model, steps = 12)$y, MARGIN = 2, FUN = mean)))
errors
}
labels <- foreach(i = 1:nrow(errors), .combine = "rbind") %dopar% {
data.frame(label = which.min(errors[i, ]) - 1)
}
## Create final list with everything needed in FFORMA
## Looks pretty good
train_list <- list(
data = data,
errors = errors,
labels = labels
)
error_softmax_obj <- function(preds, dtrain) {
labels <- xgboost::getinfo(dtrain, "label")
errors <- attr(dtrain, "errors")
preds <- exp(preds)
sp <- rowSums(preds)
for (n in 1:nrow(preds)) {
preds[n] <- preds/sp[n]
}
rowsumerrors <- rowSums(preds * errors)
grad <- preds*(errors - rowsumerrors)
hess <- errors*preds*(1.0-preds) - grad*preds
#hess <- grad*(1.0 - 2.0*preds)
#hess <- pmax(hess, 1e-16)
#the true hessian should be grad*(1.0 - 2.0*preds) but it produces numerical problems
#what we use here is a upper bound
#print(mean(rowSums(preds*errors)))
return(list(grad = t(grad), hess = t(hess)))
}
param <- list(booster = "gbtree",
max_depth = 6, eta = 0.3, nthread = 12,
num_class = ncol(errors),
objective = error_softmax_obj,
subsample = 1,
colsample_bytree = 1)
dtrain <- xgb.DMatrix(data = as.matrix(train_list$data),
label = train_list$labels[, 1])
attr(dtrain, "errors") <- train_list$errors
bst <- xgboost::xgb.train(param, dtrain, 94,
verbose = 1)
install.packages("xgboost_0.90.0.2.tar")
install.packages("./xgboost_0.90.0.2.tar")
install.packages(".\xgboost_0.90.0.2.tar")
install.packages("~./xgboost_0.90.0.2.tar")
install.packages("~/xgboost_0.90.0.2.tar")
install.packages(""~/GitHub/Evaluation-of-Machine-Learning-in-Asset-Pricing/R/xgboost_0.90.0.2.tar")
install.packages("~/GitHub/Evaluation-of-Machine-Learning-in-Asset-Pricing/R/xgboost_0.90.0.2.tar")
install.packages("xgboost_0.90.0.2.tar.gz")
?install.packages
install.packages("xgboost_0.90.0.2.tar.gz", type = NULL)
install.packages("xgboost_0.90.0.2.tar.gz", repos = NULL)
?setwd
setwd("C:\")
w
...
)
ASasA
install.packages("xgboost_0.90.0.2.tar", repos = NULL)
install.packages("xgboost_0.90.0.2.tar.gz", repos = NULL)
