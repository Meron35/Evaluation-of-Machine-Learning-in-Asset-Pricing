external.regressors = NULL),
distribution.model = "norm")
ugarch_spec_fit <- ugarchfit(ugarch_spec, data = univariate_time_series, out.sample = 12)
ugarch_forecast <- ugarchforecast(ugarch_spec_fit, n.ahead = 12, n.roll = 11)
fitted(ugarch_forecast)[1, ]
plot(ugarch_forecast, which = 2)
# ARMA 1, 1 with GARCH 1, 1 student t errors
ugarch_spec_t <- ugarchspec(variance.model = list(model = "sGARCH",
garchOrder = c(1, 1),
submodel = NULL,
external.regressors = NULL,
variance.targeting = FALSE),
mean.model     = list(armaOrder = c(1, 1),
include.mean = TRUE,
external.regressors = NULL),
distribution.model = "std")
ugarch_spec_t_fit <- ugarchfit(ugarch_spec_t, data = univariate_time_series, out.sample = 12)
ugarch_t_forecast <- ugarchforecast(ugarch_spec_t_fit, n.ahead = 12, n.roll = 11)
fitted(ugarch_t_forecast)[1, ]
plot(ugarch_t_forecast, which = 2)
## Stochastic volatility model
stochvol_model <- svlsample(univariate_time_series, draws = 10000, burnin = 1000, designmatrix = "ar1",
priormu = c(0, 100), priorphi = c(5, 1.5), priorsigma = 1,
priorrho = c(4, 4), priorbeta = c(0, 10000), thinpara = 1,
thinlatent = 1, thintime = NULL, keeptime = "all", quiet = FALSE)
### Data matrix
# Just use fforma_features function from before, easy
data <- foreach(i = 1:length(unique(pooled_panel$stock)), .combine = "rbind") %dopar% {
stock_id <- unique(pooled_panel$stock)
univariate_time_series <- pooled_panel %>%
filter(stock == stock_id[i]) %>%
filter(time %in% timeSlices[[1]]$train | time %in% timeSlices[[1]]$validation) %>%
select(rt) %>%
ts()
univariate_time_series_test <- pooled_panel %>%
filter(stock == stock_id[i]) %>%
filter(time %in% timeSlices[[1]]$test) %>%
select(rt) %>%
ts()
fforma_features(univariate_time_series)
}
errors <- foreach(i = 1:length(unique(pooled_panel$stock)), .combine = "rbind") %dopar% {
## Filter Data first
stock_id <- unique(pooled_panel$stock)
univariate_time_series <- pooled_panel %>%
filter(stock == stock_id[i]) %>%
filter(time %in% timeSlices[[1]]$train | time %in% timeSlices[[1]]$validation) %>%
select(rt) %>%
ts()
univariate_time_series_test <- pooled_panel %>%
filter(stock == stock_id[i]) %>%
filter(time %in% timeSlices[[1]]$test) %>%
select(rt) %>%
ts()
## Estimation of consituent models
# Naive
naive_model <- naive(univariate_time_series, h = 12)
# Random walk with drift
random_walk_model <- rwf(univariate_time_series, h = 12, drift = TRUE)
# Seasonal Naive, this ends up being the same as naive
seasonal_naive_model <- snaive(univariate_time_series, h = 12)
# Theta Method
theta_model <- thetaf(univariate_time_series, h = 12)
## Auto arima
auto_arima_model <- auto.arima(univariate_time_series)
# Auto ETS
auto_ets_model <- ets(univariate_time_series)
# TBATS
tbats_model <- tbats(univariate_time_series)
# Neural Network time series forecasts
nnetar_model <- nnetar(univariate_time_series)
# Naive model that forecasts 0
# Neat idea, but practically speaking ends up being the sam eas auto arima's 0, 0, 0 model most of the time
# naive0_model <- rep(0, 12)
# mse(as.vector(univariate_time_series_test), naive0_model)
# ARMA 1, 1 with GARCH 1, 1 gaussian errors
ugarch_spec <- ugarchspec(variance.model = list(model = "sGARCH",
garchOrder = c(1, 1),
submodel = NULL,
external.regressors = NULL,
variance.targeting = FALSE),
mean.model     = list(armaOrder = c(1, 1),
include.mean = TRUE,
external.regressors = NULL),
distribution.model = "norm")
ugarch_spec_fit <- ugarchfit(ugarch_spec, data = univariate_time_series, out.sample = 12,
solver = "hybrid")
ugarch_forecast <- ugarchforecast(ugarch_spec_fit, n.ahead = 12, n.roll = 11)
## Stochastic volatility model
## Theoretically the most correctly specified model, but very computatioally intensive, and may be infeasible
## Takes ~ 14 seconds to estimate
#stochvol_model <- svlsample(univariate_time_series, draws = 10000, burnin = 1000, designmatrix = "ar1",
#priormu = c(0, 100), priorphi = c(5, 1.5), priorsigma = 1,
#priorrho = c(4, 4), priorbeta = c(0, 10000), thinpara = 1,
#thinlatent = 1, thintime = NULL, keeptime = "all", quiet = TRUE)
## Put errors into a single row
errors <- data.frame(naive = mse(as.vector(univariate_time_series_test), predict(naive_model)$mean),
random_walk = mse(as.vector(univariate_time_series_test), predict(random_walk_model)$mean),
theta = mse(as.vector(univariate_time_series_test), predict(theta_model)$mean),
auto_arima = mse(as.vector(univariate_time_series_test),
forecast(auto_arima_model, h = 12)$mean),
ets = mse(as.vector(univariate_time_series_test), forecast(auto_ets_model, h = 12)$mean),
tbats = mse(as.vector(univariate_time_series_test), forecast(tbats_model, h = 12)$mean),
nnetar = mse(as.vector(univariate_time_series_test), forecast(nnetar_model, h = 12)$mean),
garch_11 = mse(as.vector(univariate_time_series_test), fitted(ugarch_forecast)[1, ]))
#stochvol = mse(as.vector(univariate_time_series_test),
#apply(X = predict(stochvol_model, steps = 12)$y, MARGIN = 2, FUN = mean)))
errors
}
labels <- foreach(i = 1:nrow(errors), .combine = "rbind") %dopar% {
data.frame(label = which.min(errors[i, ]) - 1)
}
## Create final list with everything needed in FFORMA
## Looks pretty good
train_list <- list(
data = data,
errors = errors,
labels = labels
)
softmax_transform <- function(x) {
exp(x) / sum(exp(x))
}
error_softmax_obj <- function(preds, dtrain) {
labels <- xgboost::getinfo(dtrain, "label")
errors <- attr(dtrain, "errors")
for (n in 1:nrow(preds)) {
preds[n, ] <- softmax_transform(preds[n, ])
}
rowsumerrors <- rowSums(preds * errors)
grad <- preds*(errors - rowsumerrors)
hess <- errors*preds*(1.0 - preds) - grad*preds
#hess <- grad*(1.0 - 2.0*preds)
#hess <- pmax(hess, 1e-16)
#the true hessian should be grad*(1.0 - 2.0*preds) but it produces numerical problems
#what we use here is a upper bound
#print(mean(rowSums(preds*errors)))
return(list(grad = t(grad), hess = t(hess)))
}
error_softmax_obj <- function(preds, dtrain) {
labels <- xgboost::getinfo(dtrain, "label")
errors <- attr(dtrain, "errors")
preds <- exp(preds)
sp <- rowSums(preds)
preds <- preds / replicate(ncol(preds), sp)
rowsumerrors <- replicate(ncol(preds), rowSums(preds * errors))
grad <- preds*(errors - rowsumerrors)
hess <- errors*preds*(1.0-preds) - grad*preds
#hess <- grad*(1.0 - 2.0*preds)
#hess <- pmax(hess, 1e-16)
#the true hessian should be grad*(1.0 - 2.0*preds) but it produces numerical problems
#what we use here is a upper bound
#print(mean(rowSums(preds*errors)))
return(list(grad = t(grad), hess = t(hess)))
}
param <- list(booster = "gbtree",
max_depth = 6, eta = 0.3, nthread = 12,
num_class = ncol(errors),
objective = error_softmax_obj,
subsample = 1,
colsample_bytree = 1)
dtrain <- xgb.DMatrix(data = as.matrix(train_list$data),
label = train_list$labels[, 1])
attr(dtrain, "errors") <- train_list$errors
bst <- xgboost::xgb.train(param, dtrain, 94,
verbose = 1)
dtrain <- xgb.DMatrix(data = as.matrix(train_list$data),
label = train_list$labels[, 1])
attr(dtrain, "errors") <- train_list$errors
bst <- xgboost::xgb.train(param, dtrain, 94,
verbose = 1)
dtrain
?xgb.train
bst <- xgboost::xgb.train(params = param, data = dtrain, nrounds = 94,
verbose = 1)
error_softmax_obj <- function(preds, dtrain) {
labels <- xgboost::getinfo(dtrain, "label")
errors <- attr(dtrain, "errors")
preds <- exp(preds)
sp <- rowSums(preds)
preds <- preds / replicate(ncol(preds), sp)
rowsumerrors <- replicate(ncol(preds), rowSums(preds * errors))
grad <- preds*(errors - rowsumerrors)
hess <- errors*preds*(1.0-preds) - grad*preds
#hess <- grad*(1.0 - 2.0*preds)
#hess <- pmax(hess, 1e-16)
#the true hessian should be grad*(1.0 - 2.0*preds) but it produces numerical problems
#what we use here is a upper bound
#print(mean(rowSums(preds*errors)))
return(list(grad = t(grad), hess = t(hess)))
}
bst <- xgboost::xgb.train(params = param, data = dtrain, nrounds = 94,
verbose = 1)
softmax_transform <- function(x) {
exp(x) / sum(exp(x))
}
error_softmax_obj <- function(preds, dtrain) {
labels <- xgboost::getinfo(dtrain, "label")
errors <- attr(dtrain, "errors")
for (n in 1:nrow(preds)) {
preds[n, ] <- softmax_transform(preds[n, ])
}
rowsumerrors <- rowSums(preds * errors)
grad <- preds*(errors - rowsumerrors)
hess <- errors*preds*(1.0 - preds) - grad*preds
#hess <- grad*(1.0 - 2.0*preds)
#hess <- pmax(hess, 1e-16)
#the true hessian should be grad*(1.0 - 2.0*preds) but it produces numerical problems
#what we use here is a upper bound
#print(mean(rowSums(preds*errors)))
return(list(grad = t(grad), hess = t(hess)))
}
error_softmax_obj <- function(preds, dtrain) {
labels <- xgboost::getinfo(dtrain, "label")
errors <- attr(dtrain, "errors")
preds <- exp(preds)
sp <- rowSums(preds)
preds <- preds / replicate(ncol(preds), sp)
rowsumerrors <- replicate(ncol(preds), rowSums(preds * errors))
grad <- preds*(errors - rowsumerrors)
hess <- errors*preds*(1.0-preds) - grad*preds
#hess <- grad*(1.0 - 2.0*preds)
#hess <- pmax(hess, 1e-16)
#the true hessian should be grad*(1.0 - 2.0*preds) but it produces numerical problems
#what we use here is a upper bound
#print(mean(rowSums(preds*errors)))
return(list(grad = t(grad), hess = t(hess)))
}
param <- list(booster = "gbtree",
max_depth = 6, eta = 0.3, nthread = 12,
num_class = ncol(errors),
objective = error_softmax_obj,
subsample = 1,
colsample_bytree = 1)
dtrain <- xgb.DMatrix(data = as.matrix(train_list$data),
label = train_list$labels[, 1])
attr(dtrain, "errors") <- train_list$errors
bst <- xgboost::xgb.train(params = param, data = dtrain, nrounds = 94,
verbose = 1)
error_softmax_obj <- function(preds, dtrain) {
labels <- xgboost::getinfo(dtrain, "label")
errors <- attr(dtrain, "errors")
for (n in 1:nrow(preds)) {
preds[n, ] <- softmax_transform(preds[n, ])
}
rowsumerrors <- rowSums(preds * errors)
grad <- preds*(errors - rowsumerrors)
hess <- errors*preds*(1.0 - preds) - grad*preds
#hess <- grad*(1.0 - 2.0*preds)
#hess <- pmax(hess, 1e-16)
#the true hessian should be grad*(1.0 - 2.0*preds) but it produces numerical problems
#what we use here is a upper bound
#print(mean(rowSums(preds*errors)))
return(list(grad = t(grad), hess = t(hess)))
}
bst <- xgboost::xgb.train(params = param, data = dtrain, nrounds = 94,
verbose = 1)
error_softmax_obj <- function(preds, dtrain) {
labels <- xgboost::getinfo(dtrain, "label")
errors <- attr(dtrain, "errors")
preds <- exp(preds)
sp <- rowSums(preds)
preds <- preds / replicate(ncol(preds), sp)
rowsumerrors <- replicate(ncol(preds), rowSums(preds * errors))
grad <- preds*(errors - rowsumerrors)
hess <- errors*preds*(1.0-preds) - grad*preds
#hess <- grad*(1.0 - 2.0*preds)
#hess <- pmax(hess, 1e-16)
#the true hessian should be grad*(1.0 - 2.0*preds) but it produces numerical problems
#what we use here is a upper bound
#print(mean(rowSums(preds*errors)))
return(list(grad = t(grad), hess = t(hess)))
}
bst <- xgboost::xgb.train(params = param, data = dtrain, nrounds = 94,
verbose = 1)
attr(dtrain, "errors") <- train_list$errors
bst <- xgboost::xgb.train(params = param, data = dtrain, nrounds = 94,
verbose = 1)
install.packages("xgboost_0.90.0.2.tar.gz", repos = NULL)
install.packages("Rcpp")
install.packages(c("AER", "backports", "Bessel", "BH", "bibtex", "bookdown", "broom", "callr", "car", "carData", "caret", "caTools", "checkmate", "chron", "classInt", "cli", "clipr", "coda", "curl", "data.table", "DBI", "ddalpha", "demography", "Deriv", "devtools", "digest", "dimRed", "doFuture", "doParallel", "e1071", "evaluate", "foreach", "forecast", "foreign", "formatR", "fracdiff", "fs", "ftsa", "future", "gender", "geometry", "geosphere", "git2r", "glmnet", "glmnetUtils", "globals", "gmm", "gower", "h2o", "heavy", "hexbin", "htmltools", "htmlwidgets", "httpuv", "igraph", "ipred", "iterators", "jomo", "jpeg", "keras", "kernlab", "knitr", "ks", "later", "lava", "listenv", "lmtest", "loo", "lpSolve", "lwgeom", "maptools", "markdown", "MASS", "Matrix", "matrixStats", "maxLik", "mclust", "mice", "mime", "miscTools", "mlr", "multicool", "mvtnorm", "numDeriv", "openNLP", "openssl", "openxlsx", "ordinal", "parallelMap", "ParamHelpers", "pkgbuild", "pkgconfig", "plm", "plotly", "plotrix", "pls", "plyr", "processx", "prodlim", "progress", "promises", "psych", "quadprog", "quantmod", "quantreg", "R6", "randomForestSRC", "raster", "rcmdcheck", "Rcpp", "RcppArmadillo", "RcppEigen", "recipes", "remotes", "reticulate", "rgdal", "rgeos", "RgoogleMaps", "rmarkdown", "rmgarch", "rngtools", "robustbase", "rpart.plot", "rstan", "scales", "selectr", "sf", "sfsmisc", "shiny", "slam", "sp", "SparseM", "spData", "StanHeaders", "statmod", "stringdist", "strucchange", "sys", "tensorflow", "testthat", "tinytex", "tm", "tmaptools", "tree", "tseries", "tsfeatures", "TTR", "units", "uroot", "usethis", "whisker", "xfun", "XML", "xtable", "zip", "zoo"))
install.packages("Rcpp")
install.packages("xgboost_0.90.0.2.tar.gz", repos = NULL)
install.packages("xgboost_0.90.0.2.tar.gz", repos = NULL, type = "source")
install.packages("devtools")
install.packages("xgboost_0.90.0.2.tar.gz", repos = NULL, type = "source")
devtools::install_github("pmontman/customxgboost")
library(xgboost)
################
##Load Libraries
################
library(speedglm)
library(tidyverse)
library(keras)
library(ggplot2)
library(forecast)
library(rlist)
library(Metrics)
library(tensorflow)
library(quantreg)
library(randomForestSRC)
library(hqreg)
library(caret)
library(tsfeatures)
## Vector arima/garch packages
library(xts)
library(rmgarch)
library(marima)
library(xgboost)
#Parallel Computing
library(foreach)
library(doFuture)
#Registering
registerDoFuture()
plan(multisession)
softmax_transform <- function(x) {
exp(x) / sum(exp(x))
}
error_softmax_obj <- function(preds, dtrain) {
labels <- xgboost::getinfo(dtrain, "label")
errors <- attr(dtrain, "errors")
preds <- exp(preds)
sp <- rowSums(preds)
preds <- preds / replicate(ncol(preds), sp)
rowsumerrors <- replicate(ncol(preds), rowSums(preds * errors))
grad <- preds*(errors - rowsumerrors)
hess <- errors*preds*(1.0-preds) - grad*preds
#hess <- grad*(1.0 - 2.0*preds)
#hess <- pmax(hess, 1e-16)
#the true hessian should be grad*(1.0 - 2.0*preds) but it produces numerical problems
#what we use here is a upper bound
#print(mean(rowSums(preds*errors)))
return(list(grad = t(grad), hess = t(hess)))
}
param <- list(booster = "gbtree",
max_depth = 6, eta = 0.3, nthread = 12,
num_class = ncol(errors),
objective = error_softmax_obj,
subsample = 1,
colsample_bytree = 1)
dtrain <- xgb.DMatrix(data = as.matrix(train_list$data),
label = train_list$labels[, 1])
attr(dtrain, "errors") <- train_list$errors
bst <- xgboost::xgb.train(params = param, data = dtrain, nrounds = 94,
verbose = 1)
bst
summary(bst)
bst$params
pred <- predict(bst, newdata = xgboost::xgb.DMatrix(as.matrix(train_list$data)), outputmargin = TRUE, reshape = TRUE)
pred <- t(apply(pred, 1, softmax_transform))
pred
xgb.importance(model = bst)
xgb.ggplot.importance(bst)
?xgb.train
param <- list(booster = "gbtree",
max_depth = 6, eta = 0.3, nthread = 12,
num_class = ncol(errors),
objective = error_softmax_obj,
subsample = 1,
colsample_bytree = 1)
dtrain <- xgb.DMatrix(data = as.matrix(train_list$data),
label = train_list$labels[, 1])
attr(dtrain, "errors") <- train_list$errors
bst <- xgboost::xgb.train(params = param, data = dtrain, nrounds = 94,
verbose = 1)
bst
pred <- predict(bst, newdata = xgboost::xgb.DMatrix(as.matrix(train_list$data)), outputmargin = TRUE, reshape = TRUE)
pred <- t(apply(pred, 1, softmax_transform))
pred
## Create final list with everything needed in FFORMA
## Looks pretty good
train_list <- list(
data = data,
errors = errors,
labels = labels
)
softmax_transform <- function(x) {
exp(x) / sum(exp(x))
}
error_softmax_obj <- function(preds, dtrain) {
labels <- xgboost::getinfo(dtrain, "label")
errors <- attr(dtrain, "errors")
preds <- exp(preds)
sp <- rowSums(preds)
preds <- preds / replicate(ncol(preds), sp)
rowsumerrors <- replicate(ncol(preds), rowSums(preds * errors))
grad <- preds*(errors - rowsumerrors)
hess <- errors*preds*(1.0-preds) - grad*preds
#hess <- grad*(1.0 - 2.0*preds)
#hess <- pmax(hess, 1e-16)
#the true hessian should be grad*(1.0 - 2.0*preds) but it produces numerical problems
#what we use here is a upper bound
#print(mean(rowSums(preds*errors)))
return(list(grad = t(grad), hess = t(hess)))
}
param <- list(booster = "gbtree",
max_depth = 6, eta = 0.3, nthread = 12,
num_class = ncol(errors),
objective = error_softmax_obj,
subsample = 1,
colsample_bytree = 1)
dtrain <- xgb.DMatrix(data = as.matrix(train_list$data),
label = train_list$labels[, 1])
attr(dtrain, "errors") <- train_list$errors
bst <- xgboost::xgb.train(params = param, data = dtrain, nrounds = 94,
verbose = 1)
bst <- xgboost::xgb.train(params = param, data = dtrain, nrounds = 94,
verbose = 1)
bst
pred <- predict(bst, newdata = xgboost::xgb.DMatrix(as.matrix(train_list$data)), outputmargin = TRUE, reshape = TRUE)
bst <- xgboost::xgb.train(params = param, data = dtrain, nrounds = 200,
verbose = 1)
bst
pred <- predict(bst, newdata = xgboost::xgb.DMatrix(as.matrix(train_list$data)), outputmargin = TRUE, reshape = TRUE)
pred <- t(apply(pred, 1, softmax_transform))
pred
### Data matrix
# Just use fforma_features function from before, easy
data <- foreach(i = 1:length(unique(pooled_panel$stock)), .combine = "rbind") %dopar% {
stock_id <- unique(pooled_panel$stock)
univariate_time_series <- pooled_panel %>%
filter(stock == stock_id[i]) %>%
filter(time %in% timeSlices[[1]]$train | time %in% timeSlices[[1]]$validation) %>%
select(rt) %>%
ts()
univariate_time_series_test <- pooled_panel %>%
filter(stock == stock_id[i]) %>%
filter(time %in% timeSlices[[1]]$test) %>%
select(rt) %>%
ts()
fforma_features(univariate_time_series)
}
errors <- foreach(i = 1:length(unique(pooled_panel$stock)), .combine = "rbind") %dopar% {
## Filter Data first
stock_id <- unique(pooled_panel$stock)
univariate_time_series <- pooled_panel %>%
filter(stock == stock_id[i]) %>%
filter(time %in% timeSlices[[1]]$train | time %in% timeSlices[[1]]$validation) %>%
select(rt) %>%
ts()
univariate_time_series_test <- pooled_panel %>%
filter(stock == stock_id[i]) %>%
filter(time %in% timeSlices[[1]]$test) %>%
select(rt) %>%
ts()
## Estimation of consituent models
# Naive
naive_model <- naive(univariate_time_series, h = 12)
# Random walk with drift
random_walk_model <- rwf(univariate_time_series, h = 12, drift = TRUE)
# Seasonal Naive, this ends up being the same as naive
seasonal_naive_model <- snaive(univariate_time_series, h = 12)
# Theta Method
theta_model <- thetaf(univariate_time_series, h = 12)
## Auto arima
auto_arima_model <- auto.arima(univariate_time_series)
# Auto ETS
auto_ets_model <- ets(univariate_time_series)
# TBATS
tbats_model <- tbats(univariate_time_series)
# Neural Network time series forecasts
nnetar_model <- nnetar(univariate_time_series)
# Naive model that forecasts 0
# Neat idea, but practically speaking ends up being the sam eas auto arima's 0, 0, 0 model most of the time
# naive0_model <- rep(0, 12)
# mse(as.vector(univariate_time_series_test), naive0_model)
# ARMA 1, 1 with GARCH 1, 1 gaussian errors
ugarch_spec <- ugarchspec(variance.model = list(model = "sGARCH",
garchOrder = c(1, 1),
submodel = NULL,
external.regressors = NULL,
variance.targeting = FALSE),
mean.model     = list(armaOrder = c(1, 1),
include.mean = TRUE,
external.regressors = NULL),
distribution.model = "norm")
ugarch_spec_fit <- ugarchfit(ugarch_spec, data = univariate_time_series, out.sample = 12,
solver = "hybrid")
ugarch_forecast <- ugarchforecast(ugarch_spec_fit, n.ahead = 12, n.roll = 11)
## Stochastic volatility model
## Theoretically the most correctly specified model, but very computatioally intensive, and may be infeasible
## Takes ~ 14 seconds to estimate
#stochvol_model <- svlsample(univariate_time_series, draws = 10000, burnin = 1000, designmatrix = "ar1",
#priormu = c(0, 100), priorphi = c(5, 1.5), priorsigma = 1,
#priorrho = c(4, 4), priorbeta = c(0, 10000), thinpara = 1,
#thinlatent = 1, thintime = NULL, keeptime = "all", quiet = TRUE)
## Put errors into a single row
errors <- data.frame(naive = mse(as.vector(univariate_time_series_test), predict(naive_model)$mean),
random_walk = mse(as.vector(univariate_time_series_test), predict(random_walk_model)$mean),
theta = mse(as.vector(univariate_time_series_test), predict(theta_model)$mean),
auto_arima = mse(as.vector(univariate_time_series_test),
forecast(auto_arima_model, h = 12)$mean),
ets = mse(as.vector(univariate_time_series_test), forecast(auto_ets_model, h = 12)$mean),
tbats = mse(as.vector(univariate_time_series_test), forecast(tbats_model, h = 12)$mean),
nnetar = mse(as.vector(univariate_time_series_test), forecast(nnetar_model, h = 12)$mean),
garch_11 = mse(as.vector(univariate_time_series_test), fitted(ugarch_forecast)[1, ]))
#stochvol = mse(as.vector(univariate_time_series_test),
#apply(X = predict(stochvol_model, steps = 12)$y, MARGIN = 2, FUN = mean)))
errors
}
dtrain <- xgb.DMatrix(data = as.matrix(train_list$data),
label = train_list$labels[, 1])
attr(dtrain, "errors") <- train_list$errors
bst <- xgboost::xgb.train(params = param, data = dtrain, nrounds = 200,
verbose = 1)
bst
pred <- predict(bst, newdata = xgboost::xgb.DMatrix(as.matrix(train_list$data)), outputmargin = TRUE, reshape = TRUE)
pred <- t(apply(pred, 1, softmax_transform))
pred
train_list$errors
devtools::install_github("Meron35/xgboost")
devtools::install_github("Meron35/xgboost")
devtools::install_github("Meron35/xgboost")
