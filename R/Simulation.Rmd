---
title: "Simulation"
author: "Ze Yu Zhong"
date: "21 March 2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
################
##Load Libraries
################

library(glmnet)
library(tidyverse)
library(keras)
library(ggplot2)
library(caret)
library(forecast)
library(rlist)
library(plm)
library(Metrics)
library(ranger)

set.seed(27935248)

#Change everything so that it is in terms of arrays instead of lists
#Lists were confusing and became hard to work with, also consistent with David
```

```{r global_options}
#####################################
##Simulation
#####################################

#Number of stocks
# N <- 200
# #Number of characteristics that underly true model
# P_c <- 100
# #Number of Periods
# Time <- 180

#Reduce Dimensionality to make it easier to code everything up for now. We can always increase this back up later

N <- 20

P_c <- 10

Time <- 18

```

```{r function_simulate_characteristics}
###################
##characteristics C_bar
###################

#######
##Function to Generate C_bar
#######

gen_C_bar <- function(){
  
  #Create some dimnames
  #provideDimnames doesn't do exactly what I want it to
  #Just do it manually via paste0
  
  stock_dim <- paste0("stock_", c(1:N))
  c_dim <- paste0("c_", c(1:P_c))
  time_dim <- paste0("time_", c(0:(Time+1)))
  
  #Initialize am empty array with i, j, t indexing
  
  C_bar <- array(data = 0, dim = c(N, P_c, (Time+2)), dimnames = list(stock_dim, c_dim, time_dim))
  
  #Generate rho vector to be constant across time and stocks
  #Original rho vector in draft proposal
  #rho <- runif(P_c, 1/2, 1)
  
  #Lower rho values here will give higher time series correlation, and lower cross sectional r squared. New rho is just 0.5 across all stocks and all times.
  
  rho <- rep(0.5, P_c)
  
  for (t in 1:(Time+1)) {
    for (j in 1:P_c) {
      C_bar[, j, t+1] <- C_bar[, j, t] * rho[j] + rnorm(N, 0, 1)
    }
  }
  
  ##Delete first period full of zeroes
  C_bar <- C_bar[, , -1]
  
  return(C_bar)
}

#######################################
##Generate correlation matrix
#######################################

gen_W <- function(){
  
  #Generate Lambda Matrix first
  
  Lambda <- matrix(
    data = rnorm(N*4, 0, 1),
    nrow = N, ncol = 4
  )
  
  #Use Lambda to create B matrix
  
  B <- (Lambda) %*% t(Lambda)
  B <- B + 1/10*diag(nrow = nrow(B))
  
  #Turn B into a correlation matrix
  
  W <- diag(B)^(-1/2)*B*diag(B)^(-1/2)
  return(W)
}

#########################
##Generate C_hat
#########################
#This builds in the correlation from W into original C_bar
#This already calls gen_W and does everything for you, but you need to specify a C_bar

gen_C_hat <- function(C_bar){
  W <- gen_W()
  
  stock_dim <- paste0("stock_", c(1:N))
  c_dim <- paste0("c_", c(1:P_c))
  time_dim <- paste0("time_", c(1:(Time+1)))
  
  C_hat <- array(data = 0, dim = c(N, P_c, Time+1), dimnames = list(stock_dim, c_dim, time_dim))

  for (t in 1:(Time+1)) {
    C_hat[, , t] <- W %*% C_bar[, , t]
  }
  
  return(C_hat)
}

##################################
##Generate final "observed" C
##################################

#This function "observes" characteristics by normalizing them within (-1, 1) via the rank transformation
#Takes any sort of C matrix and normalizes them via rank trasnformation

gen_C <- function(C_matrix){
  
  stock_dim <- paste0("stock_", c(1:N))
  c_dim <- paste0("c_", c(1:P_c))
  time_dim <- paste0("time_", c(1:(Time+1)))
  
  C <- array(data = 0, dim = c(N, P_c, Time+1), dimnames = list(stock_dim, c_dim, time_dim))
  
  for (t in 1:(Time+1)) {
    C[, , t] <- (2/(N*P_c+1))*
      rank(C_matrix[, , t]) - matrix(
        data = 1, nrow = N, ncol = P_c
        )
  }
  return(C)
}
```

```{r generate_characteristics}
C_bar <- gen_C_bar()
C_hat <- gen_C_hat(C_bar)
C <- gen_C(C_hat)
```

```{r function_generate_xt_multivariate}
########################
##xt set up, original multivariate specification in proposal
########################

########################
##Specify A Matrices
########################

A1 <- matrix(c(
  0.95, 0, 0,
  0, 0.95, 0,
  0, 0, 0.95),
  nrow = 3, ncol = 3
)

A2 <- matrix(c(
  1, 0, 0.25,
  0, 0.95, 0,
  0.25, 0, 0.95),
  nrow = 3, ncol = 3
)

A3 <- matrix(c(
  0.99, 0.2, 0.1,
  0.2, 0.90, -0.3,
  0.1, -0.3, -0.99),
  nrow = 3, ncol = 3
)

###################################
###Function to Generate xt series
###################################
#xt is a multivariate time series with 3 different series
#Generates xt series, gien A matrix specification

gen_xt <- function(A){
  
  xt <- data.frame(x1 = c(0:Time),
                   x2 = c(0:Time),
                   x3 = c(0:Time)
  )
  
  x_dim <- paste0("x_", 1:3)
  time_dim <- paste0("time_", c(0:(Time)))
  
  xt <- array(0, dim = c(1, 3, Time+1), dimnames = list(c(row), x_dim, time_dim))
  
  Axt <- xt
  for (t in 2:(Time+1)) {
    ut <- rnorm(3, mean = 0, sd = 1)
    Axt[, , t] <- Axt[, , t-1] + ut
  }
  Axt <- Axt[, , -1]
  return(Axt)
}

#########################################
##Function to plot xt
#########################################

#Transforms xt series to wide format and plots it

plot_xt <- function(xt){
  
  xt_tidy <- cbind(t = c(1:(Time)), xt)

  xt_tidy <- xt_tidy %>%
    gather(series, value, -t)
  
  ggplot(data = xt_tidy) +
    geom_line(aes(x = t, y = value, colour = series))
}

xt <- gen_xt(A1)

```

```{r function_univariate_xt}
#xt function, just for simplicity

gen_xt_univariate <- function(){
  #Initialize
  xt <- rep(0, Time+1)
  rho <- 0.95
  ut_sd <- sqrt(1 -rho^2)
  
  xt[1] <- rnorm(1, mean = 0, sd = ut_sd)
  
  for (t in 2:(Time+1)) {
    xt[t] <- xt[t-1]*rho + rnorm(1, mean = 0, sd = ut_sd)
  }
  
  return(matrix(data = xt, ncol = 1))
}

xt_univariate <- gen_xt_univariate()

```

```{r function_g}
############################
##Different g() functions
############################

#Logit function, used in some specified structures

logit <- function(x){
  (1 + exp(-x))^(-1)
}

#Remmeber that theta needs to be tuned later. Set up all of the functions as functions of theta

#Code these up so that they can take either the trivariate xt or univariate xt series

##############################
#g1
##############################

#Default theta, 1x3 dimensions
theta <- matrix(c(0.02, 0.02, 0.02), nrow = 1)

g1 <- function(C, x, i, t, theta){
  #Univariate xt case
  if (ncol(x) == 1) {
    matrix(c(
      C[i, 1, t], C[i, 2, t], C[i, 3, t] * x[t]
      ), nrow = 1) %*% t(theta)
  } 
  #Multivariate Case
  else {
    matrix(c(
      C[i, 1, t], C[i, 2, t], C[i, 3, t] * t(x[3, t])
      ), nrow = 1) %*% t(theta)
  }
}

#test if working
g1(C, xt_univariate, 1, 1, theta)
g1(C, xt, 1, 1, theta)

##############################
#g2
##############################

#Default theta, 1x3 dimensions
theta <- matrix(c(0.04, 0.035, 0.01), nrow = 1)

g2 <- function(C, x, i, t, theta){
  #Univariate Case
  if (ncol(x) == 1) {
    matrix(c(
      C[i, 1, t]^2, 
      C[i, 1, t]*C[i, 2, t],
      sign(
        C[i, 3, t] * x[t]
        )
    ), nrow = 1) %*% t(theta)
  }
  #Multivariate Case
  else {
    matrix(c(
      C[i, 1, t]^2, 
      C[i, 1, t]*C[i, 2, t],
      sign(
        C[i, 3, t] * t(x[3, t])
        )
    ), nrow = 1) %*% t(theta)
  }
}

#test if working
g2(C, xt_univariate, 1, 1, theta)
g2(C, xt, 1, 1, theta)

#############################
#g3
#############################

#Default theta, 1x4 dimensions
theta <- matrix(c(0.04, 0.035, 0.01, 0.01), nrow = 1)

g3 <- function(C, x, i, t, theta){
  #g3 does not care about xt series. Should work regardless
  matrix(
    c(
      (C[i, 1, t] > 0), 
      C[i, 2, t]^3,
      C[i, 1, t] * C[i, 2, t] * (C[i, 3, t] > 0),
      logit(C[i, 3, t])
      ),
    nrow = 1) %*% t(theta)
}

#test if working
g3(C, xt, 1, 1, theta)

#################################
#g4
#Remember that we are supposed to pass C hat through to g4
#################################

#Default theta, 1x3 dimensions
theta <- matrix(c(0.02, 0.02, 0.02), nrow = 1)

g4 <- function(C, x, i, t, theta){
  #Univariate
  if (ncol(x) == 1) {
    matrix(c(
      C[i, 1, t],
      C[i, 2, t],
      C[i, 3, t] * x[t]
      ), nrow = 1
    ) %*% t(theta)
  } 
  #Multivariate
  else {
    matrix(c(
      C[i, 1, t],
      C[i, 2, t],
      C[i, 3, t] * t(x[3, t])
      ), nrow = 1
    ) %*% t(theta)
  }
}

#test if working
g4(C_hat, xt_univariate, 1, 1, theta)
g4(C_hat, xt, 1, 1, theta)

#################################

#################################
## Generate an entire cross section of g(), given C, x and theta
## Will be useful later
#################################

gen_g_panel <- function(g_function, C, x, theta) {
  g_panel <- array(0, dim = c(N, 1, Time))
  for (i in 1:N) {
    for (t in 1:Time) {
      g_panel[i, 1, t] <- g_function(C, x, i, t, theta)
    }
  }
  return(g_panel)
}

#It works, yay
gen_g_panel(g1, C, xt, theta)

##################################
## Generate true factors for entire panel
## Will be used for time series and cross sectional R squared later
##################################

gen_g_factor_panel <- function(g_function, C, x, theta) {
  g_factor_panel <- array(0, dim = c(N, 1, Time))
  g_name <- as.character(substitute(g_function))
  for (i in 1:N) {
    for (t in 1:Time) {
      #G1 case
      if (g_name == "g1") {
          #Univariate xt case
          if (ncol(x) == 1) {
            matrix(c(
              C[i, 1, t], C[i, 2, t], C[i, 3, t] * x[t]
              ), nrow = 1) %*% t(theta)
          } 
          #Multivariate Case
          else {
            matrix(c(
              C[i, 1, t], C[i, 2, t], C[i, 3, t] * t(x[3, t])
              ), nrow = 1) %*% t(theta)
          }
        }
      }
      #G2 case
      else if (g_name == "g2") {
        
      }
      #G3 case
      else if (g_name == "g3") {
        
      }
      #G4 case
      else{
        
      }
    }
  }
}

```

```{r function_return_equation}
##########################
##Functions for elements in return equation
##########################

#This function generates Beta*v part of the errors
#Returns it for the entire panel of data, for each stock and across all time
#Note that v here is a 3x1 vector of errors that varies across t but is constant across all stocks

gen_betav <- function(C, v_sd){
  Beta <- C[, 1:3, ]
  Beta_v <- array(0, dim = c(N, 1, Time))
  
  for (t in 1:(Time)) {
    v <- matrix(data = rnorm(3,mean=0, sd = v_sd), 
              nrow = 3, 
              ncol = 1)
    
    for (i in 1:N) {
      Beta_v[i, 1, t] <- Beta[i, , t] %*% v
    }
    
  }
  return(Beta_v)
}

#Test if working
#It's working
Beta_v <- gen_betav(C, v_sd = 0.05)
  
#############################
##Error Structure
#############################

##Specify omega, gamma and w
omega <- -0.736
gamma <- 0.90
w <- sqrt(0.363)
##

#############################
##Function to generate errors (e)
#############################

#Modified this function that that you can specify which parts of the error you wish to turn on or off

#Gu's specification just had student t errors
#Our specification in the proposal had SV errors (much more busier)

#It takes sv, a logical value with 1 = sv errors, and 0 = standard student t errors

#This will return an array containing the entire panel of errors
#i.e. the errors for each i, and across all times
#Error itself only has one dimension, so it will be in [i, 1, t] format

gen_error <- function(sv, 
                      #epsilon sd. Note that this is the normal sd in sv case, 
                      #and student t sd in simple case
                      ep_sd,
                      #sv parameters
                      omega, gamma, w){
  #Initialize
  error <- array(data = 0, dim = c(N, 1, Time))
  ###########
  #SV errors
  ###########
  if (sv == 1) {
    ##Generate Sigma first (only indexed by time)
    sigma2 <- rep(0, Time+1)
    #Initial sigma2
    sigma2[1] <- omega + w
    
    for (t in 1:Time+1) {
      sigma2[t+1] <- omega + gamma*sigma2[t] + rnorm(1, 0, w)
    }
    
    for (t in 1:(Time)) {
      for (i in 1:N) {
        error[i, 1, t] <- exp(sigma2[t+1]/2) * rnorm(1, 0, ep_sd)
      }
    }
    return(error)
  }
  ##################
  #Student t errors
  ##################
  else {
    for (t in 1:(Time+1)) {
      error[, , t] <- matrix(data = rt(N, df = 5) * sqrt(ep_sd^2 * (5-2)/5), nrow = N)
    }
    return(error)
  }
}

#SV version check
error <- gen_error(sv = 1, 0.05, omega = -0.736, gamma = 0.90, w = sqrt(0.363))
#Non-SV version check
#You'll still need to specify values for omega, gamma and w, they just won't be used
error <- gen_error(sv = 0, 0.05, omega = -0.736, gamma = 0.90, w = sqrt(0.363))

#Bind panel of g() and errors together

g1_panel <- gen_g_panel(g1, C, xt, theta)
error_sv <- gen_error(sv = 1, 0.05, omega = -0.736, gamma = 0.90, w = sqrt(0.363))

rt_panel <- g1_panel + error_sv

```

```{r tune_rsquared, eval = FALSE}
#Nuked this shit, sub in David's code

#See Cochrane's book for more details and explanations, Chapter 12

# Absolutely no clue how Gu et al calibrated their R squared values
# But, their average time series R squared for each stock was 50%, average annualized volatility was 30%
# Cross sectional R squared was 25%, and predictive R squared was 5%.

##########################################
# Time series R-squared
##########################################

## Methodology 

# Run a standard regression for expected returns
# R_it = alpha_i + Beta_i * factor_t + e_it
# Save the coefficients for Beta from this, labelling them as Lambda
# Calculate the SSR and SST, and hence R squared for each stock return's time series

#Initialize R squared matrix
Rsquared <- matrix(0, N, 1)
SSR <- matrix(0, N, 1)
SST <- matrix(0, N, 1)
Fits <- matrix(0, (Time+1), N)
Coeff_Betas <- matrix(0, N, 3)
Xs_bar <- matrix(0, N, 3)
R_bar <- matrix(0, N, 1)R
Resids <- matrix(0, (Time+1), N)

#Done for the g1 case for now

for (i in 1:N){
  
  #True Returns
  
  Rs<- rt[i, ]
  R_bar[i, 1]<-mean(Rs)
  
  #True factor set
  
  Xs <- cbind(t(C[i, 1:2, ]),(as.matrix(C[i, 3, ]*as.matrix(x[, 1]))))
  
  #Not too sure what this is doing, not used anywhere
  Xs_bar[i, ]<- colMeans(Xs)
  
  df<-data.frame(Rs, Xs)
  
  fit<-lm(Rs ~., df)
  Fits[, i]<- fit$fitted.values
  
  Coeff_Betas[i, ] <- as.numeric(fit$coefficients[2:4])
  
  Resids[, i]<- fit$residuals
  SSR[i, 1]<- sum(Resids[, i]^2)
  SST[i, 1]<- sum((Rs-mean(Rs))^2)
  
  
  Rsquared[i, 1]<-summary(fit)$r.squared
}

mean(Rsquared)

val <- g1(C,x,theta)
true_res <- rowSums((rt - val)^2)
true_sst <- rowSums((rt - rowMeans(rt))^2)
true_rRsquared <- 1 - (sum(true_res)/sum(true_sst))

fitted_Rsquared <- 1 - (sum(SSR)/sum(SST))

##########################################
# Cross-sectional R-squared
##########################################

#Run a cross sectional regression of average returns on the estimated betas from before

# E(Rei) = alpha_i (constant) + B_i * lambda

R_bar <- rowMeans(rt)
df_new <- data.frame(R_bar, Coeff_Betas) 

cross_fit <- lm(R_bar~ Coeff_Betas, df_new) 
summary(cross_fit)

# This gives us the cross sectional R squared

```


```{r generate_returns}
##############################
##Generate Return Series
##############################

```

```{r function_build_predictor}
##############################################################################
##Function to build predictor set for the entire panel
##############################################################################

#Kronecker product thing

# Calculated with z_it = (1, xt)' \otimes c_it

gen_predictor_z <- function(C, xt){
  
}

#First strap on a vector of 1 to the xt set
xt_set <- cbind(1, xt)

#This works, dimnames need to be cleaned up back in the function to generate C though
#Pretty enough as it is
#Kronecker will return an array
z1 <- kronecker(t(xt_set[1,]), C[1, , 1], make.dimnames = TRUE)
z2 <- kronecker(t(xt_set[2,]), C[2, , 1], make.dimnames = TRUE)



```

```{r}

#We need to take all of those arrays and convert them into a panel data format

#Our P_x is 1 constant + 3 multivariate time series = 4
#Hence, ncol = 4*P_c

gen_z <- function(){
  null_matrix <- matrix(0, nrow = Time+1, ncol = 4*P_c)

  z <- rep(list(null_matrix), N)
  
  xt <- gen_xt(A1)
  
  for (i in 1:N) {
    for (t in 1:(Time+1)){
      z[[i]][t, ] <- t(
        kronecker(t(cbind(c(rep(1, Time+1)), xt)[t, ]), C[[t]][i, ])
        )
    }
  }
  return(z)
}

############################################################################
##Function to cbind the corresponding return series and the predictor set together, and return it in dataframe form
############################################################################

gen_pooled_panel <- function(){
  
  #Initialize entire dataset dimensions
  #There should be 181*200 = 36,200 rows
  #There should be 1 return column + 1 id column + 1 time column + 400 predictors = 403 columns
  
  pooled_panel <- matrix(data = 0, nrow = (181*200), ncol = (1+1+1+400))
  
  for (i in 1:N){
    #Extract return series for ith stock from the return series list first
    #Initialize empty matrix
    r <- matrix(0, nrow = Time+1, ncol = 1)
    
    for (t in 1:(Time+1)){
        r[t] <- rt_A1_g1[[t]][i]
    }
    
    #Cbind returns, id, time and predictors
    panel <- cbind(r, c(rep(i, (Time+1))), c(1:(Time+1)), z[[i]])
    pooled_panel[((i-1)*(Time+1)+1):(i*(Time+1)), ] <- panel
  }
  
  #Pass this through to data frame format with column titles so you know which column is which
  pooled_panel <- data.frame(return = pooled_panel[, 1], stock = pooled_panel[, 2], time = pooled_panel[, 3], pooled_panel[, 4:403])
  
  #Sort by time
  pooled_panel <- pooled_panel %>%
  arrange(time)
  
  #Remove 1st row because returns data only starts from t = 2
  pooled_panel <- pooled_panel %>%
    filter(time != 1)
  
  #Return
  return(pooled_panel)
  
}
```

```{r geenrate_panel_dataset}
z <- gen_z()

pooled_panel <- gen_pooled_panel()

pooled_panel_y <- pooled_panel$return

pooled_panel_x <- pooled_panel %>%
  select(-return, -stock, -time)


```

```{r train_valid_test}
#Create Training + Test Sets

#$T = 180$ monthly periods corresponds to 15 years. The training sample was set to start from $T = 108$ or 9 years, a validation set 1 year in length. The last 3 years were reserved as a test set never to be used for validation or training.

#This means that there are only 3 sample periods to train on, yay

#Caret and other packages are not so good at dealing with custom training/validation/test sets. Have to specify resampling, tuning and testing manually (ugh)

time_slices <- createTimeSlices(1:(Time-36), initialWindow = 108, horizon = 12, fixedWindow = FALSE, skip = 11)

true_test <- list(c(121:180), c(133:180), c(145:180))

time_slices <- list.append(time_slices, true_test)

names(time_slices) <- c("train", "validation", "test")

```

```{r pooled_ols}
#POLS

#Pooled OLS does not have anything hyperparameters to tune, easy example to start off with

#Expand formula manually this way because PLM is dumb and gets confused by y ~ .

plm_formula <- function(panel){
  colnames <- colnames(pooled_panel)
  colnames <- colnames[-c(1:3)]
  f <- as.formula(c("return ~ ", paste(colnames, collapse = "+")))
  return(f)
}

f <- plm_formula(pooled_panel)

#Initialize Loss Function Statistics

POLS_stats <- data.frame(
  set = c(1:3),
  #Training
  train_MAE = c(1:3),
  train_MSE = c(1:3),
  train_RMSE = c(1:3),
  train_R2 = c(1:3),
  #Validation
  valid_MAE = c(1:3),
  valid_MSE = c(1:3),
  valid_RMSE = c(1:3),
  valid_R2 = c(1:3),
  #Test
  test_MAE = c(1:3),
  test_MSE = c(1:3),
  test_RMSE = c(1:3),
  test_R2 = c(1:3)
)

#Initialize Forecasts

POLS_forecasts <- rep(list(0), 3)

#Initialize Models

POLS_models <- rep(list(0), 3)

for (set in 1:3) {
  #Load training, validation and test sets
    train <- pooled_panel %>%
      filter(time %in% unlist(time_slices$train[set]))
    validation <- pooled_panel %>%
      filter(time %in% unlist(time_slices$validation[set]))
    test <- pooled_panel %>%
      filter(time %in% unlist(time_slices$test[set]))
    
  #Initialize forecasts
    POLS_forecasts[[set]] <- data.frame(
      time = test$time,
      forecast = rep(0, length(test$time)),
      forecast_error = rep(0, length(test$time))
    )

  #Train model on training set
  
    pols <- plm(f, data = train, model = "pooling", index = c("time", "stock"))
    POLS_models[[set]] <- pols
    
  #Usually Tuning via Validation set would be here, but POLS does not need tuning
  
  #Training Set Statistics
    POLS_stats$train_MAE[set] <- mae(train$return, predict(pols))
    POLS_stats$train_MSE[set] <- mse(train$return, predict(pols))
    POLS_stats$train_RMSE[set] <- rmse(train$return, predict(pols))
    error <- (train$return - predict(pols))
    SSR <- t(error) %*% (error)
    SST <- t(train$return - mean(train$return)) %*% (train$return - mean(train$return))
    POLS_stats$train_R2[set] <- (1 - SSR/SST)
    
  #Validation Set Statistics
    POLS_stats$valid_MAE[set] <- mae(validation$return, predict(pols, newdata = validation))
    POLS_stats$valid_MSE[set] <- mse(validation$return, predict(pols, newdata = validation))
    POLS_stats$valid_RMSE[set] <- rmse(validation$return, predict(pols, newdata = validation))
    
    error <- (validation$return - predict(pols, newdata = validation))
    SSR <- t(error) %*% (error)
    SST <- t(validation$return - mean(validation$return)) %*% (validation$return - mean(validation$return))
    POLS_stats$valid_R2[set] <- (1 - SSR/SST)
  
  #Test Set Statistics
    POLS_stats$test_MAE[set] <- mae(test$return, predict(pols, newdata = test))
    POLS_stats$test_MSE[set] <- mse(test$return, predict(pols, newdata = test))
    POLS_stats$test_RMSE[set] <- rmse(test$return, predict(pols, newdata = test))
    
    error <- (test$return - predict(pols, newdata = test))
    SSR <- t(error) %*% (error)
    SST <- t(test$return - mean(test$return)) %*% (test$return - mean(test$return))
    POLS_stats$test_R2[set] <- (1 - SSR/SST)
    
  #Forecasts
    POLS_forecasts[[set]]$forecast <- predict(pols, newdata = test)
    POLS_forecasts[[set]]$forecast_error <- test$return - predict(pols, newdata = test)
}

POLS_stats

summary(POLS_models[[3]])


```

```{r elastic_net}
#Elasticnet

#Elasticnet only has alpha to tune, thank god

elasticnet_grid <- expand.grid(
  alpha = seq(0, 1, 0.01)
)

#Initialize Loss Function Statistics

elasticnet_stats <- data.frame(
  set = c(1:3),
  #Training
  train_MAE = c(1:3),
  train_MSE = c(1:3),
  train_RMSE = c(1:3),
  train_R2 = c(1:3),
  #Validation
  valid_MAE = c(1:3),
  valid_MSE = c(1:3),
  valid_RMSE = c(1:3),
  valid_R2 = c(1:3),
  #Test
  test_MAE = c(1:3),
  test_MSE = c(1:3),
  test_RMSE = c(1:3),
  test_R2 = c(1:3)
)

#Initialize Forecasts

elasticnet_forecasts <- rep(list(0), 3)

#Initialize Models

elasticnet_models <- rep(list(0), 3)

for (set in 1:3) {
  #Load training, validation and test sets
    train <- pooled_panel %>%
      filter(time %in% unlist(time_slices$train[set]))
    train_x <- train %>%
      select(-return, -time, -stock)
    train_y <- train %>%
      select(return)
    
    validation <- pooled_panel %>%
      filter(time %in% unlist(time_slices$validation[set]))
    validation_x <- validation %>%
      select(-return, -time, -stock)
    validation_y <- validation %>%
      select(return)
    
    test <- pooled_panel %>%
      filter(time %in% unlist(time_slices$test[set]))
    test_x <- test %>%
      select(-return, -time, -stock)
    test_y <- test %>%
      select(return)
    
  #Initialize forecasts
    elasticnet_forecasts[[set]] <- data.frame(
      time = test$time,
      forecast = rep(0, length(test$time)),
      forecast_error = rep(0, length(test$time))
    )

  #Train model on training set
  
    elasticnet <- glmnet(x = train_x, y = train_y)
    POLS_models[[set]] <- pols
    
  #Usually Tuning via Validation set would be here, but POLS does not need tuning
  
  #Training Set Statistics
    POLS_stats$train_MAE[set] <- mae(train$return, predict(pols))
    POLS_stats$train_MSE[set] <- mse(train$return, predict(pols))
    POLS_stats$train_RMSE[set] <- rmse(train$return, predict(pols))
    error <- (train$return - predict(pols))
    SSR <- t(error) %*% (error)
    SST <- t(train$return - mean(train$return)) %*% (train$return - mean(train$return))
    POLS_stats$train_R2[set] <- (1 - SSR/SST)
    
  #Validation Set Statistics
    POLS_stats$valid_MAE[set] <- mae(validation$return, predict(pols, newdata = validation))
    POLS_stats$valid_MSE[set] <- mse(validation$return, predict(pols, newdata = validation))
    POLS_stats$valid_RMSE[set] <- rmse(validation$return, predict(pols, newdata = validation))
    
    error <- (validation$return - predict(pols, newdata = validation))
    SSR <- t(error) %*% (error)
    SST <- t(validation$return - mean(validation$return)) %*% (validation$return - mean(validation$return))
    POLS_stats$valid_R2[set] <- (1 - SSR/SST)
  
  #Test Set Statistics
    POLS_stats$test_MAE[set] <- mae(test$return, predict(pols, newdata = test))
    POLS_stats$test_MSE[set] <- mse(test$return, predict(pols, newdata = test))
    POLS_stats$test_RMSE[set] <- rmse(test$return, predict(pols, newdata = test))
    
    error <- (test$return - predict(pols, newdata = test))
    SSR <- t(error) %*% (error)
    SST <- t(test$return - mean(test$return)) %*% (test$return - mean(test$return))
    POLS_stats$test_R2[set] <- (1 - SSR/SST)
    
  #Forecasts
    POLS_forecasts[[set]]$forecast <- predict(pols, newdata = test)
    POLS_forecasts[[set]]$forecast_error <- test$return - predict(pols, newdata = test)
}

```

```{r random_forest}
#Random Forest

#Random Forest should also be fairly straightforward, only two tuning parameters mtry and ntree (usually)

#The vanilla randomForest package is very basic and too slow
#We will use the ranger package, which is apparently a C++ implementation of the random forest algorithm and should be faster
#Another alternative is to use XGBoost (which is very very fast) and set boosting rounds = 1, because in this case it should be a standard random forest

#COmpute power is a real struggle, and the aim is just to get the code basis working for the moment. Very conservative grids are therefore used for now

#Default ntree = 500
#Default mtry = count(predictors/3) = 400/3 = 133

random_forest_grid <- expand.grid(
  ntree = seq(10, 50, 10),
  mtry = seq(10, 100, 10)
)

#Generate formula, actually same code from PLM earlier

rf_formula <- function(panel){
  colnames <- colnames(pooled_panel)
  colnames <- colnames[-c(1:3)]
  f <- as.formula(c("return ~ ", paste(colnames, collapse = "+")))
  return(f)
}

f <- rf_formula(pooled_panel)

#Initialize Loss Function Statistics

RF_stats <- data.frame(
  set = c(1:3),
  #Training
  train_MAE = c(1:3),
  train_MSE = c(1:3),
  train_RMSE = c(1:3),
  train_R2 = c(1:3),
  #Validation
  valid_MAE = c(1:3),
  valid_MSE = c(1:3),
  valid_RMSE = c(1:3),
  valid_R2 = c(1:3),
  #Test
  test_MAE = c(1:3),
  test_MSE = c(1:3),
  test_RMSE = c(1:3),
  test_R2 = c(1:3)
)

#Initialize Forecasts

RF_forecasts <- rep(list(0), 3)

#Initialize Models

RF_models <- rep(list(0), 3)

for (set in 1:3) {
  #Load training, validation and test sets
    train <- pooled_panel %>%
      filter(time %in% unlist(time_slices$train[set]))
    validation <- pooled_panel %>%
      filter(time %in% unlist(time_slices$validation[set]))
    test <- pooled_panel %>%
      filter(time %in% unlist(time_slices$test[set]))
    
  #Initialize forecasts
    RF_forecasts[[set]] <- data.frame(
      time = test$time,
      forecast = rep(0, length(test$time)),
      forecast_error = rep(0, length(test$time))
    )

  #Train model on training set
  
    rf <- ranger(f, data = train, model = "pooling", index = c("stock", "time"))
    RF_models[[set]] <- rf
    
  #Usually Tuning via Validation set would be here, but POLS does not need tuning
  
  #Training Set Statistics
    RF_stats$train_MAE[set] <- mae(train$return, predict(rf))
    RF_stats$train_MSE[set] <- mse(train$return, predict(rf))
    RF_stats$train_RMSE[set] <- rmse(train$return, predict(rf))
    #R squared still pending, haven't figured out how to do it yet fully
    
  #Validation Set Statistics
    RF_stats$valid_MAE[set] <- mae(validation$return, predict(rf, newdata = validation))
    RF_stats$valid_MSE[set] <- mse(validation$return, predict(rf, newdata = validation))
    RF_stats$valid_RMSE[set] <- rmse(validation$return, predict(rf, newdata = validation))
    #R squared still pending, haven't figured out how to do it yet fully
  
  #Test Set Statistics
    RF_stats$test_MAE[set] <- mae(test$return, predict(rf, newdata = test))
    RF_stats$test_MSE[set] <- mse(test$return, predict(rf, newdata = test))
    RF_stats$test_RMSE[set] <- rmse(test$return, predict(rf, newdata = test))
    #R squared still pending, haven't figured out how to do it yet fully
    
  #Forecasts
    RF_forecasts[[set]]$forecast <- predict(rf, newdata = test)
    RF_forecasts[[set]]$forecast_error <- test$return - predict(rf, newdata = test)
    
}

```

```{r}
#Train RF Model + tune
rf <- ranger(f, data = train, 
             #Hyperparameters
             
             #Tuning parameters
              num.trees = 10,
              mtry = 10,
             
             #No Tuning
              #Default Min node size
              min.node.size = 5,
             
             #Other
             write.forest = TRUE,
             importance = "impurity"
             )

importance(rf)

```


```{r neural_networks}
#Neural Network

```

