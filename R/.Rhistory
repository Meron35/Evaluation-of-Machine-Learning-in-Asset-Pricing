datashare_combined <- full_join(macro_predictors, datashare_normalized, by = "time") %>%
# Create Excess Returns
mutate(rt = rt - macro_Rfree) %>%
select(-macro_Rfree) %>%
# Reorder
select(time, stock, rt, macro_constant,
macro_bm, macro_dfy, macro_dp, macro_ep, macro_ntis, macro_svar, macro_tbl, macro_tms,
everything())
interaction_formula_dummyVars <- dummyVars(interaction_formula, data = datashare_combined)
# Testing if it works
final_dataset_x <- (predict(interaction_formula_dummyVars, datashare_combined))
# Strap on the stock, time and rt columns to this
final_dataset <- data.frame(stock = datashare_combined$stock,
time = datashare_combined$time,
rt = datashare_combined$rt,
final_dataset_x)
rm(final_dataset_x)
## Setting up time slices, formulas, etc
## time slices function
## Different function needed due to different time format (inclusion of quarters)
## Makes it so that it works with all of our model fitting function from earlier
realdata_custom_timeslices <- function(start, initialWindow, horizon, validation_size, test_size, set_no) {
time_slice <- list(train = 0, validation = 0, test = 0)
time_slices <- rep(list(time_slice), set_no)
for (t in 1:set_no) {
time_slice$train <- seq(start,
(start + initialWindow + (t-1) * horizon + 3/4),
0.25)
time_slice$validation <- seq(
(start + initialWindow + (t-1) * horizon + 1),
(start + initialWindow + (t-1) * horizon + validation_size + 3/4),
0.25
)
time_slice$test <- seq(
(start + initialWindow + (t-1) * horizon + validation_size + 1),
(start + (initialWindow + (t-1) * horizon) + validation_size + test_size + 3/4),
0.25
)
time_slices[[t]] <- time_slice
}
time_slices
}
# Formula
real_panel_formula <- function(panel){
#Remove the first 3 colNames, as these correspond to the return, time and stock id
panel_colnames <- colnames(panel)[-c(1:3)]
f <- as.formula(c("rt ~", paste(panel_colnames, collapse = "+")))
return(f)
}
# These are the settings working with a dataset going from 1993 - 2016
# Results in a traing:validation size ratio of 1.5
realdata_timeSlices <- realdata_custom_timeslices(
start = 1994, initialWindow = 11, horizon = 1, validation_size = 8, test_size = 1, set_no = 3
)
# Add the two quarters in from 1993
for(i in 1:3) {
realdata_timeSlices[[i]]$train <- c(1993.5, 1993.75, realdata_timeSlices[[i]]$train)
}
##################################################################
## Function to conduct variable importance for real data
# This is slightly different from the functions used in the simulated context, as we do not care about the importance of ALL interaction terms
# Use the same function names as the simulated ones for simplicity, MAKE SURE YOU RUN THIS BEFORE FITTING EMPIRICAL DATA
# General plan:
# Use grepl & partial matching to match which columns contain which factor columns contain which factor
# Much more straightforward than generating a new panel containing a new set of interaction terms
# Slight problem, some of the column names can "overlap" E.g. bm and bm_ia
# Column names were changed is data prep step to get around this
################################################
# Linear Model
LM_variable_importance <- function(test, lm_model,
macro_factor_names, individual_factor_names) {
# -1 to get rid of macro_constant
all_factor_names <- c(macro_factor_names, individual_factor_names)[-1]
test_x <- test[4:ncol(test)]
# Specify .packages = "quantreg" here as it seems it isn't supported and therefore is missed by doFuture
variable_importance_df <- foreach(i = (1:(length(all_factor_names))), .combine = "rbind", .packages = c("speedglm", "quantreg")) %dopar% {
test_x_zero <- test_x
zero_indices <- which(str_detect(colnames(test_x_zero), all_factor_names[i]) == 1)
test_x_zero[, c(zero_indices)] <- 0
original_R2 <- R2(predict(lm_model, newdata = test_x), test$rt, form = "traditional")
new_R2 <- R2(predict(lm_model, newdata = test_x_zero), test$rt, form = "traditional")
variable_importance <- data.frame(variable = all_factor_names[i], importance = (original_R2 - new_R2))
variable_importance
}
variable_importance_df
}
# Penalized Linear Model
ELN_variable_importance <- function(test, eln_model, alpha, lambda,
macro_factor_names, individual_factor_names) {
all_factor_names <- c(macro_factor_names, individual_factor_names)[-1]
test_x <- as.matrix(test[4:ncol(test)])
variable_importance_df <- foreach(i = (1:(length(all_factor_names))), .combine = "rbind",
.packages = "hqreg") %dopar% {
test_x_zero <- test_x
zero_indices <- which(str_detect(colnames(test_x_zero), all_factor_names[i]) == 1)
test_x_zero[, zero_indices] <- 0
original_R2 <- R2(predict(eln_model, test_x, alpha = alpha, lambda = lambda), test$rt, form = "traditional")
new_R2 <- R2(predict(eln_model, test_x_zero, alpha = alpha, lambda = lambda), test$rt, form = "traditional")
variable_importance <- data.frame(variable = all_factor_names[i], importance = (original_R2 - new_R2))
variable_importance
}
variable_importance_df
}
# Random Forest
RF_variable_importance <- function(test, rf_model,
macro_factor_names, individual_factor_names) {
all_factor_names <- c(macro_factor_names, individual_factor_names)[-1]
test_x <- test[4:ncol(test)]
variable_importance_df <- foreach(i = (1:(length(all_factor_names))), .combine = "rbind", .packages = "randomForestSRC") %do% {
test_x_zero <- test_x
zero_indices <- which(str_detect(colnames(test_x_zero), all_factor_names[i]) == 1)
test_x_zero[, zero_indices] <- 0
original_R2 <- R2(predict(rf_model, newdata = test_x)$predicted, test$rt, form = "traditional")
new_R2 <- R2(predict(rf_model, newdata = test_x_zero)$predicted, test$rt, form = "traditional")
variable_importance <- data.frame(variable = all_factor_names[i], importance = (original_R2 - new_R2))
variable_importance
}
variable_importance_df
}
# Neural Network
# Unfortunately it seems that the tensorflow/keras backend does not work with foreach, so a standard for loop is needed instead
NNet_variable_importance <- function(test, nnet_model,
macro_factor_names, individual_factor_names) {
all_factor_names <- c(macro_factor_names, individual_factor_names)[-1]
test_x <- test[4:ncol(test)]
variable_importance_df <- foreach(i = (1:(length(all_factor_names))), .combine = "rbind",
.packages = c("keras", "tensorflow", "reticulate")) %do% {
test_x_zero <- test_x
zero_indices <- which(str_detect(colnames(test_x_zero), all_factor_names[i]) == 1)
test_x_zero[, zero_indices] <- 0
original_R2 <- R2(predict(nnet_model, as.matrix(test_x)), test$rt, form = "traditional")
new_R2 <- R2(predict(nnet_model, as.matrix(test_x_zero)), test$rt, form = "traditional")
variable_importance <- data.frame(variable = all_factor_names[i], importance = (original_R2 - new_R2))
variable_importance
}
variable_importance_df
}
# Forecasting Variable Importance Metrics
## Linear Model
# Given a an lm object and a test dataset, return the cross sectional average forecast errors
lm_ave_forecast_resids <- function(lm_model, test) {
time_periods <- unique(test$time)
# Set .combine to "c" to return a vector of results, which is what we want
ave_forecast_resids_vector <- foreach(t = 1:length(time_periods), .combine = "c", .packages = c("speedglm", "quantreg")) %do% {
test_cross_section <- test %>%
filter(time == time_periods[t])
residuals <- test_cross_section$rt - predict(lm_model, newdata = test_cross_section)
mean(residuals)
}
ave_forecast_resids_vector
}
## Elastic Net
eln_ave_forecast_resids <- function(eln_model, test, alpha, lambda) {
time_periods <- unique(test$time)
# Set .combine to "c" to return a vector of results, which is what we want
ave_forecast_resids_vector <- foreach(t = 1:length(time_periods), .combine = "c") %do% {
test_cross_section <- test %>%
filter(time == time_periods[t])
test_cross_section_x <- as.matrix(test_cross_section[4:ncol(test_cross_section)])
residuals <- test_cross_section$rt - predict(eln_model, test_cross_section_x,
alpha = alpha, lambda = lambda)
mean(residuals)
}
ave_forecast_resids_vector
}
## Random Forest
rf_ave_forecast_resids <- function(rf_model, test) {
time_periods <- unique(test$time)
# Set .combine to "c" to return a vector of results, which is what we want
ave_forecast_resids_vector <- foreach(t = 1:length(time_periods), .combine = "c") %do% {
test_cross_section <- test %>%
filter(time == time_periods[t])
residuals <- test_cross_section$rt - predict(rf_model, newdata = test_cross_section)$predicted
mean(residuals)
}
ave_forecast_resids_vector
}
## Neural Networks
nnet_ave_forecast_resids <- function(nnet_model, test) {
time_periods <- unique(test$time)
# Set .combine to "c" to return a vector of results, which is what we want
ave_forecast_resids_vector <- foreach(t = 1:length(time_periods), .combine = "c") %do% {
test_cross_section <- test %>%
filter(time == time_periods[t])
test_cross_section_x <- test_cross_section[4:ncol(test_cross_section)]
residuals <- test_cross_section$rt - (nnet_model %>% predict(as.matrix(test_cross_section_x)))
mean(residuals)
}
ave_forecast_resids_vector
}
# Mostly the same as the simulation ones, but slightly different due to different variable importance schemes
##############################################################################################################
LM_fit <- function(pooled_panel, timeSlices, loss_function, f,
macro_factor_names, individual_factor_names) {
#Initialize Loss Function Statistics
LM_stats <- rep(list(0), 3)
for (set in 1:3) {
LM_stats[[set]] <- list(loss_stats = data.frame(train_MAE = 0, train_MSE = 0, train_RMSE = 0, train_RSquare = 0,
validation_MAE = 0, validation_MSE = 0, validation_RMSE = 0, validation_RSquare = 0,
test_MAE = 0, test_MSE = 0, test_RMSE = 0, test_RSquare = 0),
#Other useful things
forecast_resids = 0,
model = 0,
#Variable Importance
variable_importance = 0)
#Load Training, validation and test sets
train <- pooled_panel %>%
filter(time %in% timeSlices[[set]]$train)
validation <- pooled_panel %>%
filter(time %in% timeSlices[[set]]$validation)
test <- pooled_panel %>%
filter(time %in% timeSlices[[set]]$test)
#Train Model on training set
# Set model = FALSE so that it won't save a copy of the training data in the model object
# Doen for memory efficiency
#MSE case
if (loss_function == "mse") {
lm <- lm(f, data = train, model = FALSE, x = FALSE, y = FALSE)
} else {
# Use pfn as method here for much faster computation
lm <- rq(f, data = train, tau = 0.5, method = "pfn", eps = 1e-03)
}
#LM_stats[[set]]$model <- lm
#No Tuning Needed
#Statistics
#Training Set
train_predict <- predict(lm, newdata = train)
LM_stats[[set]]$loss_stats$train_MAE <- mae(train$rt, train_predict)
LM_stats[[set]]$loss_stats$train_MSE <- mse(train$rt, train_predict)
LM_stats[[set]]$loss_stats$train_RMSE <- rmse(train$rt, train_predict)
LM_stats[[set]]$loss_stats$train_RSquare <- R2(train_predict, train$rt, form = "traditional")
#Validation Set Statistics
validation_predict <- predict(lm, newdata = validation)
LM_stats[[set]]$loss_stats$validation_MAE <- mae(validation$rt, validation_predict)
LM_stats[[set]]$loss_stats$validation_MSE <- mse(validation$rt, validation_predict)
LM_stats[[set]]$loss_stats$validation_RMSE <- rmse(validation$rt, validation_predict)
LM_stats[[set]]$loss_stats$validation_RSquare <- R2(validation_predict, validation$rt, form = "traditional")
#Test Set Statistics
test_predict <- predict(lm, newdata = test)
LM_stats[[set]]$loss_stats$test_MAE <- mae(test$rt, test_predict)
LM_stats[[set]]$loss_stats$test_MSE <- mse(test$rt, test_predict)
LM_stats[[set]]$loss_stats$test_RMSE <- rmse(test$rt, test_predict)
LM_stats[[set]]$loss_stats$test_RSquare <- R2(test_predict, test$rt, form = "traditional")
#Forecast Residuals
LM_stats[[set]]$forecast_resids <- lm_ave_forecast_resids(lm_model = lm, test = test)
#Variable Importance
LM_stats[[set]]$variable_importance <- LM_variable_importance(test, lm,
macro_factor_names, individual_factor_names)
}
return(LM_stats)
}
###################################################################################################################
ELN_model_grid <- function(alpha_grid, train_x, train_y, validation_x, validation_y, loss_function, nlamb) {
ELN_model_grid_list <- foreach(i = (1:length(alpha_grid))) %dopar% {
ELN_model_grid <- list(ELN_grid = 0, model = 0)
#MSE Case
if (loss_function == "mse") {
ELN <- hqreg(train_x, train_y, method = "ls", alpha = alpha_grid[i], nlambda = nlamb)
ELN_model_grid$model <- ELN
ELN_model_grid$ELN_grid <- cbind(alpha = alpha_grid[i], lambda = ELN$lambda,
train_loss = apply(predict(ELN, train_x), 2, mse, actual = train_y),
validation_loss = apply(predict(ELN, validation_x), 2, mse, actual = validation_y))
} else {
#MAE Case
ELN <- hqreg(train_x, train_y, method = "quantile", tau = 0.5, alpha = alpha_grid[i], nlambda = nlamb)
ELN_model_grid$model <- ELN
ELN_model_grid$ELN_grid <- cbind(alpha = alpha_grid[i], lambda = ELN$lambda,
train_loss = apply(predict(ELN, train_x), 2, mse, actual = train_y),
validation_loss = apply(predict(ELN, validation_x), 2, mae, actual = validation_y))
}
ELN_model_grid
}
ELN_model_grid_list
}
# Given a list containing the model grid, return the "best" model according to validation loss
# Use in conjunction with previous function
get_ELN_best_tune <- function(model_grid) {
ELN_tune_dataframe <- cbind(model_grid[[1]]$ELN_grid, list_index = 1)
for (i in 2:length(model_grid)) {
ELN_tune_dataframe <- rbind(ELN_tune_dataframe, cbind(model_grid[[i]]$ELN_grid, list_index = i))
}
ELN_tune_dataframe <- data.frame(ELN_tune_dataframe)
return(ELN_tune_dataframe[which.min(ELN_tune_dataframe$validation_loss), ])
}
ELN_fit_stats <- function(alpha_grid, nlamb, timeSlices, pooled_panel, loss_function,
macro_factor_names, individual_factor_names) {
ELN_stats <- rep(list(0), 3)
for (set in 1:3) {
#Load Training, validation and test sets
train <- pooled_panel %>%
filter(time %in% timeSlices[[set]]$train)
validation <- pooled_panel %>%
filter(time %in% timeSlices[[set]]$validation)
test <- pooled_panel %>%
filter(time %in% timeSlices[[set]]$test)
train_x <- as.matrix(train[4:ncol(train)])
train_y <- as.matrix(train$rt)
validation_x <- as.matrix(validation[4:ncol(validation)])
validation_y <- as.matrix(validation$rt)
test_x <- as.matrix(test[4:ncol(test)])
test_y <- as.matrix(test$rt)
#Get models fit over the grid of hyperparameters
ELN_model_grid <- ELN_model_grid(alpha_grid, train_x, train_y, validation_x, validation_y, loss_function, nlamb = nlamb)
#Get the best tuning parameters
best_model_params <- get_ELN_best_tune(ELN_model_grid)
#Initialize stats list
ELN_stats[[set]] <- list(loss_stats = data.frame(train_MAE = 0, train_MSE = 0, train_RMSE = 0, train_RSquare = 0,
validation_MAE = 0, validation_MSE = 0, validation_RMSE = 0, validation_RSquare = 0,
test_MAE = 0, test_MSE = 0, test_RMSE = 0, test_RSquare = 0),
#Other useful things
forecast_resids = 0,
model = 0,
hyperparameters = 0,
variable_importance = 0)
#Model
model <- ELN_model_grid[[best_model_params$list_index]]$model
#ELN_stats[[set]]$model <- model
#Hyperparameters
ELN_stats[[set]]$hyperparameters <- best_model_params
#Loss Stats Dataframe
#Train
train_predict <- predict(model, train_x, alpha = best_model_params$alpha, lambda = best_model_params$lambda)
ELN_stats[[set]]$loss_stats$train_MAE <- mae(train_y, train_predict)
ELN_stats[[set]]$loss_stats$train_MSE <- mse(train_y, train_predict)
ELN_stats[[set]]$loss_stats$train_RMSE <- rmse(train_y, train_predict)
ELN_stats[[set]]$loss_stats$train_RSquare <- (1 - sse(train_y, train_predict) / sum((train_y - mean(train_y))^2))
#Validation
valid_predict <- predict(model, validation_x, alpha = best_model_params$alpha, lambda = best_model_params$lambda)
ELN_stats[[set]]$loss_stats$validation_MAE <- mae(validation_y, valid_predict)
ELN_stats[[set]]$loss_stats$validation_MSE <- mse(validation_y, valid_predict)
ELN_stats[[set]]$loss_stats$validation_RMSE <- rmse(validation_y, valid_predict)
ELN_stats[[set]]$loss_stats$validation_RSquare <- (1 - sse(validation_y, valid_predict) / sum((validation_y - mean(validation_y))^2))
#Test
test_predict <- predict(model, test_x, alpha = best_model_params$alpha, lambda = best_model_params$lambda)
ELN_stats[[set]]$loss_stats$test_MAE <- mae(test_y, test_predict)
ELN_stats[[set]]$loss_stats$test_MSE <- mse(test_y, test_predict)
ELN_stats[[set]]$loss_stats$test_RMSE <- rmse(test_y, test_predict)
ELN_stats[[set]]$loss_stats$test_RSquare <- (1 - sse(test_y, test_predict) / sum((test_y - mean(test_y))^2))
#Forecast residuals
ELN_stats[[set]]$forecast_resids <- eln_ave_forecast_resids(eln_model = model, test,
# Hyperparameters
alpha = best_model_params$alpha,
lambda = best_model_params$lambda)
#Variable Importance
ELN_stats[[set]]$variable_importance <- ELN_variable_importance(test, model,
alpha = best_model_params$alpha, lambda = best_model_params$lambda,
macro_factor_names, individual_factor_names)
}
return(ELN_stats)
}
################################################################################################################################
RF_fit_model_grid <- function(f, train, validation, RF_grid, loss_function) {
#Initialize List
RF_model_grid <- rep(list(0), nrow(RF_grid))
for (i in 1:nrow(RF_grid)) {
RF_model_grid[[i]] <- list(RF_grid = 0, model = 0)
#MSE Case
if (loss_function == "mse") {
RF <- rfsrc(f, train,
#Hyperparameters
ntree = RF_grid$ntree[i],
mtry = RF_grid$mtry[i],
nodesize = RF_grid$nodesize[i],
splitrule = "mse"
)
#RF_model_grid[[i]]$model <- RF
RF_model_grid[[i]]$RF_grid <- cbind(RF_grid[i, ],
#Train Loss
train_loss = mse(train$rt, predict(RF, newdata = train)$predicted),
#Validation Loss
validation_loss = mse(validation$rt, predict(RF, newdata = validation)$predicted)
)
} else {
#MAE Case
RF <- rfsrc(f, train,
#Hyperparameters
ntree = RF_grid$ntree[i],
mtry = RF_grid$mtry[i],
nodesize = RF_grid$nodesize[i],
splitrule = "quantile.regr",
prob = 0.5
)
#RF_model_grid[[i]]$model <- RF
RF_model_grid[[i]]$RF_grid <- cbind(RF_grid[i, ],
#Train Loss
train_loss = mae(train$rt, predict(RF)$predicted),
#Validation Loss
validation_loss = mae(validation$rt, predict(RF, newdata = validation)$predicted)
)
}
}
return(RF_model_grid)
}
#Returns the dataframe row containing the "best" hyperparameters
get_RF_best_tune <- function(RF_model_grid) {
RF_tune_grid <- RF_model_grid[[1]]$RF_grid
for (i in 2:length(RF_model_grid)) {
RF_tune_grid <- rbind(RF_tune_grid, RF_model_grid[[i]]$RF_grid)
}
return(RF_tune_grid[which.min(RF_tune_grid$validation_loss), ])
}
RF_fit_stats <- function(pooled_panel, RF_grid, timeSlices, loss_function, f,
macro_factor_names, individual_factor_names) {
#Initialize
RF_stats <- rep(list(0), 3)
#Load training, validation and test sets
for (set in 1:3) {
RF_stats[[set]] <- list(loss_stats = data.frame(train_MAE = 0, train_MSE = 0, train_RMSE = 0, train_RSquare = 0,
validation_MAE = 0, validation_MSE = 0, validation_RMSE = 0, validation_RSquare = 0,
test_MAE = 0, test_MSE = 0, test_RMSE = 0, test_RSquare = 0),
#Other useful things
forecast_resids = 0,
model = 0,
hyperparameters = 0,
variable_importance = 0, vimp = 0)
train <- pooled_panel %>%
filter(time %in% timeSlices[[set]]$train)
validation <- pooled_panel %>%
filter(time %in% timeSlices[[set]]$validation)
test <- pooled_panel %>%
filter(time %in% timeSlices[[set]]$test)
#Fit on training Set over grid of hyperparameters
model_grid <- RF_fit_model_grid(f, train, validation, RF_grid, loss_function)
#Get the best hyperparameters
best_model_params <- get_RF_best_tune(model_grid)
RF_stats[[set]]$hyperparameters <- best_model_params
#Compute the optimal model
if (loss_function == "mse") {
model <- rfsrc(f, train,
#Hyperparameters
ntree = best_model_params$ntree,
mtry = best_model_params$mtry,
nodesize = best_model_params$nodesize,
splitrule = "mse",
bootstrap = "by.root", samptype = "swr",
forest = TRUE
)
} else {
model <- rfsrc(f, train,
#Hyperparameters
ntree = best_model_params$ntree,
mtry = best_model_params$mtry,
nodesize = best_model_params$nodesize,
splitrule = "quantile.regr",
prob = 0.5,
bootstrap = "by.root", samptype = "swr",
forest = TRUE
)
}
#RF_stats[[set]]$model <- model
#Train
train_predict <- predict(model, train)$predicted
RF_stats[[set]]$loss_stats$train_MAE <- mae(train$rt, train_predict)
RF_stats[[set]]$loss_stats$train_MSE <- mse(train$rt, train_predict)
RF_stats[[set]]$loss_stats$train_RMSE <- rmse(train$rt, train_predict)
RF_stats[[set]]$loss_stats$train_RSquare <- (1 - sse(train$rt, train_predict) / sum((train$rt - mean(train$rt))^2))
#Validation
valid_predict <- predict(model, newdata = validation)$predicted
RF_stats[[set]]$loss_stats$validation_MAE <- mae(validation$rt, valid_predict)
RF_stats[[set]]$loss_stats$validation_MSE <- mse(validation$rt, valid_predict)
RF_stats[[set]]$loss_stats$validation_RMSE <- rmse(validation$rt, valid_predict)
RF_stats[[set]]$loss_stats$validation_RSquare <- (1 - sse(validation$rt, valid_predict) / sum((validation$rt - mean(validation$rt))^2))
#Test
test_predict <- predict(model, newdata = test)$predicted
RF_stats[[set]]$loss_stats$test_MAE <- mae(test$rt, test_predict)
RF_stats[[set]]$loss_stats$test_MSE <- mse(test$rt, test_predict)
RF_stats[[set]]$loss_stats$test_RMSE <- rmse(test$rt, test_predict)
RF_stats[[set]]$loss_stats$test_RSquare <- (1 - sse(test$rt, test_predict) / sum((test$rt - mean(test$rt))^2))
#Forecast residuals
RF_stats[[set]]$forecast_resids <- rf_ave_forecast_resids(rf_model = model, test = test)
#Variable Importance
RF_stats[[set]]$variable_importance <- RF_variable_importance(test, model,
macro_factor_names, individual_factor_names)
## vimp
RF_stats[[set]]$vimp <- list(breiman_cutler = vimp(model, importance = "permute",
block.size = 1,
newdata = test)$importance,
ishwaran_kogalur = vimp(model, importance = "permute",
block.size = best_model_params$ntree,
newdata = test)$importance)
}
return(RF_stats)
}
#########################################################################################################################
RF_grid <- expand.grid(
#ntree usually isn't tuned. Just set to max of computationally feasible
ntree = 50,
mtry = seq(20, round(ncol(final_dataset[4:ncol(final_dataset)])/4), 20),
nodesize = seq(6, 15, 3)
# nodedepth recommended not to be changed
#nodedepth = 1
)
RF_mse_stats <- RF_fit_stats(final_dataset, RF_grid, realdata_timeSlices, "mse", f,
macro_factor_names, individual_factor_names)
f <- real_panel_formula(final_dataset)
RF_grid <- expand.grid(
#ntree usually isn't tuned. Just set to max of computationally feasible
ntree = 50,
mtry = seq(20, round(ncol(final_dataset[4:ncol(final_dataset)])/4), 20),
nodesize = seq(6, 15, 3)
# nodedepth recommended not to be changed
#nodedepth = 1
)
RF_mse_stats <- RF_fit_stats(final_dataset, RF_grid, realdata_timeSlices, "mse", f,
macro_factor_names, individual_factor_names)
saveRDS(RF_mse_stats, file = "~/GitHub/Evaluation-of-Machine-Learning-in-Asset-Pricing/R/Model_results/empirical/RF_mse_stats_1.5_train_valid.rds")
RF_mae_stats <- RF_fit_stats(final_dataset, RF_grid, realdata_timeSlices, "mae", f,
macro_factor_names, individual_factor_names)
saveRDS(RF_mae_stats, file = "~/GitHub/Evaluation-of-Machine-Learning-in-Asset-Pricing/R/Model_results/empirical/RF_mae_stats_1.5_train_valid.rds")
RF_mae_stats[[1]]$loss_stats
RF_mae_stats[[2]]$loss_stats
RF_mae_stats[[3]]$loss_stats
