)
} else {
#MAE Case
RF <- rfsrc(f, train,
#Hyperparameters
ntree = RF_grid$ntree[i],
mtry = RF_grid$mtry[i],
nodesize = RF_grid$nodesize[i],
splitrule = "quantile.regr",
prob = 0.5
)
#RF_model_grid[[i]]$model <- RF
RF_model_grid[[i]]$RF_grid <- cbind(RF_grid[i, ],
#Train Loss
train_loss = mae(train$rt, predict(RF)$predicted),
#Validation Loss
validation_loss = mae(validation$rt, predict(RF, newdata = validation)$predicted)
)
}
}
return(RF_model_grid)
}
#Returns the dataframe row containing the "best" hyperparameters
get_RF_best_tune <- function(RF_model_grid) {
RF_tune_grid <- RF_model_grid[[1]]$RF_grid
for (i in 2:length(RF_model_grid)) {
RF_tune_grid <- rbind(RF_tune_grid, RF_model_grid[[i]]$RF_grid)
}
return(RF_tune_grid[which.min(RF_tune_grid$validation_loss), ])
}
RF_fit_stats <- function(f, pooled_panel, RF_grid, timeSlices, loss_function) {
#Initialize
RF_stats <- rep(list(0), 3)
#Load training, validation and test sets
for (set in 1:3) {
RF_stats[[set]] <- list(loss_stats = data.frame(train_MAE = 0, train_MSE = 0, train_RMSE = 0, train_RSquare = 0,
validation_MAE = 0, validation_MSE = 0, validation_RMSE = 0, validation_RSquare = 0,
test_MAE = 0, test_MSE = 0, test_RMSE = 0, test_RSquare = 0),
#Other useful things
forecasts = 0,
forecast_resids = 0,
model = 0)
train <- pooled_panel %>%
filter(time %in% timeSlices[[set]]$train)
validation <- pooled_panel %>%
filter(time %in% timeSlices[[set]]$validation)
test <- pooled_panel %>%
filter(time %in% timeSlices[[set]]$test)
#Fit on training Set over grid of hyperparameters
model_grid <- RF_fit_model_grid(f, train, validation, RF_grid, loss_function)
#Get the best hyperparameters
best_model_params <- get_RF_best_tune(model_grid)
#Compute the optimal model
if (loss_function == "mse") {
model <- rfsrc(f, train,
#Hyperparameters
ntree = best_model_params$ntree,
mtry = best_model_params$mtry,
nodesize = best_model_params$nodesize,
splitrule = "mse"
)
} else {
model <- rfsrc(f, train,
#Hyperparameters
ntree = best_model_params$ntree,
mtry = best_model_params$mtry,
nodesize = best_model_params$nodesize,
splitrule = "quantile.regr",
prob = 0.5
)
}
RF_stats[[set]]$model <- model
#Train
train_predict <- predict(model, train, alpha = best_model_params$alpha, lambda = best_model_params$lambda)$predicted
RF_stats[[set]]$loss_stats$train_MAE <- mae(train$rt, train_predict)
RF_stats[[set]]$loss_stats$train_MSE <- mse(train$rt, train_predict)
RF_stats[[set]]$loss_stats$train_RMSE <- rmse(train$rt, train_predict)
RF_stats[[set]]$loss_stats$train_RSquare <- (1 - sse(train$rt, train_predict) / sum((train$rt - mean(train$rt))^2))
#Validation
valid_predict <- predict(model, newdata = validation, alpha = best_model_params$alpha, lambda = best_model_params$lambda)$predicted
RF_stats[[set]]$loss_stats$validation_MAE <- mae(validation$rt, valid_predict)
RF_stats[[set]]$loss_stats$validation_MSE <- mse(validation$rt, valid_predict)
RF_stats[[set]]$loss_stats$validation_RMSE <- rmse(validation$rt, valid_predict)
RF_stats[[set]]$loss_stats$validation_RSquare <- (1 - sse(validation$rt, valid_predict) / sum((validation$rt - mean(validation$rt))^2))
#Test
test_predict <- predict(model, newdata = test, alpha = best_model_params$alpha, lambda = best_model_params$lambda)$predicted
RF_stats[[set]]$loss_stats$test_MAE <- mae(test$rt, test_predict)
RF_stats[[set]]$loss_stats$test_MSE <- mse(test$rt, test_predict)
RF_stats[[set]]$loss_stats$test_RMSE <- rmse(test$rt, test_predict)
RF_stats[[set]]$loss_stats$test_RSquare <- (1 - sse(test$rt, test_predict) / sum((test$rt - mean(test$rt))^2))
#Forecasts
RF_stats[[set]]$forecasts <- test_predict
#Forecast residuals
RF_stats[[set]]$forecast_resids <- test$rt - test_predict
}
return(RF_stats)
}
RF_MSE <- RF_fit_stats(f, pooled_panel, RF_grid, timeSlices, "mse")
RF_MAE <- RF_fit_stats(f, pooled_panel, RF_grid, timeSlices, "mae")
RF_MSE[[1]]$loss_stats
RF_MSE[[2]]$loss_stats
RF_MSE[[3]]$loss_stats
RF_MAE[[1]]$loss_stats
RF_MAE[[2]]$loss_stats
RF_MAE[[3]]$loss_stats
# Notes
# Performance seems to be worse than other methods, quite surprising
# Quantile trees (ie MAE) do better than MSE trees
# Quantile trees are much more computationally intensive
# Neural Networks
set.seed(27935248)
#Build neural networks with neuron numbers according to geometric pyramid rule, and ReLU activation function for all layers
# IE Input layer > 32 neurons > 16 neurons > 8 neurons > 4 neurons > 2 neurons > output
##Neural Network 1
#Initialize
nnet_stats_list <- rep(list(0), 3)
for (set in 1:3) {
nnet_stats_list[[set]] <- list(loss_stats = data.frame(train_MAE = 0, train_MSE = 0, train_RMSE = 0, train_RSquare = 0,
validation_MAE = 0, validation_MSE = 0, validation_RMSE = 0, validation_RSquare = 0,
test_MAE = 0, test_MSE = 0, test_RMSE = 0, test_RSquare = 0),
#Other useful things
forecasts = 0,
forecast_resids = 0,
model = 0)
#Load Training, validation and test sets
train <- pooled_panel %>%
filter(time %in% timeSlices[[set]]$train)
validation <- pooled_panel %>%
filter(time %in% timeSlices[[set]]$validation)
test<- pooled_panel %>%
filter(time %in% timeSlices[[set]]$test)
#Fit the model
}
train_x <- train[4:ncol(train)]
train_y <- train$rt
validation_x <- validation[4:ncol(validation)]
validation_y <- validation$rt
# The patience parameter is the amount of epochs to check for improvement.
early_stop <- callback_early_stopping(monitor = "val_loss", patience = 20)
#It's clearer to separate these "layers" out as much as possible
build_NN1 <- function(loss_function) {
model <- keras_model_sequential() %>%
# Layer 1
layer_dense(units = 32, input_shape = ncol(train_x)) %>%
layer_activation("relu") %>%
layer_activity_regularization(l1 = 0.001) %>%
layer_batch_normalization() %>%
# Output Layer
layer_dense(units = 1, activation = "linear")
model %>% compile(
#MSE loss function
loss = loss_function,
#We're using ADAM
optimizer = "adam",
metrics = list("mean_absolute_error", "mean_squared_error")
)
model
}
## WRT MSE
neural_network_1_mse <- build_NN1("mse")
neural_network_1_mse %>% summary()
#Just do something straightforward for now
neural_network_1_mse %>% fit(as.matrix(train_x), as.matrix(train_y),
epochs = 100, verbose = 1,
validation_data = list(as.matrix(validation_x), as.matrix(validation_y)),
callbacks = list(early_stop))
#Make predictions
test_x <- test[4:ncol(test)]
NN1_mse_predictions <- neural_network_1_mse %>% predict(as.matrix(test_x))
mae(test$rt, NN1_mse_predictions)
mse(test$rt, NN1_mse_predictions)
###########################################
## WRT MAE
neural_network_1_mae <- build_NN1("mae")
neural_network_1_mae %>% summary()
#Just do something straightforward for now
neural_network_1_mae %>% fit(as.matrix(train_x), as.matrix(train_y),
epochs = 100, verbose = 1,
validation_data = list(as.matrix(validation_x), as.matrix(validation_y)),
callbacks = list(early_stop))
#Make predictions
test_x <- test[4:ncol(test)]
NN1_mae_predictions <- neural_network_1_mae %>% predict(as.matrix(test_x))
mae(test$rt, NN1_mae_predictions)
mse(test$rt, NN1_mae_predictions)
R2(NN1_mae_predictions, test$rt, form = "traditional")
R2(NN1_mse_predictions, test$rt, form = "traditional")
# Neural Network 5
train_x <- train[4:ncol(train)]
train_y <- train$rt
validation_x <- validation[4:ncol(validation)]
validation_y <- validation$rt
# The patience parameter is the amount of epochs to check for improvement.
early_stop <- callback_early_stopping(monitor = "val_loss", patience = 1)
#It's clearer to separate these "layers" out as much as possible
build_NN5 <- function(loss_function) {
model <- keras_model_sequential() %>%
# Layer 1
layer_dense(units = 32, input_shape = ncol(train_x)) %>%
layer_activation("relu") %>%
layer_activity_regularization(l1 = 0.001) %>%
layer_batch_normalization() %>%
# Layer 2
layer_dense(units = 16) %>%
layer_activation("relu") %>%
layer_activity_regularization(l1 = 0.001) %>%
layer_batch_normalization() %>%
#Layer 3
layer_dense(units = 8) %>%
layer_activation("relu") %>%
layer_activity_regularization(l1 = 0.001) %>%
layer_batch_normalization() %>%
#Layer 4
layer_dense(units = 4) %>%
layer_activation("relu") %>%
layer_activity_regularization(l1 = 0.001) %>%
layer_batch_normalization() %>%
#Layer 5
layer_dense(units = 2) %>%
layer_activation("relu") %>%
layer_activity_regularization(l1 = 0.001) %>%
layer_batch_normalization() %>%
# Output Layer
layer_dense(units = 1, activation = "linear")
model %>% compile(
#MSE loss function
loss = loss_function,
#We're using ADAM
optimizer = "adam",
metrics = list("mean_absolute_error", "mean_squared_error")
)
model
}
## WRT MSE
neural_network_5_mse <- build_NN5("mse")
neural_network_5_mse %>% summary()
#Just do something straightforward for now
neural_network_5_mse %>% fit(as.matrix(train_x), as.matrix(train_y),
epochs = 100, verbose = 1,
validation_data = list(as.matrix(validation_x), as.matrix(validation_y)),
callbacks = list(early_stop))
#Make predictions
test_x <- test[4:ncol(test)]
NN5_mse_predictions <- neural_network_5_mse %>% predict(as.matrix(test_x))
mae(test$rt, NN5_mse_predictions)
mse(test$rt, NN5_mse_predictions)
###########################################
## WRT MAE
neural_network_5_mae <- build_NN5("mae")
neural_network_5_mae %>% summary()
#Just do something straightforward for now
neural_network_5_mae %>% fit(as.matrix(train_x), as.matrix(train_y),
epochs = 100, verbose = 1,
validation_data = list(as.matrix(validation_x), as.matrix(validation_y)),
callbacks = list(early_stop))
#Make predictions
test_x <- test[4:ncol(test)]
NN5_mae_predictions <- neural_network_5_mae %>% predict(as.matrix(test_x))
mae(test$rt, NN5_mae_predictions)
mse(test$rt, NN5_mae_predictions)
R2(NN5_mae_predictions, test$rt, form = "traditional")
R2(NN5_mse_predictions, test$rt, form = "traditional")
# Neural Network 5
train_x <- train[4:ncol(train)]
train_y <- train$rt
validation_x <- validation[4:ncol(validation)]
validation_y <- validation$rt
# The patience parameter is the amount of epochs to check for improvement.
early_stop <- callback_early_stopping(monitor = "val_loss", patience = 10)
#It's clearer to separate these "layers" out as much as possible
build_NN5 <- function(loss_function) {
model <- keras_model_sequential() %>%
# Layer 1
layer_dense(units = 32, input_shape = ncol(train_x)) %>%
layer_activation("relu") %>%
layer_activity_regularization(l1 = 0.001) %>%
layer_batch_normalization() %>%
# Layer 2
layer_dense(units = 16) %>%
layer_activation("relu") %>%
layer_activity_regularization(l1 = 0.001) %>%
layer_batch_normalization() %>%
#Layer 3
layer_dense(units = 8) %>%
layer_activation("relu") %>%
layer_activity_regularization(l1 = 0.001) %>%
layer_batch_normalization() %>%
#Layer 4
layer_dense(units = 4) %>%
layer_activation("relu") %>%
layer_activity_regularization(l1 = 0.001) %>%
layer_batch_normalization() %>%
#Layer 5
layer_dense(units = 2) %>%
layer_activation("relu") %>%
layer_activity_regularization(l1 = 0.001) %>%
layer_batch_normalization() %>%
# Output Layer
layer_dense(units = 1, activation = "linear")
model %>% compile(
#MSE loss function
loss = loss_function,
#We're using ADAM
optimizer = "adam",
metrics = list("mean_absolute_error", "mean_squared_error")
)
model
}
## WRT MSE
neural_network_5_mse <- build_NN5("mse")
neural_network_5_mse %>% summary()
#Just do something straightforward for now
neural_network_5_mse %>% fit(as.matrix(train_x), as.matrix(train_y),
epochs = 100, verbose = 1,
validation_data = list(as.matrix(validation_x), as.matrix(validation_y)),
callbacks = list(early_stop))
#Make predictions
test_x <- test[4:ncol(test)]
NN5_mse_predictions <- neural_network_5_mse %>% predict(as.matrix(test_x))
mae(test$rt, NN5_mse_predictions)
mse(test$rt, NN5_mse_predictions)
###########################################
## WRT MAE
neural_network_5_mae <- build_NN5("mae")
neural_network_5_mae %>% summary()
#Just do something straightforward for now
neural_network_5_mae %>% fit(as.matrix(train_x), as.matrix(train_y),
epochs = 100, verbose = 1,
validation_data = list(as.matrix(validation_x), as.matrix(validation_y)),
callbacks = list(early_stop))
#Make predictions
test_x <- test[4:ncol(test)]
NN5_mae_predictions <- neural_network_5_mae %>% predict(as.matrix(test_x))
mae(test$rt, NN5_mae_predictions)
mse(test$rt, NN5_mae_predictions)
R2(NN5_mae_predictions, test$rt, form = "traditional")
R2(NN5_mse_predictions, test$rt, form = "traditional")
knitr::opts_chunk$set(echo = TRUE)
################
##Load Libraries
################
library(glmnet)
library(tidyverse)
library(keras)
library(ggplot2)
library(forecast)
library(rlist)
library(plm)
library(Metrics)
library(ranger)
library(glmnetUtils)
library(tensorflow)
library(quantreg)
library(randomForestSRC)
library(hqreg)
library(devtools)
library(caret)
#forestr
# This apparently needs admin privileges to install, do this at home maybe
# devtools::install_github("andeek/forestr")
# devtools::install_github("andeek/rpart")
# unlink("C:/R/Library/00LOCK-rpart", recursive = TRUE)
set.seed(27935248)
nnet_stats_list <- rep(list(0), 3)
set.seed(27935248)
NNet_stats_list <- rep(list(0), 3)
for (set in 1:3) {
NNet_stats_list[[set]] <- list(loss_stats = data.frame(train_MAE = 0, train_MSE = 0, train_RMSE = 0, train_RSquare = 0,
validation_MAE = 0, validation_MSE = 0, validation_RMSE = 0, validation_RSquare = 0,
test_MAE = 0, test_MSE = 0, test_RMSE = 0, test_RSquare = 0),
#Other useful things
forecasts = 0,
forecast_resids = 0,
model = 0)
#Load Training, validation and test sets
train <- pooled_panel %>%
filter(time %in% timeSlices[[set]]$train)
validation <- pooled_panel %>%
filter(time %in% timeSlices[[set]]$validation)
test<- pooled_panel %>%
filter(time %in% timeSlices[[set]]$test)
train_x <- train[4:ncol(train)]
train_y <- train$rt
validation_x <- validation[4:ncol(validation)]
validation_y <- validation$rt
test_x <- test[4:ncol(test)]
test_y <- test$rt
# Fit the model
# The patience parameter is the amount of epochs to check for improvement.
early_stop <- callback_early_stopping(monitor = "val_loss", patience = 20)
#It's clearer to separate these "layers" out as much as possible
build_NN1 <- function(loss_function) {
model <- keras_model_sequential() %>%
# Layer 1
layer_dense(units = 32, input_shape = ncol(train_x)) %>%
layer_activation("relu") %>%
layer_activity_regularization(l1 = 0.001) %>%
layer_batch_normalization() %>%
# Output Layer
layer_dense(units = 1, activation = "linear")
model %>% compile(
#MSE loss function
loss = loss_function,
#We're using ADAM
optimizer = "adam",
metrics = list("mean_absolute_error", "mean_squared_error")
)
model
}
neural_network_1_mse <- build_NN1("mse")
neural_network_1_mse %>% fit(as.matrix(train_x), as.matrix(train_y),
epochs = 100, verbose = 1,
validation_data = list(as.matrix(validation_x), as.matrix(validation_y)),
callbacks = list(early_stop))
#Train
train_predict <- neural_network_1_mse %>% predict(as.matrix(train_x))
NNet_stats[[set]]$loss_stats$train_MAE <- mae(train$rt, train_predict)
NNet_stats[[set]]$loss_stats$train_MSE <- mse(train$rt, train_predict)
NNet_stats[[set]]$loss_stats$train_RMSE <- rmse(train$rt, train_predict)
NNet_stats[[set]]$loss_stats$train_RSquare <- (1 - sse(train$rt, train_predict) / sum((train$rt - mean(train$rt))^2))
#Validation
valid_predict <- neural_network_1_mse %>% predict(as.matrix(validation_x))
NNet_stats[[set]]$loss_stats$validation_MAE <- mae(validation$rt, valid_predict)
NNet_stats[[set]]$loss_stats$validation_MSE <- mse(validation$rt, valid_predict)
NNet_stats[[set]]$loss_stats$validation_RMSE <- rmse(validation$rt, valid_predict)
NNet_stats[[set]]$loss_stats$validation_RSquare <- (1 - sse(validation$rt, valid_predict) / sum((validation$rt - mean(validation$rt))^2))
#Test
test_predict <- neural_network_1_mse %>% predict(as.matrix(test_x))
NNet_stats[[set]]$loss_stats$test_MAE <- mae(test$rt, test_predict)
NNet_stats[[set]]$loss_stats$test_MSE <- mse(test$rt, test_predict)
NNet_stats[[set]]$loss_stats$test_RMSE <- rmse(test$rt, test_predict)
NNet_stats[[set]]$loss_stats$test_RSquare <- (1 - sse(test$rt, test_predict) / sum((test$rt - mean(test$rt))^2))
#Forecasts
NNet_stats[[set]]$forecasts <- test_predict
#Forecast residuals
NNet_stats[[set]]$forecast_resids <- test$rt - test_predict
}
NNet_stats <- rep(list(0), 3)
for (set in 1:3) {
NNet_stats[[set]] <- list(loss_stats = data.frame(train_MAE = 0, train_MSE = 0, train_RMSE = 0, train_RSquare = 0,
validation_MAE = 0, validation_MSE = 0, validation_RMSE = 0, validation_RSquare = 0,
test_MAE = 0, test_MSE = 0, test_RMSE = 0, test_RSquare = 0),
#Other useful things
forecasts = 0,
forecast_resids = 0,
model = 0)
#Load Training, validation and test sets
train <- pooled_panel %>%
filter(time %in% timeSlices[[set]]$train)
validation <- pooled_panel %>%
filter(time %in% timeSlices[[set]]$validation)
test<- pooled_panel %>%
filter(time %in% timeSlices[[set]]$test)
train_x <- train[4:ncol(train)]
train_y <- train$rt
validation_x <- validation[4:ncol(validation)]
validation_y <- validation$rt
test_x <- test[4:ncol(test)]
test_y <- test$rt
# Fit the model
# The patience parameter is the amount of epochs to check for improvement.
early_stop <- callback_early_stopping(monitor = "val_loss", patience = 20)
#It's clearer to separate these "layers" out as much as possible
build_NN1 <- function(loss_function) {
model <- keras_model_sequential() %>%
# Layer 1
layer_dense(units = 32, input_shape = ncol(train_x)) %>%
layer_activation("relu") %>%
layer_activity_regularization(l1 = 0.001) %>%
layer_batch_normalization() %>%
# Output Layer
layer_dense(units = 1, activation = "linear")
model %>% compile(
#MSE loss function
loss = loss_function,
#We're using ADAM
optimizer = "adam",
metrics = list("mean_absolute_error", "mean_squared_error")
)
model
}
neural_network_1_mse <- build_NN1("mse")
neural_network_1_mse %>% fit(as.matrix(train_x), as.matrix(train_y),
epochs = 100, verbose = 1,
validation_data = list(as.matrix(validation_x), as.matrix(validation_y)),
callbacks = list(early_stop))
#Train
train_predict <- neural_network_1_mse %>% predict(as.matrix(train_x))
NNet_stats[[set]]$loss_stats$train_MAE <- mae(train$rt, train_predict)
NNet_stats[[set]]$loss_stats$train_MSE <- mse(train$rt, train_predict)
NNet_stats[[set]]$loss_stats$train_RMSE <- rmse(train$rt, train_predict)
NNet_stats[[set]]$loss_stats$train_RSquare <- (1 - sse(train$rt, train_predict) / sum((train$rt - mean(train$rt))^2))
#Validation
valid_predict <- neural_network_1_mse %>% predict(as.matrix(validation_x))
NNet_stats[[set]]$loss_stats$validation_MAE <- mae(validation$rt, valid_predict)
NNet_stats[[set]]$loss_stats$validation_MSE <- mse(validation$rt, valid_predict)
NNet_stats[[set]]$loss_stats$validation_RMSE <- rmse(validation$rt, valid_predict)
NNet_stats[[set]]$loss_stats$validation_RSquare <- (1 - sse(validation$rt, valid_predict) / sum((validation$rt - mean(validation$rt))^2))
#Test
test_predict <- neural_network_1_mse %>% predict(as.matrix(test_x))
NNet_stats[[set]]$loss_stats$test_MAE <- mae(test$rt, test_predict)
NNet_stats[[set]]$loss_stats$test_MSE <- mse(test$rt, test_predict)
NNet_stats[[set]]$loss_stats$test_RMSE <- rmse(test$rt, test_predict)
NNet_stats[[set]]$loss_stats$test_RSquare <- (1 - sse(test$rt, test_predict) / sum((test$rt - mean(test$rt))^2))
#Forecasts
NNet_stats[[set]]$forecasts <- test_predict
#Forecast residuals
NNet_stats[[set]]$forecast_resids <- test$rt - test_predict
}
NNet_stats[[1]]$loss_stats
NNet_stats[[2]]$loss_stats
NNet_stats[[3]]$loss_stats
