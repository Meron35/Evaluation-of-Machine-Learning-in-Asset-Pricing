#This will return an array containing the entire panel of errors
#i.e. the errors for each i, and across all times
#Error itself only has one dimension, so it will be in [i, 1, t] format
gen_error <- function(sv,
#epsilon sd. Note that this is the normal sd in sv case,
#and student t sd in simple case
ep_sd,
#sv parameters
omega, gamma, w,
#Beta_v Error parameters
C, v_sd){
#Initialize
error <- array(data = 0, dim = c(N, 1, Time))
##Beta_v component, add this on to the other error component at the end
Beta <- C[, 1:3, ]
Beta_v <- array(0, dim = c(N, 1, Time))
for (t in 1:(Time)) {
v <- matrix(data = rnorm(3, mean = 0, sd = v_sd),
nrow = 3,
ncol = 1)
for (i in 1:N) {
Beta_v[i, 1, t] <- Beta[i, , t] %*% v
}
}
###########
#SV errors
###########
if (sv == 1) {
##Generate Sigma first (only indexed by time)
sigma2 <- rep(0, Time+1)
#Initial sigma2
sigma2[1] <- omega + w
for (t in 1:Time+1) {
sigma2[t+1] <- omega + gamma*sigma2[t] + rnorm(1, 0, w)
}
for (t in 1:(Time)) {
for (i in 1:N) {
error[i, 1, t] <- exp(sigma2[t+1]/2) * rnorm(1, 0, ep_sd)
}
}
return(error + Beta_v)
}
##################
#NON-SV Student t errors
##################
else {
for (t in 1:(Time)) {
error[, , t] <- matrix(data = rt(N, df = 5) * sqrt(ep_sd^2 * (5-2)/5), nrow = N)
}
return(error + Beta_v)
}
}
#SV version check
error <- gen_error(sv = 1, ep_sd = 0.05, omega = -0.736, gamma = 0.90, w = sqrt(0.363), C, v_sd = 0.05)
#Non-SV version check
#You'll still need to specify values for omega, gamma and w, they just won't be used
error <- gen_error(sv = 0, ep_sd = 0.05, omega = -0.736, gamma = 0.90, w = sqrt(0.363), C, v_sd = 0.05)
#Both Working, yay
##Just a recap chunk of everything that needs to be run to generate everything thus far before we move on to tuning R squared
C_bar <- gen_C_bar(rho_a = 0.9, rho_b = 1)
C_hat <- gen_C_hat(C_bar)
#Change this to C_hat if you want to build in cross sectional correlation
C <- gen_C(C_bar)
A1 <- matrix(c(
0.95, 0, 0,
0, 0.95, 0,
0, 0, 0.95),
nrow = 3, ncol = 3
)
A2 <- matrix(c(
1, 0, 0.25,
0, 0.95, 0,
0.25, 0, 0.95),
nrow = 3, ncol = 3
)
A3 <- matrix(c(
0.99, 0.2, 0.1,
0.2, 0.90, -0.3,
0.1, -0.3, -0.99),
nrow = 3, ncol = 3
)
xt <- gen_xt(A1)
xt_univariate <- gen_xt_univariate()
#Generate the true underlying factors first
g1_factor_panel <- gen_g_factor_panel("g1", C, xt)
#Then pass them through to multiply them by theta to get g()
g1_panel <- gen_g_panel(g1_factor_panel, theta = matrix(c(0.04, 0.035, 0.01), nrow = 1))
#Generate the errors
#SV version
error_sv <- gen_error(sv = 1, ep_sd = 0.05, omega = -0.736, gamma = 0.90, w = sqrt(0.363), C, v_sd = 0.05)
#Non-SV version
error_nosv <- gen_error(sv = 0, ep_sd = 0.05, omega = -0.736, gamma = 0.90, w = sqrt(0.363), C, v_sd = 0.05)
#Finally generate a returns panel
rt_panel_g1_A1 <- g1_panel + error_nosv
#All works, hooray
############################
# Function Form
############################
# Given a return series panel, its signal (non-error component), and its corresponding underlying true factors, return its mean individual time series R squared, mean annualized volatility, cross sectional r squared and predictive r squared
panel_tune_stats <- function(return_panel, signal_panel, true_factor_panel) {
#Initialize
tune_stats <- data.frame(time_series_fitted.rsquare = 0, annual_vol = 0, true_rsquare = 0, cross_section_rsquare = 0)
########
## Time Series + Annual Vol
########
#Empty Matrices to be used
Rsquared <- matrix(0, N, 1)
SSR <- matrix(0, N, 1)
SST <- matrix(0, N, 1)
Fits <- matrix(0, Time, N)
#This stores all the BETAS, NOT the constant
Coeff_Betas <- matrix(0, N, ncol(true_factor_panel))
Xs_bar <- matrix(0, N, ncol(true_factor_panel))
R_bar <- matrix(0, N, 1)
Resids <- matrix(0, Time, N)
annual_vol <- matrix(0, N, 1)
#Done for the g1 case for now
#First run individual time series regressions
for (i in 1:N) {
#True Returns
Rs <- return_panel[i, , ]
R_bar[i, 1] <- mean(Rs)
#True Factor Set
Xs <- t(true_factor_panel[i, , ])
#Not too sure what this is doing, not used anywhere else
Xs_bar[i, ] <- colMeans(Xs)
df <- data.frame(Rs, Xs)
fit <- lm(Rs ~., df)
Fits[, i] <- fit$fitted.values
#Save all the betas except for the constant, slightly more clever way that handles different number of true factors
Coeff_Betas[i, ] <- as.numeric(fit$coefficients[-1])
Resids[, i] <- fit$residuals
SSR[i, 1] <- sum(Resids[, i]^2)
SST[i, 1] <- sum((Rs-mean(Rs))^2)
Rsquared[i, 1] <- summary(fit)$r.squared
#annualized volatility
annual_vol[i] <- sd(Rs) * sqrt(12)
}
tune_stats$time_series_fitted.rsquare <- mean(Rsquared)
tune_stats$annual_vol <- mean(annual_vol)
#Predictive R Squared
tune_stats$true_rsquare <- R2(signal_panel, return_panel, form = "traditional")
# David's approach at doing true (predictive) r squared
# val <- g1(C,x,theta)
# true_res <- rowSums((rt - val)^2)
# true_sst <- rowSums((rt - rowMeans(rt))^2)
# true_rRsquared <- 1 - (sum(true_res)/sum(true_sst))
# fitted_Rsquared <- 1 - (sum(SSR)/sum(SST))
# Cross Sectional R squared
R_bar <- rowMeans(return_panel)
df_new <- data.frame(R_bar, Coeff_Betas)
cross_fit <- lm(R_bar ~ Coeff_Betas, df_new)
tune_stats$cross_section_rsquare <- summary(cross_fit)$r.squared
return(tune_stats)
}
# Testing
xt_univariate <- gen_xt_univariate()
#Generate the true underlying factors first
g1_factor_panel <- gen_g_factor_panel("g1", C, xt)
#Then pass them through to multiply them by theta to get g()
g1_panel <- gen_g_panel(g1_factor_panel, theta = matrix(c(0.04, 0.035, 0.01), nrow = 1))
#Generate the errors
#SV version
error_sv <- gen_error(sv = 1, ep_sd = 0.05, omega = -0.736, gamma = 0.90, w = sqrt(0.363), C, v_sd = 0.05)
#Non-SV version
error_nosv <- gen_error(sv = 0, ep_sd = 0.05, omega = -0.736, gamma = 0.90, w = sqrt(0.363), C, v_sd = 0.05)
#Finally generate a returns panel
rt_panel_g1_A1 <- g1_panel + error_nosv
# Generate a predictor panel
# z_panel_g1_A1 <- gen_predictor_z(C, xt)
panel_tune_stats(rt_panel_g1_A1, g1_panel, g1_factor_panel)
##############################################################################
##Function to build predictor set for the entire panel
##############################################################################
#Kronecker product
# Calculated with z_it = (1, xt)' \otimes c_it
# xt should be P_x x 1
# c_it should be P_c x 1
# z_it should be a P_c*(P_x + 1) x 1 vector of features
gen_predictor_z <- function(C, x){
xt_set <- rbind(1, x)
dimnames(xt_set)[[1]][1] <- "constant"
stock_dim <- paste0("stock_", c(1:N))
c_dim <- dimnames(kronecker(t(xt_set[, 1]), t(C[1, , 1]), make.dimnames = TRUE))[[2]]
time_dim <- paste0("time_", c(1:(Time)))
z_panel <- array(0, dim = c(N, (nrow(xt_set)) * P_c, Time), dimnames = list(stock_dim, c_dim, time_dim))
for (i in 1:N) {
for (t in 1:Time) {
z_panel[i, , t] <- kronecker(t(xt_set[, t]), t(C[i, , t]), make.dimnames = TRUE)
}
}
return(z_panel)
}
z_panel_g1_A1 <- gen_predictor_z(C, xt)
# Bind Returns and Predictor Sets Together into one 2D dataframe, ready to be used for training models
bind_rt_predictor <- function(rt_panel, z_panel) {
df <- cbind(data.frame(rt_panel[, , 1]), time = 2, stock = paste0("stock_", c(1:N)), data.frame(z_panel[, , 1]))
colnames(df)[1] <- "rt"
row.names(df) <- NULL
for (t in 2:Time) {
df_new <- cbind(data.frame(rt_panel[, , t]), time = 1+t, stock = paste0("stock_", c(1:N)), data.frame(z_panel[, , t]))
colnames(df_new)[1] <- "rt"
row.names(df_new) <- NULL
df <- rbind(df, df_new)
}
return(df)
}
#test
panel_g1_A1 <- bind_rt_predictor(rt_panel_g1_A1, z_panel_g1_A1)
#With that... DONE WITH THIS PART OF THE PROJECT
##########################
A1 <- matrix(c(
0.95, 0, 0,
0, 0.95, 0,
0, 0, 0.95),
nrow = 3, ncol = 3
)
A2 <- matrix(c(
1, 0, 0.25,
0, 0.95, 0,
0.25, 0, 0.95),
nrow = 3, ncol = 3
)
A3 <- matrix(c(
0.99, 0.2, 0.1,
0.2, 0.90, -0.3,
0.1, -0.3, -0.99),
nrow = 3, ncol = 3
)
##########################
# Parallel implementation of simulation design
# This is absolutely amazing, finishes 100 simulations in less than 5 minutes on 12 logical cores
sim_panel_data <- function(sim_N,
char_rho_a, char_rho_b,
cross_corr, A_matrix, xt_multi, g_function, theta,
#Error Parameters
error_sv, error_ep_sd, error_omega, error_gamma, error_w, error_v_sd) {
sim_list <- foreach(i = (1:sim_N)) %dopar% {
# Set Seed for each worker so that it is reproducible
set.seed(27925248 + i)
sim <- list(panel = 0, statistics = 0)
C_bar <- gen_C_bar(char_rho_a, char_rho_b)
C_hat <- gen_C_hat(C_bar)
if (cross_corr == 0) {
C <- gen_C(C_bar)
}
else {
C <- gen_C(C_hat)
}
if (xt_multi == 1) {
xt <- gen_xt(A_matrix)
}
else {
xt <- gen_xt_univariate()
}
#Generate the true underlying factors first
g_factor_panel <- gen_g_factor_panel(g_function, C, xt)
#Then pass them through to multiply them by theta to get g()
g_panel <- gen_g_panel(g_factor_panel, theta)
#Generate the errors
error <- gen_error(sv = error_sv, ep_sd = error_ep_sd, omega = error_omega, gamma = error_gamma, w = error_w, C, v_sd = error_v_sd)
#rt panel
rt_panel <- g_panel + error
z_panel <- gen_predictor_z(C, xt)
sim$panel <- bind_rt_predictor(rt_panel, z_panel)
#Statistics
sim$statistics <- panel_tune_stats(rt_panel, g_panel, g_factor_panel)
sim
}
return(sim_list)
}
# This function is very straightforward and does not need to be parallelized
# Run this after generating the data from the previous function (see example right below)
sim_tune_statistics <- function(sim_panel_list) {
sim_N <- length(sim_panel_list)
#Initialize
sim_tune_stats <- data.frame(time_series_fitted.rsquare = rep(0, sim_N),
annual_vol = rep(0, sim_N),
true_rsquare = rep(0, sim_N),
cross_section_rsquare = rep(0, sim_N))
for (i in 1:sim_N) {
sim_tune_stats[i, ] <- sim_panel_list[[i]]$statistics
}
return(sim_tune_stats)
}
# gu_et_al_g1 <- sim_panel_data_parallel(100, cross_corr = 0, A1, xt_multi = 0, g_function = "g1", theta = matrix(c(0.02, 0.02, 0.02), nrow = 1),
#                                        error_sv = 0, error_ep_sd = 0.05, error_omega = -0.736, error_gamma = 0.9, error_w = sqrt(0.363), error_v_sd = 0.05)
#
# gu_et_al_g2 <- sim_panel_data_parallel(100, cross_corr = 0, A1, xt_multi = 0, g_function = "g2", theta = matrix(c(0.04, 0.035, 0.01), nrow = 1),
#                                        error_sv = 0, error_ep_sd = 0.05, error_omega = -0.736, error_gamma = 0.9, error_w = sqrt(0.363), error_v_sd = 0.05)
#
# summary(sim_tune_statistics(gu_et_al_g1))
#
# summary(sim_tune_statistics(gu_et_al_g2))
# Observations
# You need a reasonably large number of simulations in order to gauge these designs
# Turning the simulation number from 10 to 100 does yield some changes
# Gu et al's design doesn't seem to be replicable
# True r squared (predictive r squared) is somewhat close to what they specified
N <- 200
P_c <- 100
Time <- 180
#Number of realizations
simN <- 4
g1_A1_panel <- sim_panel_data(simN,
char_rho_a = 0.5, char_rho_b = 1,
cross_corr = 1, A1, xt_multi = 1, g_function = "g1", theta = matrix(c(0.007, 0.007, 0.007), nrow = 1),
error_sv = 1, error_ep_sd = 0.05,
error_omega = -0.736, error_gamma = 0.9, error_w = sqrt(0.363), error_v_sd = 0.05)
summary(sim_tune_statistics(g1_A1_panel))
pooled_panel <- g1_A1_panel[[1]]$panel
f <- panel_formula(pooled_panel)
LM_stats_mse <- LM_fit(pooled_panel, timeSlices, "mse")
LM_stats_mse[[1]]$loss_stats
LM_stats_mse[[2]]$loss_stats
LM_stats_mse[[3]]$loss_stats
C_bar <- gen_C_bar(0.5, 1)
C_hat <- gen_C_hat(C_bar)
C_hat
C_hat[, , 1]
corr(C_hat[, , 1])
cor(C_hat[, , 1])
gen_W <- function(){
#Generate Lambda Matrix first
Lambda <- matrix(
data = rnorm(N*4, 0, 1),
nrow = N, ncol = 4
)
#Use Lambda to create B matrix
B <- (Lambda) %*% t(Lambda)
B <- B + 2/10*diag(nrow = nrow(B))
# B is now a positive semi definite matrix
# It is therefore a valid covariance matrix
#Decompose it via cholesky to give the lower triangle matrix
# R by defualt gives the upper triangle, therefore it needs to be transposed
W <- chol(B)
return(W)
}
g1_A1_panel <- sim_panel_data(simN,
char_rho_a = 0.5, char_rho_b = 1,
cross_corr = 1, A1, xt_multi = 1, g_function = "g1", theta = matrix(c(0.001, 0.001, 0.001), nrow = 1),
error_sv = 1, error_ep_sd = 0.05,
error_omega = -0.736, error_gamma = 0.9, error_w = sqrt(0.363), error_v_sd = 0.05)
summary(sim_tune_statistics(g1_A1_panel))
pooled_panel <- g1_A1_panel[[1]]$panel
g1_A1_panel[[1]]$statistics
LM_fit <- function(pooled_panel, timeSlices, loss_function) {
#Initialize Loss Function Statistics
LM_stats <- rep(list(0), 3)
for (set in 1:3) {
LM_stats[[set]] <- list(loss_stats = data.frame(train_MAE = 0, train_MSE = 0, train_RMSE = 0, train_RSquare = 0,
validation_MAE = 0, validation_MSE = 0, validation_RMSE = 0, validation_RSquare = 0,
test_MAE = 0, test_MSE = 0, test_RMSE = 0, test_RSquare = 0),
#Other useful things
forecasts = 0,
forecast_resids = 0,
model = 0,
#Variable Importance
variable_importance = 0)
#Load Training, validation and test sets
train <- pooled_panel %>%
filter(time %in% timeSlices[[set]]$train)
validation <- pooled_panel %>%
filter(time %in% timeSlices[[set]]$validation)
test <- pooled_panel %>%
filter(time %in% timeSlices[[set]]$test)
#Train Model on training set
#MSE case
if (loss_function == "mse") {
lm <- lm(f, data = train)
} else {
# Use pfn as method here for much faster computation
lm <- rq(f, data = train, tau = 0.5, method = "pfn")
}
LM_stats[[set]]$model <- lm
#No Tuning Needed
#Statistics
#Training Set
train_predict <- predict(lm)
LM_stats[[set]]$loss_stats$train_MAE <- mae(train$rt, train_predict)
LM_stats[[set]]$loss_stats$train_MSE <- mse(train$rt, train_predict)
LM_stats[[set]]$loss_stats$train_RMSE <- rmse(train$rt, train_predict)
LM_stats[[set]]$loss_stats$train_RSquare <- (1 - sse(train$rt, train_predict) / sum((train$rt - mean(train$rt))^2))
#Validation Set Statistics
validation_predict <- predict(lm, newdata = validation)
LM_stats[[set]]$loss_stats$validation_MAE <- mae(validation$rt, validation_predict)
LM_stats[[set]]$loss_stats$validation_MSE <- mse(validation$rt, validation_predict)
LM_stats[[set]]$loss_stats$validation_RMSE <- rmse(validation$rt, validation_predict)
LM_stats[[set]]$loss_stats$validation_RSquare <- (1 - sse(validation$rt, validation_predict) / sum((validation$rt - mean(validation$rt))^2))
#Test Set Statistics
test_predict <- predict(lm, newdata = test)
LM_stats[[set]]$loss_stats$test_MAE <- mae(test$rt, test_predict)
LM_stats[[set]]$loss_stats$test_MSE <- mse(test$rt, test_predict)
LM_stats[[set]]$loss_stats$test_RMSE <- rmse(test$rt, test_predict)
LM_stats[[set]]$loss_stats$test_RSquare <- (1 - sse(test$rt, test_predict) / sum((test$rt - mean(test$rt))^2))
#Forecasts
LM_stats[[set]]$forecasts <- test_predict
#Forecast Residuals
LM_stats[[set]]$forecast_resids <- test$rt - test_predict
#Variable Importance
LM_stats[[set]]$variable_importance <- LM_variable_importance(test, timeSlices, lm)
}
return(LM_stats)
}
LM_stats_mse <- LM_fit(pooled_panel, timeSlices, "mse")
LM_stats_mse[[1]]$loss_stats
LM_stats_mse[[2]]$loss_stats
LM_stats_mse[[3]]$loss_stats
LM_stats_mse[[1]]$variable_importance %>% arrange(desc(importance))
NNet_1_mse_stats <- NNet_fit_stats(pooled_panel, timeSlices, 1, "mse", batch_size = 32, 10)
NNet_1_mse_stats[[1]]$loss_stats
NNet_1_mse_stats[[2]]$loss_stats
NNet_1_mse_stats[[3]]$loss_stats
gen_W <- function(){
#Generate Lambda Matrix first
Lambda <- matrix(
data = rnorm(N*4, 0, 1),
nrow = N, ncol = 4
)
#Use Lambda to create B matrix
B <- (Lambda) %*% t(Lambda)
B <- B + 2/10*diag(nrow = nrow(B))
# B is now a positive semi definite matrix
# It is therefore a valid covariance matrix
#Decompose it via cholesky to give the lower triangle matrix
# R by defualt gives the upper triangle, therefore it needs to be transposed
W <- t(chol(B))
return(W)
}
g1_A1_panel <- sim_panel_data(simN,
char_rho_a = 0.5, char_rho_b = 1,
cross_corr = 1, A1, xt_multi = 1, g_function = "g1", theta = matrix(c(0.001, 0.001, 0.001), nrow = 1),
error_sv = 1, error_ep_sd = 0.05,
error_omega = -0.736, error_gamma = 0.9, error_w = sqrt(0.363), error_v_sd = 0.05)
summary(sim_tune_statistics(g1_A1_panel))
pooled_panel <- g1_A1_panel[[1]]$panel
LM_stats_mse <- LM_fit(pooled_panel, timeSlices, "mse")
LM_stats_mse[[1]]$loss_stats
LM_stats_mse[[2]]$loss_stats
LM_stats_mse[[3]]$loss_stats
g1_A1_panel <- sim_panel_data(simN,
char_rho_a = 0.5, char_rho_b = 1,
cross_corr = 1, A1, xt_multi = 1, g_function = "g1", theta = matrix(c(0.02, 0.02, 0.02), nrow = 1),
error_sv = 1, error_ep_sd = 0.05,
error_omega = -0.736, error_gamma = 0.9, error_w = sqrt(0.363), error_v_sd = 0.05)
summary(sim_tune_statistics(g1_A1_panel))
pooled_panel <- g1_A1_panel[[1]]$panel
f <- panel_formula(pooled_panel)
LM_stats_mse <- LM_fit(pooled_panel, timeSlices, "mse")
LM_stats_mse[[1]]$loss_stats
LM_stats_mse[[2]]$loss_stats
LM_stats_mse[[3]]$loss_stats
g1_A1_panel[[1]]$statistics
View(F_F_Research_Data_5_Factors_2x3_RAW)
########################
# Fama French Workhorse Factor
########################
# Not too sure what "workhorse" factors are, this is just the 5 factors for now as it seems like a superset of all of their factors
F_F_Research_Data_5_Factors_2x3_RAW <- read_csv("data/factors/F-F_Research_Data_5_Factors_2x3.csv",
skip = 3, n_max = 671) %>%
mutate(Time = as.yearmon(as.character(X1))) %>%
select(-X1)
View(F_F_Research_Data_5_Factors_2x3_RAW)
########################
# Fama French Workhorse Factor
########################
# Not too sure what "workhorse" factors are, this is just the 5 factors for now as it seems like a superset of all of their factors
F_F_Research_Data_5_Factors_2x3_RAW <- read_csv("data/factors/F-F_Research_Data_5_Factors_2x3.csv",
skip = 3, n_max = 671) %>%
mutate(Time = as.yearmon(as.character(X1), "%Y%m")) %>%
select(-X1)
g1_A1_panel[[1]]$statistics
g1_A1_panel <- sim_panel_data(simN,
char_rho_a = 0.5, char_rho_b = 1,
cross_corr = 1, A1, xt_multi = 1, g_function = "g1", theta = matrix(c(0.007, 0.007, 0.007), nrow = 1),
error_sv = 1, error_ep_sd = 0.05,
error_omega = -0.736, error_gamma = 0.9, error_w = sqrt(0.363), error_v_sd = 0.05)
summary(sim_tune_statistics(g1_A1_panel))
pooled_panel <- g1_A1_panel[[1]]$panel
LM_stats_mse <- LM_fit(pooled_panel, timeSlices, "mse")
LM_stats_mse[[1]]$loss_stats
LM_stats_mse[[2]]$loss_stats
g1_A1_panel[[1]]$statistics
LM_stats_mse[[3]]$loss_stats
LM_stats_mse[[1]]$variable_importance %>% arrange(desc(importance))
gen_W <- function(){
#Generate Lambda Matrix first
Lambda <- matrix(
data = rnorm(N*4, 0, 1),
nrow = N, ncol = 4
)
#Use Lambda to create B matrix
B <- (Lambda) %*% t(Lambda)
B <- B + 1/10*diag(nrow = nrow(B))
# B is now a positive semi definite matrix
# It is therefore a valid covariance matrix
#Decompose it via cholesky to give the lower triangle matrix
# R by defualt gives the upper triangle, therefore it needs to be transposed
W <- t(chol(B))
return(W)
}
g1_A1_panel <- sim_panel_data(simN,
char_rho_a = 0.5, char_rho_b = 1,
cross_corr = 1, A1, xt_multi = 1, g_function = "g1", theta = matrix(c(0.007, 0.007, 0.007), nrow = 1),
error_sv = 1, error_ep_sd = 0.05,
error_omega = -0.736, error_gamma = 0.9, error_w = sqrt(0.363), error_v_sd = 0.05)
summary(sim_tune_statistics(g1_A1_panel))
pooled_panel <- g1_A1_panel[[1]]$panel
LM_stats_mse <- LM_fit(pooled_panel, timeSlices, "mse")
LM_stats_mse[[1]]$loss_stats
LM_stats_mse[[2]]$loss_stats
LM_stats_mse[[3]]$loss_stats
