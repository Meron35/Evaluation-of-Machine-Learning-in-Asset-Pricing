dataset2 <- list(sim_tune_stats = data.frame(time_series_fitted.rsquare = 0,
annual_vol = 0,
cross_section_rsquare = 0),
data = 0
)
datasets <- list(dataset, dataset2)
for (s in 1:sim_N) {
C_bar <- gen_C_bar()
C_hat <- gen_C_hat(C_bar)
if (cross_corr == 0) {
C <- gen_C(C_bar)
}
else {
C <- gen_C(C_hat)
}
if (xt_multi == 1) {
xt <- gen_xt(A_matrix)
}
else {
xt <- gen_xt_univariate()
}
#Generate the true underlying factors first
g_factor_panel <- gen_g_factor_panel(g_function, C, xt)
#Then pass them through to multiply them by theta to get g()
g_panel <- gen_g_panel(g_factor_panel, theta)
#Generate the errors
error <- gen_error(sv = error_sv, 0.05, omega = error_omega, gamma = error_gamma, w = error_w, C, v_sd = error_v_sd)
rt_panel <- g_panel + error
sim_tune_stats[s,] <- panel_tune_stats(rt_panel, g_factor_panel)
}
}
View(panel_g1_A1)
?createTimeSlices
start <- 2
end <- 181
initialWindow <- 108
horizon <- 12
test <- 36
knitr::opts_chunk$set(echo = TRUE)
################
##Load Libraries
################
library(glmnet)
library(tidyverse)
library(keras)
library(ggplot2)
library(caret)
library(forecast)
library(rlist)
library(plm)
library(Metrics)
library(ranger)
set.seed(27935248)
total <- c(start:end)
set_num <- (total - test - initialWindow) / horizon
offset <- start - 1
for (t in 1:set_num) {
train <- c(start:(initialWindow + (t-1) * horizon + offset))
validation <- c((initialWindow + (t-1) * horizon + offset + 1):(initialWindow + (t) * horizon + offset + 1))
test <- c((initialWindow + (t) * horizon + offset + 1 + 1):end)
}
total <- c(start:end)
set_num <- (end - start - test - initialWindow) / horizon
offset <- start - 1
for (t in 1:set_num) {
train <- c(start:(initialWindow + (t-1) * horizon + offset))
validation <- c((initialWindow + (t-1) * horizon + offset + 1):(initialWindow + (t) * horizon + offset + 1))
test <- c((initialWindow + (t) * horizon + offset + 1 + 1):end)
}
train
total <- c(start:end)
set_num <- (end - start - test - initialWindow) / horizon
offset <- start - 1
test <- 36
total <- c(start:end)
set_num <- (end - start - test - initialWindow) / horizon
test_size <- 36
t
total <- c(start:end)
set_num <- (end - start - test_size - initialWindow) / horizon
offset <- start - 1
for (t in 1:set_num) {
train <- c(start:(initialWindow + (t-1) * horizon + offset))
validation <- c((initialWindow + (t-1) * horizon + offset + 1):(initialWindow + (t) * horizon + offset + 1))
test <- c((initialWindow + (t) * horizon + offset + 1 + 1):end)
}
set_num <- (end - start - test_size - initialWindow) / horizon
offset <- start - 1
set_num <- (end - start + offset - test_size - initialWindow) / horizon
for (t in 1:set_num) {
train <- c(start:(initialWindow + (t-1) * horizon + offset))
validation <- c((initialWindow + (t-1) * horizon + offset + 1):(initialWindow + (t) * horizon + offset + 1))
test <- c((initialWindow + (t) * horizon + offset + 1 + 1):end)
}
train
validation
test
length(validation)
length(train)
length(test)
total <- c(start:end)
offset <- start - 1
set_num <- (end - start + offset - test_size - initialWindow) / horizon
for (t in 1:set_num) {
train <- c(start:(initialWindow + (t-1) * horizon + offset))
validation <- c((initialWindow + (t-1) * horizon + offset + 1):(initialWindow + (t) * horizon + offset))
test <- c((initialWindow + (t) * horizon + offset + 1):end)
}
length(test)
length(validation)
time_slice <- list(train = 0, validation = 0, test = 0)
View(time_slice)
time_slice$train
time_slices <- rep(time_slice, set_num)
time_slices <- list(time_slice, time_slice, time_slice)
time_slices <- list(1 = time_slice, 2 = time_slice, 3 = time_slice)
time_slice <- list(train = 0, validation = 0, test = 0)
for (t in 1:set_num) {
time_slice$train <- c(start:(initialWindow + (t-1) * horizon + offset))
time_slice$validation <- c((initialWindow + (t-1) * horizon + offset + 1):(initialWindow + (t) * horizon + offset))
time_slice$test <- c((initialWindow + (t) * horizon + offset + 1):end)
}
View(time_slices)
time_slices <- rep(time_slice, 3)
time_slices <- list(rep(time_slice, 3))
rep(time_slice, 3)
time_slices <- rep(list(time_slice), 3)
offset <- start - 1
set_num <- (end - start + offset - test_size - initialWindow) / horizon
time_slice <- list(train = 0, validation = 0, test = 0)
time_slices <- rep(list(time_slice), 3)
for (t in 1:set_num) {
time_slice$train <- c(start:(initialWindow + (t-1) * horizon + offset))
time_slice$validation <- c((initialWindow + (t-1) * horizon + offset + 1):(initialWindow + (t) * horizon + offset))
time_slice$test <- c((initialWindow + (t) * horizon + offset + 1):end)
time_slices[[1]] <- time_slice
}
for (t in 1:set_num) {
time_slice$train <- c(start:(initialWindow + (t-1) * horizon + offset))
time_slice$validation <- c((initialWindow + (t-1) * horizon + offset + 1):(initialWindow + (t) * horizon + offset))
time_slice$test <- c((initialWindow + (t) * horizon + offset + 1):end)
time_slices[[t]] <- time_slice
}
View(panel_g1_A1)
time_slices
timeSlices
#Create custom time slices
timeSlices <- customeTimeSlices(start = 2, end = 181, initialWindow = 108, horizon = 12, test_size = 36)
customTimeSlices <- function(start, end, initialWindow, horizon, test_size) {
total <- c(start:end)
offset <- start - 1
set_num <- (end - start + offset - test_size - initialWindow) / horizon
time_slice <- list(train = 0, validation = 0, test = 0)
time_slices <- rep(list(time_slice), 3)
for (t in 1:set_num) {
time_slice$train <- c(start:(initialWindow + (t-1) * horizon + offset))
time_slice$validation <- c((initialWindow + (t-1) * horizon + offset + 1):(initialWindow + (t) * horizon + offset))
time_slice$test <- c((initialWindow + (t) * horizon + offset + 1):end)
time_slices[[t]] <- time_slice
}
}
#Create custom time slices
timeSlices <- customTimeSlices(start = 2, end = 181, initialWindow = 108, horizon = 12, test_size = 36)
timeSlices
customTimeSlices <- function(start, end, initialWindow, horizon, test_size) {
total <- c(start:end)
offset <- start - 1
set_num <- (end - start + offset - test_size - initialWindow) / horizon
time_slice <- list(train = 0, validation = 0, test = 0)
time_slices <- rep(list(time_slice), 3)
for (t in 1:set_num) {
time_slice$train <- c(start:(initialWindow + (t-1) * horizon + offset))
time_slice$validation <- c((initialWindow + (t-1) * horizon + offset + 1):(initialWindow + (t) * horizon + offset))
time_slice$test <- c((initialWindow + (t) * horizon + offset + 1):end)
time_slices[[t]] <- time_slice
}
return(time_slices)
}
#Create custom time slices
timeSlices <- customTimeSlices(start = 2, end = 181, initialWindow = 108, horizon = 12, test_size = 36)
timeSlices
timeSlices
timeSlices[[1]]
pooled_panel <- panel_g1_A1
#Expand formula manually this way because PLM is dumb and gets confused by y ~ .
plm_formula <- function(panel){
colnames <- colnames(panel)
#Remove the first 3 colNames, as these correspond to the return, time and stock id
colnames <- colnames[-c(1:3)]
f <- as.formula(c("return ~ ", paste(colnames, collapse = "+")))
return(f)
}
f <- plm_formula(pooled_panel)
POLS_forecasts <- rep(list(0), 3)
POLS_models <- rep(list(0), 3)
train <- pooled_panel %>%
filter(time %in% timeSlices[[1]]$train)
View(train)
for (set in 1:3) {
#Load Training, validation and test sets
train <- pooled_panel %>%
filter(time %in% timeSlices[[set]]$train)
validation <- pooled_panel %>%
filter(time %in% timeSlices[[set]]$validation)
test<- pooled_panel %>%
filter(time %in% timeSlices[[set]]$test)
#Train Model on training set
pols <- plm(f, data = train, model = "pooling", index = c("time", "stock"))
POLS_models[[set]] <- pols
#No Tuning Needed
#Statistics
#Training Set Statistics
POLS_stats$train_MAE[set] <- mae(train$return, predict(pols))
POLS_stats$train_MSE[set] <- mse(train$return, predict(pols))
POLS_stats$train_RMSE[set] <- rmse(train$return, predict(pols))
error <- (train$return - predict(pols))
SSR <- t(error) %*% (error)
SST <- t(train$return - mean(train$return)) %*% (train$return - mean(train$return))
POLS_stats$train_R2[set] <- (1 - SSR/SST)
#Validation Set Statistics
POLS_stats$valid_MAE[set] <- mae(validation$return, predict(pols, newdata = validation))
POLS_stats$valid_MSE[set] <- mse(validation$return, predict(pols, newdata = validation))
POLS_stats$valid_RMSE[set] <- rmse(validation$return, predict(pols, newdata = validation))
error <- (validation$return - predict(pols, newdata = validation))
SSR <- t(error) %*% (error)
SST <- t(validation$return - mean(validation$return)) %*% (validation$return - mean(validation$return))
POLS_stats$valid_R2[set] <- (1 - SSR/SST)
#Test Set Statistics
POLS_stats$test_MAE[set] <- mae(test$return, predict(pols, newdata = test))
POLS_stats$test_MSE[set] <- mse(test$return, predict(pols, newdata = test))
POLS_stats$test_RMSE[set] <- rmse(test$return, predict(pols, newdata = test))
error <- (test$return - predict(pols, newdata = test))
SSR <- t(error) %*% (error)
SST <- t(test$return - mean(test$return)) %*% (test$return - mean(test$return))
POLS_stats$test_R2[set] <- (1 - SSR/SST)
#Forecasts
POLS_forecasts[[set]]$forecast <- predict(pols, newdata = test)
POLS_forecasts[[set]]$forecast_error <- test$return - predict(pols, newdata = test)
}
pols <- plm(f, data = train, model = "pooling", index = c("time", "stock"))
#Expand formula manually this way because PLM is dumb and gets confused by y ~ .
plm_formula <- function(panel){
colnames <- colnames(panel)
#Remove the first 3 colNames, as these correspond to the return, time and stock id
colnames <- colnames[-c(1:3)]
f <- as.formula(c(colnames(panel)[1], paste(colnames, collapse = "+")))
return(f)
}
f <- plm_formula(pooled_panel)
#Expand formula manually this way because PLM is dumb and gets confused by y ~ .
plm_formula <- function(panel){
colnames <- colnames(panel)
#Remove the first 3 colNames, as these correspond to the return, time and stock id
colnames <- colnames[-c(1:3)]
f <- as.formula(c(paste(colnames(panel)[1]), paste(colnames, collapse = "+")))
return(f)
}
f <- plm_formula(pooled_panel)
colnames(panel)[1]
colnames(panel)
colnames(pooled_panel)[1]
#Expand formula manually this way because PLM is dumb and gets confused by y ~ .
plm_formula <- function(panel){
colnames <- colnames(panel)
#Remove the first 3 colNames, as these correspond to the return, time and stock id
colnames <- colnames[-c(1:3)]
f <- as.formula(c("rt", paste(colnames, collapse = "+")))
return(f)
}
f <- plm_formula(pooled_panel)
#Expand formula manually this way because PLM is dumb and gets confused by y ~ .
plm_formula <- function(panel){
panel_colnames <- colnames(panel)
#Remove the first 3 colNames, as these correspond to the return, time and stock id
colnames <- panel_colnames[-c(1:3)]
f <- as.formula(c("rt", paste(colnames, collapse = "+")))
return(f)
}
f <- plm_formula(pooled_panel)
pnale <- pooled_panel
panel <- pooled_panel
panel_colnames <- colnames(panel)
#Remove the first 3 colNames, as these correspond to the return, time and stock id
colnames <- panel_colnames[-c(1:3)]
#Remove the first 3 colNames, as these correspond to the return, time and stock id
panel_colnames <- panel_colnames[-c(1:3)]
f <- as.formula(c("rt", paste(panel_colnames, collapse = "+")))
#Expand formula manually this way because PLM is dumb and gets confused by y ~ .
plm_formula <- function(panel){
panel_colnames <- colnames(panel)
#Remove the first 3 colNames, as these correspond to the return, time and stock id
panel_colnames <- panel_colnames[-c(1:3)]
f <- as.formula(c("rt", paste(panel_colnames, collapse = "+")))
return(f)
}
f <- plm_formula(pooled_panel)
paste(panel_colnames, collapse = "+")
c("rt", paste(panel_colnames, collapse = "+"))
#Expand formula manually this way because PLM is dumb and gets confused by y ~ .
plm_formula <- function(panel){
#Remove the first 3 colNames, as these correspond to the return, time and stock id
panel_colnames <- colnames(panel)[-c(1:3)]
f <- as.formula(c("rt", paste(panel_colnames, collapse = "+")))
return(f)
}
f <- plm_formula(pooled_panel)
colnames(panel)[-c(1:3)]
f <- plm_formula(pooled_panel)
plm_formula(pooled_panel)
install.packages("devtools", lib="C:/Program Files/R/R-3.5.0/library")
knitr::opts_chunk$set(echo = TRUE)
################
##Load Libraries
################
library(glmnet)
library(tidyverse)
library(keras)
library(ggplot2)
library(caret)
library(forecast)
library(rlist)
library(plm)
library(Metrics)
library(ranger)
library(glmnetUtils)
install.packages("glmnetUtils", lib="C:/Program Files/R/R-3.5.0/library")
knitr::opts_chunk$set(echo = TRUE)
################
##Load Libraries
################
library(glmnet)
library(tidyverse)
library(keras)
library(ggplot2)
library(caret)
library(forecast)
library(rlist)
library(plm)
library(Metrics)
library(ranger)
library(glmnetUtils)
#forestr
devtools::install_github("andeek/forestr")
devtools::install_github("andeek/rpart")
devtools::install_github("andeek/forestr")
devtools::install_github("andeek/forestr")
devtools::install_github("andeek/forestr")
Sys.getenv("R_LIBS_USER")
devtools::install_github("andeek/forestr")
knitr::opts_chunk$set(echo = TRUE)
################
##Load Libraries
################
library(glmnet)
library(tidyverse)
library(keras)
library(ggplot2)
library(caret)
library(forecast)
library(rlist)
library(plm)
library(Metrics)
library(ranger)
library(glmnetUtils)
#forestr
# This apparently needs admin privileges to install, do this at home maybe
# devtools::install_github("andeek/forestr")
# devtools::install_github("andeek/rpart")
set.seed(27935248)
library(tensorflow)
#Neural Networks
set.seed(27935248)
#Build neural networks with neuron numbers according to geometric pyramid rule, and ReLU activation function for all layers
# IE Input layer > 32 neurons > 16 neurons > 8 neurons > 4 neurons > 2 neurons > output
##Neural Network 1
train_x <- train[4:ncol(train)]
train_y <- train$rt
validation_x <- validation[4:ncol(validation)]
validation_y <- validation$rt
# The patience parameter is the amount of epochs to check for improvement.
early_stop <- callback_early_stopping(monitor = "val_loss", patience = 20)
#It's clearer to separate these "layers" out as much as possible
build_NN1 <- function() {
model <- keras_model_sequential() %>%
# Layer 1
layer_dense(units = 32, input_shape = ncol(train_x)) %>%
layer_activation("relu") %>%
layer_activity_regularization(l1 = 0.001) %>%
layer_batch_normalization() %>%
# Output Layer
layer_dense(units = 1, activation = "linear")
model %>% compile(
#MSE loss function
loss = "mse",
#We're using ADAM
optimizer = "adam",
metrics = list("mean_absolute_error", "mean_squared_error")
)
model
}
neural_network_1 <- build_NN1()
neural_network_1 %>% summary()
#Just do something straightforward for now
neural_network_1 %>% fit(as.matrix(train_x), as.matrix(train_y),
epochs = 100, verbose = 1,
validation_data = list(as.matrix(validation_x), as.matrix(validation_y)),
callbacks = list(early_stop))
#Make predictions
test_x <- test[4:ncol(test)]
NN1_predictions <- neural_network_1 %>% predict(as.matrix(test_x))
mae(test$rt, NN2_predictions)
install.packages("quantregForest")
install.packages("quantreg")
knitr::opts_chunk$set(echo = TRUE)
################
##Load Libraries
################
library(glmnet)
library(tidyverse)
library(keras)
library(ggplot2)
library(caret)
library(forecast)
library(rlist)
library(plm)
library(Metrics)
library(ranger)
library(glmnetUtils)
library(tensorflow)
library(quantregForest)
library(quantreg)
#forestr
# This apparently needs admin privileges to install, do this at home maybe
# devtools::install_github("andeek/forestr")
# devtools::install_github("andeek/rpart")
set.seed(27935248)
install.packages("randomForestSRC")
knitr::opts_chunk$set(echo = TRUE)
################
##Load Libraries
################
library(glmnet)
library(tidyverse)
library(keras)
library(ggplot2)
library(caret)
library(forecast)
library(rlist)
library(plm)
library(Metrics)
library(ranger)
library(glmnetUtils)
library(tensorflow)
library(quantregForest)
library(randomForestSRC)
library(quantreg)
#forestr
# This apparently needs admin privileges to install, do this at home maybe
# devtools::install_github("andeek/forestr")
# devtools::install_github("andeek/rpart")
set.seed(27935248)
library(randomForestSRC)
?quantileReg.rfsrc
?quantileReg
?rfsrc
?quantileReg
?quantreg
library(randomForestSRC)
knitr::opts_chunk$set(echo = TRUE)
################
##Load Libraries
################
library(glmnet)
library(tidyverse)
library(keras)
library(ggplot2)
library(caret)
library(forecast)
library(rlist)
library(plm)
library(Metrics)
library(ranger)
library(glmnetUtils)
library(tensorflow)
library(quantreg)
library(randomForestSRC)
#forestr
# This apparently needs admin privileges to install, do this at home maybe
# devtools::install_github("andeek/forestr")
# devtools::install_github("andeek/rpart")
set.seed(27935248)
?quantileReg
rf_MAE <- rfsrc(f, train, ntree = 500,
#Hyperparameters
mtry = 10,
nodesize = 5,
#Other
splitrule = "quantile.regr")
panel_formula <- function(panel){
#Remove the first 3 colNames, as these correspond to the return, time and stock id
panel_colnames <- colnames(panel)[-c(1:3)]
f <- as.formula(c("rt ~", paste(panel_colnames, collapse = "+")))
return(f)
}
pooled_panel <- panel_g1_A1
f <- panel_formula(pooled_panel)
rf_MAE <- rfsrc(f, train, ntree = 500,
#Hyperparameters
mtry = 10,
nodesize = 5,
#Other
splitrule = "quantile.regr")
