pred <- t(apply(pred, 1, softmax_transform))
View(pred)
xgb.plot.importance(bst)
xgb.importance(feature_names = colnames(train_list$data),
model = bst)
bst
labels <- foreach(i = 1:nrow(errors), .combine = "rbind") %dopar% {
data.frame(label = which.min(errors[i, ]) - 1)
}
## Create final list with everything needed in FFORMA
## Looks pretty good
train_list <- list(
data = data,
errors = errors,
labels = labels
)
param <- list(max_depth = 14, eta = 0.575188, nthread = 12, silent = 0,
objective = error_softmax_obj,
num_class = ncol(errors),
subsample = 0.9161483,
colsample_bytree = 0.7670739)
xgb.iter.update <- function(booster_handle, dtrain, iter, obj = NULL) {
if (!identical(class(booster_handle), "xgb.Booster.handle")) {
stop("booster_handle must be of xgb.Booster.handle class")
}
if (!inherits(dtrain, "xgb.DMatrix")) {
stop("dtrain must be of xgb.DMatrix class")
}
if (is.null(obj)) {
.Call(XGBoosterUpdateOneIter_R, booster_handle, as.integer(iter), dtrain)
} else {
pred <- predict(booster_handle, dtrain)
gpair <- obj(pred, dtrain)
.Call(XGBoosterBoostOneIter_R, booster_handle, dtrain, gpair$grad, gpair$hess)
}
return(TRUE)
}
dtrain <- xgb.DMatrix(data = as.matrix(train_list$data),
label = train_list$labels[, 1])
attr(dtrain, "errors") <- train_list$errors
bst <- xgboost::xgb.train(param, dtrain, 94,
verbose = 1)
bst
param <- list(max_depth = 14, eta = 0.575188, nthread = 12, silent = 0,
objective = error_softmax_obj,
num_class = ncol(errors),
subsample = 0.9161483,
colsample_bytree = 0.7670739)
bst <- xgboost::xgb.train(param, dtrain, 94,
verbose = 1)
bst
pred <- predict(bst, newdata = xgboost::xgb.DMatrix(as.matrix(train_list$data)), outputmargin = TRUE, reshape=TRUE)
pred <- t(apply(pred, 1, softmax_transform))
View(pred)
xgb.importance(feature_names = colnames(train_list$data),
model = bst)
bst <- xgboost::xgb.train(NULL, dtrain, 94,
verbose = 1)
bst <- xgboost::xgb.train(dtrain, 94,
verbose = 1)
param <- list(max_depth = 14, eta = 0.575188, nthread = 12, silent = 0,
objective = error_softmax_obj,
num_class = ncol(errors),
subsample = 0.9161483,
colsample_bytree = 0.7670739)
param <- list(max_depth = 14, eta = 0.575188, nthread = 12, silent = 0,
objective = error_softmax_obj,
num_class = ncol(errors),
subsample = 0.9161483,
colsample_bytree = 0.7670739,
eval_metric = mlogloss)
param <- list(max_depth = 14, eta = 0.575188, nthread = 12, silent = 0,
objective = error_softmax_obj,
num_class = ncol(errors),
subsample = 0.9161483,
colsample_bytree = 0.7670739,
eval_metric = "mlogloss")
dtrain <- xgb.DMatrix(data = as.matrix(train_list$data),
label = train_list$labels[, 1])
attr(dtrain, "errors") <- train_list$errors
bst <- xgboost::xgb.train(params, dtrain, 94,
verbose = 1)
bst <- xgboost::xgb.train(param, dtrain, 94,
verbose = 1)
bst
param <- list(booster = "gbtree",
max_depth = 14, eta = 0.3, nthread = 12,
objective = error_softmax_obj,
num_class = ncol(errors),
subsample = 0.9161483,
colsample_bytree = 0.7670739,
eval_metric = "mlogloss")
bst <- xgboost::xgb.train(param, dtrain, 94,
verbose = 1)
bst
pred <- predict(bst, newdata = xgboost::xgb.DMatrix(as.matrix(train_list$data)), outputmargin = TRUE, reshape=TRUE)
pred <- t(apply(pred, 1, softmax_transform))
pred
param <- list(booster = "gbtree",
max_depth = 6, eta = 0.3, nthread = 12,
objective = error_softmax_obj,
num_class = ncol(errors),
subsample = 0.9161483,
colsample_bytree = 0.7670739,
eval_metric = "mlogloss")
dtrain <- xgb.DMatrix(data = as.matrix(train_list$data),
label = train_list$labels[, 1])
attr(dtrain, "errors") <- train_list$errors
bst <- xgboost::xgb.train(param, dtrain, 94,
verbose = 1)
bst <- xgboost::xgb.train(param, dtrain, 94,
verbose = 1)
bst
pred <- predict(bst, newdata = xgboost::xgb.DMatrix(as.matrix(train_list$data)), outputmargin = TRUE, reshape=TRUE)
pred <- t(apply(pred, 1, softmax_transform))
pred
param <- list(booster = "gbtree",
max_depth = 6, eta = 0.3, nthread = 12,
objective = error_softmax_obj,
num_class = ncol(errors),
subsample = 1,
colsample_bytree = 1,
eval_metric = "mlogloss")
bst <- xgboost::xgb.train(param, dtrain, 94,
verbose = 1)
bst
pred <- predict(bst, newdata = xgboost::xgb.DMatrix(as.matrix(train_list$data)), outputmargin = TRUE, reshape=TRUE)
pred <- t(apply(pred, 1, softmax_transform))
pred
### Errors
# This is trickier
errors <- data.frame(naive = mse(as.vector(univariate_time_series_test), predict(naive_model)$mean),
random_walk = mse(as.vector(univariate_time_series_test), predict(random_walk_model)$mean),
theta = mse(as.vector(univariate_time_series_test), predict(theta_model)$mean),
auto_arima = mse(as.vector(univariate_time_series_test),
forecast(auto_arima_model, h = 12)$mean),
ets = mse(as.vector(univariate_time_series_test), forecast(auto_ets_model, h = 12)$mean),
tbats = mse(as.vector(univariate_time_series_test), forecast(tbats_model, h = 12)$mean),
nnetar = mse(as.vector(univariate_time_series_test), forecast(nnetar_model, h = 12)$mean),
garch_11 = mse(as.vector(univariate_time_series_test), fitted(ugarch_forecast)[1, ]),
garch_11_t = mse(as.vector(univariate_time_series_test), fitted(ugarch_t_forecast)[1, ]),
stochvol = mse(as.vector(univariate_time_series_test),
apply(X = predict(stochvol_model, steps = 12)$y, MARGIN = 2, FUN = mean)))
errors <- foreach(i = 1:length(unique(pooled_panel$stock)), .combine = "rbind") %dopar% {
## Filter Data first
stock_id <- unique(pooled_panel$stock)
univariate_time_series <- pooled_panel %>%
filter(stock == stock_id[i]) %>%
filter(time %in% timeSlices[[1]]$train | time %in% timeSlices[[1]]$validation) %>%
select(rt) %>%
ts()
univariate_time_series_test <- pooled_panel %>%
filter(stock == stock_id[i]) %>%
filter(time %in% timeSlices[[1]]$test) %>%
select(rt) %>%
ts()
## Estimation of consituent models
# Naive
naive_model <- naive(univariate_time_series, h = 12)
# Random walk with drift
random_walk_model <- rwf(univariate_time_series, h = 12, drift = TRUE)
# Seasonal Naive, this ends up being the same as naive
seasonal_naive_model <- snaive(univariate_time_series, h = 12)
# Theta Method
theta_model <- thetaf(univariate_time_series, h = 12)
## Auto arima
auto_arima_model <- auto.arima(univariate_time_series)
# Auto ETS
auto_ets_model <- ets(univariate_time_series)
# TBATS
tbats_model <- tbats(univariate_time_series)
# Neural Network time series forecasts
nnetar_model <- nnetar(univariate_time_series)
# Naive model that forecasts 0
# Neat idea, but practically speaking ends up being the sam eas auto arima's 0, 0, 0 model most of the time
# naive0_model <- rep(0, 12)
# mse(as.vector(univariate_time_series_test), naive0_model)
# ARMA 1, 1 with GARCH 1, 1 gaussian errors
ugarch_spec <- ugarchspec(variance.model = list(model = "sGARCH",
garchOrder = c(1, 1),
submodel = NULL,
external.regressors = NULL,
variance.targeting = FALSE),
mean.model     = list(armaOrder = c(1, 1),
include.mean = TRUE,
external.regressors = NULL),
distribution.model = "norm")
ugarch_spec_fit <- ugarchfit(ugarch_spec, data = univariate_time_series, out.sample = 12,
solver = "hybrid")
ugarch_forecast <- ugarchforecast(ugarch_spec_fit, n.ahead = 12, n.roll = 11)
## Stochastic volatility model
## Theoretically the most correctly specified model, but very computatioally intensive, and may be infeasible
## Takes ~ 14 seconds to estimate
stochvol_model <- svlsample(univariate_time_series, draws = 10000, burnin = 1000, designmatrix = "ar1",
priormu = c(0, 100), priorphi = c(5, 1.5), priorsigma = 1,
priorrho = c(4, 4), priorbeta = c(0, 10000), thinpara = 1,
thinlatent = 1, thintime = NULL, keeptime = "all", quiet = TRUE)
## Put errors into a single row
errors <- data.frame(naive = mse(as.vector(univariate_time_series_test), predict(naive_model)$mean),
random_walk = mse(as.vector(univariate_time_series_test), predict(random_walk_model)$mean),
theta = mse(as.vector(univariate_time_series_test), predict(theta_model)$mean),
auto_arima = mse(as.vector(univariate_time_series_test),
forecast(auto_arima_model, h = 12)$mean),
ets = mse(as.vector(univariate_time_series_test), forecast(auto_ets_model, h = 12)$mean),
tbats = mse(as.vector(univariate_time_series_test), forecast(tbats_model, h = 12)$mean),
nnetar = mse(as.vector(univariate_time_series_test), forecast(nnetar_model, h = 12)$mean),
garch_11 = mse(as.vector(univariate_time_series_test), fitted(ugarch_forecast)[1, ]),
stochvol = mse(as.vector(univariate_time_series_test),
apply(X = predict(stochvol_model, steps = 12)$y, MARGIN = 2, FUN = mean)))
errors
}
library(stochvol)
errors <- foreach(i = 1:length(unique(pooled_panel$stock)), .combine = "rbind") %dopar% {
## Filter Data first
stock_id <- unique(pooled_panel$stock)
univariate_time_series <- pooled_panel %>%
filter(stock == stock_id[i]) %>%
filter(time %in% timeSlices[[1]]$train | time %in% timeSlices[[1]]$validation) %>%
select(rt) %>%
ts()
univariate_time_series_test <- pooled_panel %>%
filter(stock == stock_id[i]) %>%
filter(time %in% timeSlices[[1]]$test) %>%
select(rt) %>%
ts()
## Estimation of consituent models
# Naive
naive_model <- naive(univariate_time_series, h = 12)
# Random walk with drift
random_walk_model <- rwf(univariate_time_series, h = 12, drift = TRUE)
# Seasonal Naive, this ends up being the same as naive
seasonal_naive_model <- snaive(univariate_time_series, h = 12)
# Theta Method
theta_model <- thetaf(univariate_time_series, h = 12)
## Auto arima
auto_arima_model <- auto.arima(univariate_time_series)
# Auto ETS
auto_ets_model <- ets(univariate_time_series)
# TBATS
tbats_model <- tbats(univariate_time_series)
# Neural Network time series forecasts
nnetar_model <- nnetar(univariate_time_series)
# Naive model that forecasts 0
# Neat idea, but practically speaking ends up being the sam eas auto arima's 0, 0, 0 model most of the time
# naive0_model <- rep(0, 12)
# mse(as.vector(univariate_time_series_test), naive0_model)
# ARMA 1, 1 with GARCH 1, 1 gaussian errors
ugarch_spec <- ugarchspec(variance.model = list(model = "sGARCH",
garchOrder = c(1, 1),
submodel = NULL,
external.regressors = NULL,
variance.targeting = FALSE),
mean.model     = list(armaOrder = c(1, 1),
include.mean = TRUE,
external.regressors = NULL),
distribution.model = "norm")
ugarch_spec_fit <- ugarchfit(ugarch_spec, data = univariate_time_series, out.sample = 12,
solver = "hybrid")
ugarch_forecast <- ugarchforecast(ugarch_spec_fit, n.ahead = 12, n.roll = 11)
## Stochastic volatility model
## Theoretically the most correctly specified model, but very computatioally intensive, and may be infeasible
## Takes ~ 14 seconds to estimate
stochvol_model <- svlsample(univariate_time_series, draws = 10000, burnin = 1000, designmatrix = "ar1",
priormu = c(0, 100), priorphi = c(5, 1.5), priorsigma = 1,
priorrho = c(4, 4), priorbeta = c(0, 10000), thinpara = 1,
thinlatent = 1, thintime = NULL, keeptime = "all", quiet = TRUE)
## Put errors into a single row
errors <- data.frame(naive = mse(as.vector(univariate_time_series_test), predict(naive_model)$mean),
random_walk = mse(as.vector(univariate_time_series_test), predict(random_walk_model)$mean),
theta = mse(as.vector(univariate_time_series_test), predict(theta_model)$mean),
auto_arima = mse(as.vector(univariate_time_series_test),
forecast(auto_arima_model, h = 12)$mean),
ets = mse(as.vector(univariate_time_series_test), forecast(auto_ets_model, h = 12)$mean),
tbats = mse(as.vector(univariate_time_series_test), forecast(tbats_model, h = 12)$mean),
nnetar = mse(as.vector(univariate_time_series_test), forecast(nnetar_model, h = 12)$mean),
garch_11 = mse(as.vector(univariate_time_series_test), fitted(ugarch_forecast)[1, ]),
stochvol = mse(as.vector(univariate_time_series_test),
apply(X = predict(stochvol_model, steps = 12)$y, MARGIN = 2, FUN = mean)))
errors
}
labels <- foreach(i = 1:nrow(errors), .combine = "rbind") %dopar% {
data.frame(label = which.min(errors[i, ]) - 1)
}
## Create final list with everything needed in FFORMA
## Looks pretty good
train_list <- list(
data = data,
errors = errors,
labels = labels
)
param <- list(booster = "gbtree",
max_depth = 6, eta = 0.3, nthread = 12,
objective = error_softmax_obj,
num_class = ncol(errors),
subsample = 1,
colsample_bytree = 1,
eval_metric = "mlogloss")
dtrain <- xgb.DMatrix(data = as.matrix(train_list$data),
label = train_list$labels[, 1])
attr(dtrain, "errors") <- train_list$errors
bst <- xgboost::xgb.train(param, dtrain, 94,
verbose = 1)
bst
pred <- predict(bst, newdata = xgboost::xgb.DMatrix(as.matrix(train_list$data)), outputmargin = TRUE, reshape=TRUE)
pred <- t(apply(pred, 1, softmax_transform))
pred
train_list$data
xgb.importance(feature_names = colnames(train_list$data),
model = bst)
?xgb.ggplot.importance
xgb.importance(feature_names = colnames(train_list$data),
model = bst)
xgb.importance(model = bst)
xgb.ggplot.importance(bst)
xgb.importance(model = bst)
param <- list(booster = "gbtree",
max_depth = 6, eta = 0.3, nthread = 12,
objective = error_softmax_obj,
num_class = ncol(errors),
subsample = 1,
colsample_bytree = 1,
eval_metric = "mlogloss")
dtrain <- xgb.DMatrix(data = as.matrix(train_list$data),
label = train_list$labels[, 1])
attr(dtrain, "errors") <- train_list$errors
bst <- xgboost::xgb.train(param, dtrain, 94,
verbose = 1)
bst <- xgboost::xgb.train(param, dtrain, 94,
verbose = 1)
bst
?xgb.train
param <- list(booster = "gbtree",
max_depth = 6, eta = 0.3, nthread = 12,
objective = "multi:softprob",
num_class = ncol(errors),
subsample = 1,
colsample_bytree = 1,
eval_metric = "mlogloss")
bst <- xgboost::xgb.train(param, dtrain, 94,
verbose = 1)
bst
pred <- predict(bst, newdata = xgboost::xgb.DMatrix(as.matrix(train_list$data)), outputmargin = TRUE, reshape=TRUE)
pred <- t(apply(pred, 1, softmax_transform))
pred
param <- list(booster = "gbtree",
max_depth = 6, eta = 0.3, nthread = 12,
objective = "multi:softmax",
num_class = ncol(errors),
subsample = 1,
colsample_bytree = 1,
eval_metric = "mlogloss")
dtrain <- xgb.DMatrix(data = as.matrix(train_list$data),
label = train_list$labels[, 1])
attr(dtrain, "errors") <- train_list$errors
bst <- xgboost::xgb.train(param, dtrain, 94,
verbose = 1)
bst
pred <- predict(bst, newdata = xgboost::xgb.DMatrix(as.matrix(train_list$data)), outputmargin = TRUE, reshape=TRUE)
pred
param <- list(booster = "gbtree",
max_depth = 6, eta = 0.3, nthread = 12,
num_class = ncol(errors),
subsample = 1,
colsample_bytree = 1,
eval_metric = "mlogloss")
bst <- xgboost::xgb.train(param, dtrain, 94,
verbose = 1)
bst
pred <- predict(bst, newdata = xgboost::xgb.DMatrix(as.matrix(train_list$data)), outputmargin = TRUE, reshape=TRUE)
pred
pred <- t(apply(pred, 1, softmax_transform))
pred
param <- list(booster = "gbtree",
max_depth = 6, eta = 0.3, nthread = 12,
objective = error_softmax_obj,
subsample = 1,
colsample_bytree = 1,
eval_metric = "mlogloss")
bst <- xgboost::xgb.train(param, dtrain, 94,
verbose = 1)
error_softmax_obj <- function(preds, dtrain) {
labels <- xgboost::getinfo(dtrain, "label")
errors <- attr(dtrain, "errors")
preds <- exp(preds)
sp <- rowSums(preds)
preds <- preds / replicate(ncol(preds), sp)
rowsumerrors <- replicate( ncol(preds), rowSums(preds * errors))
grad <- preds*(errors - rowsumerrors)
hess <- errors*preds*(1.0-preds) - grad*preds
#hess <- grad*(1.0 - 2.0*preds)
#hess <- pmax(hess, 1e-16)
#the true hessian should be grad*(1.0 - 2.0*preds) but it produces numerical problems
#what we use here is a upper bound
#print(mean(rowSums(preds*errors)))
return(list(grad = t(grad), hess = t(hess)))
}
bst <- xgboost::xgb.train(param, dtrain, 94,
verbose = 1)
attr(dtrain, "errors") <- train_list$errors
bst <- xgboost::xgb.train(param, dtrain, 94,
verbose = 1)
param <- list(booster = "gbtree",
max_depth = 6, eta = 0.3, nthread = 12,
objective = error_softmax_obj,
subsample = 1,
colsample_bytree = 1,
eval_metric = "mlogloss")
dtrain <- xgb.DMatrix(data = as.matrix(train_list$data),
label = train_list$labels[, 1])
attr(dtrain, "errors") <- train_list$errors
bst <- xgboost::xgb.train(param, dtrain, 94,
verbose = 1)
param <- list(booster = "gbtree",
max_depth = 6, eta = 0.3, nthread = 12,
num_class = ncol(errors),
objective = error_softmax_obj,
subsample = 1,
colsample_bytree = 1,
eval_metric = "mlogloss")
bst <- xgboost::xgb.train(param, dtrain, 94,
verbose = 1)
bst
pred <- predict(bst, newdata = xgboost::xgb.DMatrix(as.matrix(train_list$data)), outputmargin = TRUE, reshape=TRUE)
pred
attr(dtrain, "errors")
?replicate
################
##Load Libraries
################
library(speedglm)
library(tidyverse)
library(keras)
library(ggplot2)
library(forecast)
library(rlist)
library(Metrics)
library(tensorflow)
library(quantreg)
library(randomForestSRC)
library(hqreg)
library(caret)
library(tsfeatures)
## Vector arima/garch packages
library(kernlab)
library(xts)
library(tsne)
library(rmgarch)
library(marima)
library(xgboost)
#Parallel Computing
library(foreach)
library(doFuture)
#Registering
registerDoFuture()
plan(multisession)
error_softmax_obj <- function(preds, dtrain) {
labels <- xgboost::getinfo(dtrain, "label")
errors <- attr(dtrain, "errors")
preds <- exp(preds)
sp <- rowSums(preds)
preds <- preds / replicate(ncol(preds), sp)
rowsumerrors <- replicate(ncol(preds), rowSums(preds * errors))
grad <- preds*(errors - rowsumerrors)
hess <- errors*preds*(1.0-preds) - grad*preds
#hess <- grad*(1.0 - 2.0*preds)
#hess <- pmax(hess, 1e-16)
#the true hessian should be grad*(1.0 - 2.0*preds) but it produces numerical problems
#what we use here is a upper bound
#print(mean(rowSums(preds*errors)))
return(list(grad = t(grad), hess = t(hess)))
}
param <- list(booster = "gbtree",
max_depth = 6, eta = 0.3, nthread = 12,
num_class = ncol(errors),
objective = error_softmax_obj,
subsample = 1,
colsample_bytree = 1)
dtrain <- xgb.DMatrix(data = as.matrix(train_list$data),
label = train_list$labels[, 1])
attr(dtrain, "errors") <- train_list$errors
bst <- xgboost::xgb.train(param, dtrain, 94,
verbose = 1)
softmax_transform <- function(x) {
exp(x) / sum(exp(x))
}
error_softmax_obj <- function(preds, dtrain) {
labels <- xgboost::getinfo(dtrain, "label")
errors <- attr(dtrain, "errors")
preds <- exp(preds)
sp <- rowSums(preds)
preds <- preds / replicate(ncol(preds), sp)
rowsumerrors <- replicate(ncol(preds), rowSums(preds * errors))
grad <- preds*(errors - rowsumerrors)
hess <- errors*preds*(1.0-preds) - grad*preds
#hess <- grad*(1.0 - 2.0*preds)
#hess <- pmax(hess, 1e-16)
#the true hessian should be grad*(1.0 - 2.0*preds) but it produces numerical problems
#what we use here is a upper bound
#print(mean(rowSums(preds*errors)))
return(list(grad = t(grad), hess = t(hess)))
}
param <- list(booster = "gbtree",
max_depth = 6, eta = 0.3, nthread = 12,
num_class = ncol(errors),
objective = error_softmax_obj,
subsample = 1,
colsample_bytree = 1)
dtrain <- xgb.DMatrix(data = as.matrix(train_list$data),
label = train_list$labels[, 1])
attr(dtrain, "errors") <- train_list$errors
bst <- xgboost::xgb.train(param, dtrain, 94,
verbose = 1)
