---
title: "Simulation"
author: "Ze Yu Zhong"
date: "21 March 2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(glmnet)
library(tidyverse)
library(keras)
library(ggplot2)
```


Simulate a latent factor model with stochastic volatility for excess return, $r_{t+1}$, for $t=1,\dots,T$:

\begin{flalign*}
r_{i, t+1}&=g\left(z_{i, t}\right)+\beta_{i,t+1}v_{t+1}+e_{i, t+1}, \quad z_{i, t}=\left(1, x_{t}\right)^{\prime} \otimes c_{i, t}, \quad \beta_{i, t}=\left(c_{i 1, t}, c_{i 2, t}, c_{i 3, t}\right)\\ e_{i, t+1}&=\exp(\sigma_{i, t+1}/2)\varepsilon_{i, t+1},\\\sigma^2_{i,t+1}&=\omega+\alpha_{i}e_{i,t+1}^{2}+\gamma_i\sigma^2_{t,i}+w_{i,t+1}.
\end{flalign*}

Let $v_{t+1}$ be a $3\times 1$ vector of errors, and $w_{i,t+1},\varepsilon_{i,t+1}$ scalar error terms. The matrix $C_t$ is an $N\times P_c$ vector of latent factors, where the first three columns correspond to $\beta_{i,t}$, across the $1\leq i\leq N$ dimensions, while the remaining $P_c-3$ factors do not enter the return equation. The $P_x\times1$ vector $x_t$ is a multivariate time series, and $\varepsilon_{t+1}$ is a $N\times 1$ vector of idiosyncratic errors. 



One of my key concerns with the Gu et al. (2019) design is that the factors are uncorrelated across $i$, and, in particular, that the factors which do not matter in the return equation are uncorrelated with those that matter. This is not what is observed in practice. 

Instead, we will choose a simulation mechanism for $C_t$ that gives some correlation across the factors and across time. To that end, first consider drawing normal random numbers for each $1\leq i\leq N$ and $1\leq j\leq P_{c}$, according to 
$$\overline{c}_{i j, t}=\rho_{j} \overline{c}_{i j, t-1}+\epsilon_{i j, t}, \;\rho_{j}\mathcal{U}[1/2,1].$$Then, define the matrix $$
B:=\Lambda\Lambda'+\frac{1}{10}\mathbb{I}_{n},\;\Lambda_i=(\lambda_{i1},\dots,\lambda_{i4})',\;\lambda_{ik}\sim N(0,1),\; k=1,\dots,4, $$ which we transform into a correlation matrix $W$ via $$W=\text{diag}^{-1/2}(W)W\text{diag}^{-1/2}(W).$$
To build in cross-sectional correlation, from the $N\times P_{c}$ matrix $\bar{C}_t$, we simulate characteristics according to $$\widehat{C}_{t}=W\overline{C}_{t}.$$
Finally, we can construct the ``observed'' characteristics for each $1\leq i\leq N$ and for $j=1,\dots,P_{c}$ according to  $$c_{i j, t}=\frac{2}{n+1} \operatorname{rank}\left(\overline{c}_{i j, t}\right)-1.$$

For simulation of $x_{t}$ we consider a VAR model
\begin{flalign*}
x_{t}=Ax_{t-1}+u_t,
\end{flalign*}where we have three separate specifications for the matrix $A$:
\begin{flalign*}
(1)&\; A=\begin{pmatrix}.95&0&0\\0&.95&0\\0&0&.95\end{pmatrix}\;\;
(2)\; A=\begin{pmatrix}1&0&.25\\0&.95&0\\.25&0&.95\end{pmatrix}\;\;
(3)\; A=\begin{pmatrix}.99&.2&.1\\.2&.90&-.3\\.1&-.3&-.99\end{pmatrix}\end{flalign*}


We will consider four different functions 
$g(\cdot)$
\begin{flalign*}(1)\; & g\left(z_{i, t}\right)=\left(c_{i 1, t}, c_{i 2, t}, c_{i 3, t} \times x_{t}'\right) \theta_{0}, \;\text { where } \theta_{0}=(0.02,0.02,0.02)^{\prime}\\(2)\;&g\left(z_{i, t}\right)=\left(c_{i 1, t}^{2}, c_{i 1, t} \times c_{i 2, t}, \operatorname{sgn}\left(c_{i 3, t} \times  x_{t}'\right)\right) \theta_{0}, \; \text { where } \; \theta_{0}=(0.04,0.035,0.01)^{\prime} \\(3)\; & g\left(z_{i, t}\right)=\left(1[c_{i3,t}>0],c_{i 2, t}^{3}, c_{i 1, t} \times c_{i 2, t}\times 1[c_{i3,t}>0], \text{logit}\left({c}_{i 3, t} \right)\right) \theta_{0}, \;\text { where } \; \theta_{0}=(0.04,0.035,0.01)^{\prime}  \\(4)\; &g\left(z_{i, t}\right)=\left(\hat{c}_{i 1, t}, \hat{c}_{i 2, t}, \hat{c}_{i 3, t} \times x_{t}'\right) \theta_{0}, \;\text { where } \theta_{0}=(0.02,0.02,0.02)^{\prime}
\end{flalign*}

Need to work out the corresponding cr0ss-sectional $R^2$ in this case. We can then tune $\theta^0$ to be this close to Gu et al. (2019), as well as the predictive $R^2$. This will require some work. 

Follow Gu et al. (2019) in regards to the choice of $N,T,P_{c}$

```{r}
#Simulation
N <- 200
P_c <- 100

#Errors
epsilon <- c(rnorm(N, 0, 1))
w <- c(rnorm(N, 0, 1))
sigma2 <- 
```



```{r}
#characteristics C


elm <- matrix(
  data = 0, nrow = N, ncol = P_c
)

Ct <- rep(list(elm), 180)

for (t in 1:179) {
  for (j in 1:P_c) {
    rho <- runif(1, 1/2, 1)
    for (i in 1:N) {
      Ct[[t+1]][i, j] <- Ct[[t]][i, j]*rho + rnorm(1, 0, 1)
    }
  }  
}

#B and lambda matrix
Lambda <- matrix(
  data = rnorm(N*4, 0, 1),
  nrow = N, ncol = 4
)

B <- (Lambda)%*%t(Lambda)
B <- B + 1/10*diag(nrow = nrow(B))

#Transform into correlation matrix W
W <- diag(B)^(-1/2)*B*diag(B)^(-1/2)

C_hat <- rep(list(elm), 180)

for (t in 1:180) {
  C_hat[[t]] <- W %*% Ct[[t]]
}

#Final proper C

C <- rep(list(elm), 180)

for (t in 1:179) {
  for (j in 1:P_c) {
    rho <- runif(1, 1/2, 1)
    for (i in 1:N) {
      Ct[[t+1]][i, j] <- 2/(N+1)*(C_hat[[t]][i, j]) -1
    }
  }  
}
```

```{r}
#xt
A1 <- matrix(c(
  0.95, 0, 0,
  0, 0.95, 0,
  0, 0, 0.95),
  nrow = 3, ncol = 3
)

A2 <- matrix(c(
  1, 0, 0.25,
  0, 0.95, 0,
  0.25, 0, 0.95),
  nrow = 3, ncol = 3
)

A3 <- matrix(c(
  0.99, 0.2, 0.1,
  0.2, 0.90, -0.3,
  0.1, -0.3, -0.99),
  nrow = 3, ncol = 3
)

xt <- data.frame(x1 = c(0:179),
                 x2 = c(0:179),
                 x3 = c(0:179)
)

for (i in 1:179) {
  ut <- rnorm(3, mean = 0, sd = 1)
  xt[1+i,] <- A1*xt[i,] + ut
}

xt_tidy <- cbind(t = c(1:180), xt)

xt_tidy <- xt_tidy %>%
  gather(series, value, -t)
  

ggplot(data = xt_tidy) +
  geom_line(aes(x = t, y = value, colour = series))
```


```{r}
#Different g() functions

#g1
g1 <- function(C, x, i, t){
  theta <- matrix(c(0.02, 0.02, 0.02), ncol = 1)
  crossprod(C[[t]][i,1:3], x[t,]) %*% theta
}
#g2

#g3

#g4

g1(C, xt, 1, 1)

```

```{r}
#OLS

```

```{r}
#Elasticnet


```

```{r}
#Random Forest

```

```{r}
#Neural Network

```

