}
ave_forecast_resids_vector
}
# Mostly the same as the simulation ones, but slightly different due to different variable importance schemes
##############################################################################################################
LM_fit <- function(pooled_panel, timeSlices, loss_function, f,
macro_factor_names, individual_factor_names) {
#Initialize Loss Function Statistics
LM_stats <- rep(list(0), 3)
for (set in 1:3) {
LM_stats[[set]] <- list(loss_stats = data.frame(train_MAE = 0, train_MSE = 0, train_RMSE = 0, train_RSquare = 0,
validation_MAE = 0, validation_MSE = 0, validation_RMSE = 0, validation_RSquare = 0,
test_MAE = 0, test_MSE = 0, test_RMSE = 0, test_RSquare = 0),
#Other useful things
forecast_resids = 0,
model = 0,
#Variable Importance
variable_importance = 0)
#Load Training, validation and test sets
train <- pooled_panel %>%
filter(time %in% timeSlices[[set]]$train)
validation <- pooled_panel %>%
filter(time %in% timeSlices[[set]]$validation)
test <- pooled_panel %>%
filter(time %in% timeSlices[[set]]$test)
#Train Model on training set
# Set model = FALSE so that it won't save a copy of the training data in the model object
# Doen for memory efficiency
#MSE case
if (loss_function == "mse") {
lm <- lm(f, data = train, model = FALSE, x = FALSE, y = FALSE)
} else {
# Use pfn as method here for much faster computation
lm <- rq(f, data = train, tau = 0.5, method = "pfn")
}
#LM_stats[[set]]$model <- lm
#No Tuning Needed
#Statistics
#Training Set
train_predict <- predict(lm, newdata = train)
LM_stats[[set]]$loss_stats$train_MAE <- mae(train$rt, train_predict)
LM_stats[[set]]$loss_stats$train_MSE <- mse(train$rt, train_predict)
LM_stats[[set]]$loss_stats$train_RMSE <- rmse(train$rt, train_predict)
LM_stats[[set]]$loss_stats$train_RSquare <- R2(train_predict, train$rt, form = "traditional")
#Validation Set Statistics
validation_predict <- predict(lm, newdata = validation)
LM_stats[[set]]$loss_stats$validation_MAE <- mae(validation$rt, validation_predict)
LM_stats[[set]]$loss_stats$validation_MSE <- mse(validation$rt, validation_predict)
LM_stats[[set]]$loss_stats$validation_RMSE <- rmse(validation$rt, validation_predict)
LM_stats[[set]]$loss_stats$validation_RSquare <- R2(validation_predict, validation$rt, form = "traditional")
#Test Set Statistics
test_predict <- predict(lm, newdata = test)
LM_stats[[set]]$loss_stats$test_MAE <- mae(test$rt, test_predict)
LM_stats[[set]]$loss_stats$test_MSE <- mse(test$rt, test_predict)
LM_stats[[set]]$loss_stats$test_RMSE <- rmse(test$rt, test_predict)
LM_stats[[set]]$loss_stats$test_RSquare <- R2(test_predict, test$rt, form = "traditional")
#Forecast Residuals
LM_stats[[set]]$forecast_resids <- lm_ave_forecast_resids(lm_model = lm, test = test)
#Variable Importance
LM_stats[[set]]$variable_importance <- LM_variable_importance(test, lm,
macro_factor_names, individual_factor_names)
}
return(LM_stats)
}
###################################################################################################################
ELN_fit_stats <- function(alpha_grid, nlamb, timeSlices, pooled_panel, loss_function,
macro_factor_names, individual_factor_names) {
ELN_stats <- rep(list(0), 3)
for (set in 1:3) {
#Load Training, validation and test sets
train <- pooled_panel %>%
filter(time %in% timeSlices[[set]]$train)
validation <- pooled_panel %>%
filter(time %in% timeSlices[[set]]$validation)
test <- pooled_panel %>%
filter(time %in% timeSlices[[set]]$test)
train_x <- as.matrix(train[4:ncol(train)])
train_y <- as.matrix(train$rt)
validation_x <- as.matrix(validation[4:ncol(validation)])
validation_y <- as.matrix(validation$rt)
test_x <- as.matrix(test[4:ncol(test)])
test_y <- as.matrix(test$rt)
#Get models fit over the grid of hyperparameters
ELN_model_grid <- ELN_model_grid(alpha_grid, train_x, train_y, validation_x, validation_y, loss_function, nlamb = nlamb)
#Get the best tuning parameters
best_model_params <- get_ELN_best_tune(ELN_model_grid)
#Initialize stats list
ELN_stats[[set]] <- list(loss_stats = data.frame(train_MAE = 0, train_MSE = 0, train_RMSE = 0, train_RSquare = 0,
validation_MAE = 0, validation_MSE = 0, validation_RMSE = 0, validation_RSquare = 0,
test_MAE = 0, test_MSE = 0, test_RMSE = 0, test_RSquare = 0),
#Other useful things
forecast_resids = 0,
model = 0,
hyperparameters = 0,
variable_importance = 0)
#Model
model <- ELN_model_grid[[best_model_params$list_index]]$model
ELN_stats[[set]]$model <- model
#Hyperparameters
ELN_stats[[set]]$hyperparameters <- best_model_params
#Loss Stats Dataframe
#Train
train_predict <- predict(model, train_x, alpha = best_model_params$alpha, lambda = best_model_params$lambda)
ELN_stats[[set]]$loss_stats$train_MAE <- mae(train_y, train_predict)
ELN_stats[[set]]$loss_stats$train_MSE <- mse(train_y, train_predict)
ELN_stats[[set]]$loss_stats$train_RMSE <- rmse(train_y, train_predict)
ELN_stats[[set]]$loss_stats$train_RSquare <- (1 - sse(train_y, train_predict) / sum((train_y - mean(train_y))^2))
#Validation
valid_predict <- predict(model, validation_x, alpha = best_model_params$alpha, lambda = best_model_params$lambda)
ELN_stats[[set]]$loss_stats$validation_MAE <- mae(validation_y, valid_predict)
ELN_stats[[set]]$loss_stats$validation_MSE <- mse(validation_y, valid_predict)
ELN_stats[[set]]$loss_stats$validation_RMSE <- rmse(validation_y, valid_predict)
ELN_stats[[set]]$loss_stats$validation_RSquare <- (1 - sse(validation_y, valid_predict) / sum((validation_y - mean(validation_y))^2))
#Test
test_predict <- predict(model, test_x, alpha = best_model_params$alpha, lambda = best_model_params$lambda)
ELN_stats[[set]]$loss_stats$test_MAE <- mae(test_y, test_predict)
ELN_stats[[set]]$loss_stats$test_MSE <- mse(test_y, test_predict)
ELN_stats[[set]]$loss_stats$test_RMSE <- rmse(test_y, test_predict)
ELN_stats[[set]]$loss_stats$test_RSquare <- (1 - sse(test_y, test_predict) / sum((test_y - mean(test_y))^2))
#Forecast residuals
ELN_stats[[set]]$forecast_resids <- eln_ave_forecast_resids(eln_model = model, test,
# Hyperparameters
alpha = best_model_params$alpha,
lambda = best_model_params$lambda)
#Variable Importance
ELN_stats[[set]]$variable_importance <- ELN_variable_importance(test, model,
alpha = best_model_params$alpha, lambda = best_model_params$lambda,
macro_factor_names, individual_factor_names)
}
return(ELN_stats)
}
################################################################################################################################
RF_fit_stats <- function(pooled_panel, RF_grid, timeSlices, loss_function, f,
macro_factor_names, individual_factor_names) {
#Initialize
RF_stats <- rep(list(0), 3)
#Load training, validation and test sets
for (set in 1:3) {
RF_stats[[set]] <- list(loss_stats = data.frame(train_MAE = 0, train_MSE = 0, train_RMSE = 0, train_RSquare = 0,
validation_MAE = 0, validation_MSE = 0, validation_RMSE = 0, validation_RSquare = 0,
test_MAE = 0, test_MSE = 0, test_RMSE = 0, test_RSquare = 0),
#Other useful things
forecast_resids = 0,
model = 0,
hyperparameters = 0,
variable_importance = 0)
train <- pooled_panel %>%
filter(time %in% timeSlices[[set]]$train)
validation <- pooled_panel %>%
filter(time %in% timeSlices[[set]]$validation)
test <- pooled_panel %>%
filter(time %in% timeSlices[[set]]$test)
#Fit on training Set over grid of hyperparameters
model_grid <- RF_fit_model_grid(f, train, validation, RF_grid, loss_function)
#Get the best hyperparameters
best_model_params <- get_RF_best_tune(model_grid)
RF_stats[[set]]$hyperparameters <- best_model_params
#Compute the optimal model
if (loss_function == "mse") {
model <- rfsrc(f, train,
#Hyperparameters
ntree = best_model_params$ntree,
mtry = best_model_params$mtry,
nodesize = best_model_params$nodesize,
splitrule = "mse",
bootstrap = "by.root", samptype = "swr"
)
} else {
model <- rfsrc(f, train,
#Hyperparameters
ntree = best_model_params$ntree,
mtry = best_model_params$mtry,
nodesize = best_model_params$nodesize,
splitrule = "quantile.regr",
prob = 0.5,
bootstrap = "by.root", samptype = "swr"
)
}
#RF_stats[[set]]$model <- model
#Train
train_predict <- predict(model, train)$predicted
RF_stats[[set]]$loss_stats$train_MAE <- mae(train$rt, train_predict)
RF_stats[[set]]$loss_stats$train_MSE <- mse(train$rt, train_predict)
RF_stats[[set]]$loss_stats$train_RMSE <- rmse(train$rt, train_predict)
RF_stats[[set]]$loss_stats$train_RSquare <- (1 - sse(train$rt, train_predict) / sum((train$rt - mean(train$rt))^2))
#Validation
valid_predict <- predict(model, newdata = validation)$predicted
RF_stats[[set]]$loss_stats$validation_MAE <- mae(validation$rt, valid_predict)
RF_stats[[set]]$loss_stats$validation_MSE <- mse(validation$rt, valid_predict)
RF_stats[[set]]$loss_stats$validation_RMSE <- rmse(validation$rt, valid_predict)
RF_stats[[set]]$loss_stats$validation_RSquare <- (1 - sse(validation$rt, valid_predict) / sum((validation$rt - mean(validation$rt))^2))
#Test
test_predict <- predict(model, newdata = test)$predicted
RF_stats[[set]]$loss_stats$test_MAE <- mae(test$rt, test_predict)
RF_stats[[set]]$loss_stats$test_MSE <- mse(test$rt, test_predict)
RF_stats[[set]]$loss_stats$test_RMSE <- rmse(test$rt, test_predict)
RF_stats[[set]]$loss_stats$test_RSquare <- (1 - sse(test$rt, test_predict) / sum((test$rt - mean(test$rt))^2))
#Forecast residuals
RF_stats[[set]]$forecast_resids <- rf_ave_forecast_resids(rf_model = model, test = test)
#Variable Importance
RF_stats[[set]]$variable_importance <- RF_variable_importance(test, model,
macro_factor_names, individual_factor_names)
}
return(RF_stats)
}
#########################################################################################################################
NNet_fit_stats <- function(pooled_panel, timeSlices, hidden_layers, loss_function, batch_size, patience,
macro_factor_names, individual_factor_names) {
#Initialize
NNet_stats <- rep(list(0), 3)
for (set in 1:3) {
NNet_stats[[set]] <- list(loss_stats = data.frame(train_MAE = 0, train_MSE = 0, train_RMSE = 0, train_RSquare = 0,
validation_MAE = 0, validation_MSE = 0, validation_RMSE = 0, validation_RSquare = 0,
test_MAE = 0, test_MSE = 0, test_RMSE = 0, test_RSquare = 0),
#Other useful things
# Keep forecasts here in for nnet objects to make sure they aren't doing something stupid
forecasts = 0,
forecast_resids = 0,
model = 0,
variable_importance = 0)
#Load Training, validation and test sets
train <- pooled_panel %>%
filter(time %in% timeSlices[[set]]$train)
validation <- pooled_panel %>%
filter(time %in% timeSlices[[set]]$validation)
test<- pooled_panel %>%
filter(time %in% timeSlices[[set]]$test)
train_x <- train[4:ncol(train)]
train_x <- scale(train_x)
col_means_train <- attr(train_x, "scaled:center")
col_stddevs_train <- attr(train_x, "scaled:scale")
train_y <- train$rt
validation_x <- validation[4:ncol(validation)]
validation_x <- scale(validation_x, center = col_means_train, scale = col_stddevs_train)
validation_y <- validation$rt
test_x <- test[4:ncol(test)]
test_x <- scale(test_x, center = col_means_train, scale = col_stddevs_train)
test_y <- test$rt
# Fit the model
# The patience parameter is the amount of epochs to check for improvement.
# Gu et al don't say what their early stopping parameter p is
early_stop <- callback_early_stopping(monitor = "val_loss", patience = patience, restore_best_weights = TRUE)
lr_reduce <- callback_reduce_lr_on_plateau(monitor = "val_loss", factor = 0.5,
patience = 5, verbose = 0, mode = c("auto", "min", "max"),
min_delta = 1e-04, cooldown = 0, min_lr = 0)
print_dot_callback <- callback_lambda(
on_epoch_end = function(epoch, logs) {
if (epoch %% 50 == 0) cat("\n")
cat(".")
}
)
l1_penalty <- 0.05
build_NN <- function(hidden_layers, loss_function) {
if (hidden_layers == 1) {
model <- keras_model_sequential() %>%
# Layer 1
layer_dense(units = 32, input_shape = ncol(train_x),
bias_initializer = initializer_zeros(),
kernel_initializer = initializer_truncated_normal(mean = 0, stddev = 0.05, seed = NULL)) %>%
layer_activation(activation = "tanh") %>%
layer_activity_regularization(l1 = l1_penalty) %>%
layer_batch_normalization() %>%
# Output Layer
layer_dense(units = 1, activation = "linear")
} else if (hidden_layers == 2) {
model <- keras_model_sequential() %>%
# Layer 1
layer_dense(units = 32, input_shape = ncol(train_x),
bias_initializer = initializer_zeros(),
kernel_initializer = initializer_truncated_normal(mean = 0, stddev = 0.05, seed = NULL)) %>%
layer_activation(activation = "tanh") %>%
layer_activity_regularization(l1 = l1_penalty) %>%
layer_batch_normalization() %>%
# Layer 2
layer_dense(units = 16,
bias_initializer = initializer_zeros(),
kernel_initializer = initializer_truncated_normal(mean = 0, stddev = 0.05, seed = NULL)) %>%
layer_activation(activation = "tanh") %>%
layer_activity_regularization(l1 = l1_penalty) %>%
layer_batch_normalization() %>%
# Output Layer
layer_dense(units = 1, activation = "linear")
} else if (hidden_layers == 3) {
model <- keras_model_sequential() %>%
# Layer 1
layer_dense(units = 32, input_shape = ncol(train_x),
bias_initializer = initializer_zeros(),
kernel_initializer = initializer_truncated_normal(mean = 0, stddev = 0.05, seed = NULL)) %>%
layer_activation(activation = "tanh") %>%
layer_activity_regularization(l1 = l1_penalty) %>%
layer_batch_normalization() %>%
# Layer 2
layer_dense(units = 16,
bias_initializer = initializer_zeros(),
kernel_initializer = initializer_truncated_normal(mean = 0, stddev = 0.05, seed = NULL)) %>%
layer_activation(activation = "tanh") %>%
layer_activity_regularization(l1 = l1_penalty) %>%
layer_batch_normalization() %>%
#Layer 3
layer_dense(units = 8,
bias_initializer = initializer_zeros(),
kernel_initializer = initializer_truncated_normal(mean = 0, stddev = 0.05, seed = NULL)) %>%
layer_activation(activation = "tanh") %>%
layer_activity_regularization(l1 = l1_penalty) %>%
layer_batch_normalization() %>%
# Output Layer
layer_dense(units = 1, activation = "linear")
} else if (hidden_layers == 4) {
model <- keras_model_sequential() %>%
# Layer 1
layer_dense(units = 32, input_shape = ncol(train_x),
bias_initializer = initializer_zeros(),
kernel_initializer = initializer_truncated_normal(mean = 0, stddev = 0.05, seed = NULL)) %>%
layer_activation(activation = "tanh") %>%
layer_activity_regularization(l1 = l1_penalty) %>%
layer_batch_normalization() %>%
# Layer 2
layer_dense(units = 16,
bias_initializer = initializer_zeros(),
kernel_initializer = initializer_truncated_normal(mean = 0, stddev = 0.05, seed = NULL)) %>%
layer_activation(activation = "tanh") %>%
layer_activity_regularization(l1 = l1_penalty) %>%
layer_batch_normalization() %>%
#Layer 3
layer_dense(units = 8,
bias_initializer = initializer_zeros(),
kernel_initializer = initializer_truncated_normal(mean = 0, stddev = 0.05, seed = NULL)) %>%
layer_activation(activation = "tanh") %>%
layer_activity_regularization(l1 = l1_penalty) %>%
layer_batch_normalization() %>%
#Layer 4
layer_dense(units = 4,
bias_initializer = initializer_zeros(),
kernel_initializer = initializer_truncated_normal(mean = 0, stddev = 0.05, seed = NULL)) %>%
layer_activation(activation = "tanh") %>%
layer_activity_regularization(l1 = l1_penalty) %>%
layer_batch_normalization() %>%
# Output Layer
layer_dense(units = 1, activation = "linear")
} else {
model <- keras_model_sequential() %>%
# Layer 1
layer_dense(units = 32, input_shape = ncol(train_x),
bias_initializer = initializer_zeros(),
kernel_initializer = initializer_truncated_normal(mean = 0, stddev = 0.05, seed = NULL)) %>%
layer_activation(activation = "tanh") %>%
layer_activity_regularization(l1 = l1_penalty) %>%
layer_batch_normalization() %>%
# Layer 2
layer_dense(units = 16,
bias_initializer = initializer_zeros(),
kernel_initializer = initializer_truncated_normal(mean = 0, stddev = 0.05, seed = NULL)) %>%
layer_activation(activation = "tanh") %>%
layer_activity_regularization(l1 = l1_penalty) %>%
layer_batch_normalization() %>%
#Layer 3
layer_dense(units = 8,
bias_initializer = initializer_zeros(),
kernel_initializer = initializer_truncated_normal(mean = 0, stddev = 0.05, seed = NULL)) %>%
layer_activation(activation = "tanh") %>%
layer_activity_regularization(l1 = l1_penalty) %>%
layer_batch_normalization() %>%
#Layer 4
layer_dense(units = 4,
bias_initializer = initializer_zeros(),
kernel_initializer = initializer_truncated_normal(mean = 0, stddev = 0.05, seed = NULL)) %>%
layer_activation(activation = "tanh") %>%
layer_activity_regularization(l1 = l1_penalty) %>%
layer_batch_normalization() %>%
#Layer 5
layer_dense(units = 2,
bias_initializer = initializer_zeros(),
kernel_initializer = initializer_truncated_normal(mean = 0, stddev = 0.05, seed = NULL)) %>%
layer_activation(activation = "tanh") %>%
layer_activity_regularization(l1 = l1_penalty) %>%
layer_batch_normalization() %>%
# Output Layer
layer_dense(units = 1, activation = "linear")
}
model %>% compile(
loss = loss_function,
optimizer = optimizer_adam(lr = 0.001, beta_1 = 0.9, beta_2 = 0.999,
epsilon = NULL, decay = 0, amsgrad = FALSE, clipnorm = NULL,
clipvalue = NULL),
metrics = list("mae", "mse")
)
model
}
neural_network <- build_NN(hidden_layers, loss_function)
# Other options used throughout the neural network fitting process are specified here
# Namely, batch size
# In general, larger batch sizes result in faster progress in training, but don't always converge as fast. Smaller batch sizes train slower, but can converge faster.
# Default batch size is 32
neural_network %>% fit(as.matrix(train_x), as.matrix(train_y),
batch_size = batch_size, epochs = 500, verbose = 1,
validation_data = list(as.matrix(validation_x), as.matrix(validation_y)),
callbacks = list(early_stop, print_dot_callback))
# Model
# NNet_stats[[set]]$model <- neural_network
#Train
train_predict <- neural_network %>% predict(as.matrix(train_x))
NNet_stats[[set]]$loss_stats$train_MAE <- mae(train$rt, train_predict)
NNet_stats[[set]]$loss_stats$train_MSE <- mse(train$rt, train_predict)
NNet_stats[[set]]$loss_stats$train_RMSE <- rmse(train$rt, train_predict)
NNet_stats[[set]]$loss_stats$train_RSquare <- R2(train_predict, train$rt, form = "traditional")
#Validation
validation_predict <- neural_network %>% predict(as.matrix(validation_x))
NNet_stats[[set]]$loss_stats$validation_MAE <- mae(validation$rt, validation_predict)
NNet_stats[[set]]$loss_stats$validation_MSE <- mse(validation$rt, validation_predict)
NNet_stats[[set]]$loss_stats$validation_RMSE <- rmse(validation$rt, validation_predict)
NNet_stats[[set]]$loss_stats$validation_RSquare <- R2(validation_predict, validation$rt, form = "traditional")
#Test
test_predict <- neural_network %>% predict(as.matrix(test_x))
NNet_stats[[set]]$loss_stats$test_MAE <- mae(test$rt, test_predict)
NNet_stats[[set]]$loss_stats$test_MSE <- mse(test$rt, test_predict)
NNet_stats[[set]]$loss_stats$test_RMSE <- rmse(test$rt, test_predict)
NNet_stats[[set]]$loss_stats$test_RSquare <- R2(test_predict, test$rt, form = "traditional")
#Forecasts
NNet_stats[[set]]$forecasts <- test_predict
#Forecast residuals
NNet_stats[[set]]$forecast_resids <- nnet_ave_forecast_resids(nnet_model = neural_network, test = test)
#Variable Importance
NNet_stats[[set]]$variable_importance <- NNet_variable_importance(test, neural_network,
macro_factor_names, individual_factor_names)
}
# clear Keras session
k_clear_session()
NNet_stats
}
###################################################################################################################
################################################################
## Function to fit all models for real data
## Slightly different from simulation model fitting function
################################################################
fit_all_models_real_data <- function(pooled_panel, macro_factor_names, individual_factor_names,
# Logical arguments specifying which models you want to fit
# This is useful if you don't want to fit some of the most intensive methods such as RF and Neural Networks
LM, ELN, RF, NNet) {
# Initialize List
real_data_results_list <- list(0)
real_data_results_list <- list(
# Panel Statistics
pooled_panel_stats = 0,
# Y Values (used for Diebold Mariano Tests later)
returns = 0,
# Models
LM_MSE = 0, LM_MAE = 0,
ELN_MSE = 0, ELN_MAE = 0,
RF_MSE = 0, RF_MAE = 0,
# Neural Networks
NN1_MSE = 0, NN1_MAE = 0,
NN2_MSE = 0, NN2_MAE = 0,
NN3_MSE = 0, NN3_MAE = 0,
NN4_MSE = 0, NN4_MAE = 0,
NN5_MSE = 0, NN5_MAE = 0
)
realdata_timeSlices <- realdata_custom_timeslices(start = 1993, initialWindow = 15, horizon = 1, validation_size = 3, test_size = 3, set_no = 3)
f <- real_panel_formula(pooled_panel)
if (LM == 1) {
# Linear Models
real_data_results_list$LM_MSE <- LM_fit(pooled_panel, realdata_timeSlices, "mse", f)
real_data_results_list$LM_MAE <- LM_fit(pooled_panel, realdata_timeSlices, "mae", f)
}
if (ELN == 1) {
# Penalized Linear Models
alpha_grid <- seq(0, 1, 0.01)
real_data_results_list$ELN_MAE <- ELN_fit_stats(alpha_grid, nlamb = 100, realdata_timeSlices, pooled_panel, loss_function = "mae")
real_data_results_list$ELN_MSE <- ELN_fit_stats(alpha_grid, nlamb = 100, realdata_timeSlices, pooled_panel, loss_function = "mse")
}
if (RF == 1) {
# Random Forests
RF_grid <- expand.grid(
#ntree usually isn't tuned. Just set to max of computationally feasible
ntree = 100,
mtry = seq(10, round(ncol(pooled_panel[4:ncol(pooled_panel)])/3), 20)
# nodesize = seq(2, 14, 2)
# nodedepth recommended not to be changed
#nodedepth = 1
)
real_data_results_list$RF_MSE <- RF_fit_stats(pooled_panel, RF_grid, realdata_timeSlices, "mse")
real_data_results_list$RF_MAE <- RF_fit_stats(pooled_panel, RF_grid, realdata_timeSlices, "mae")
}
if (NNet == 1) {
# Neural Networks
# Commented for now because honours lab computers don't have keras/tensorflow
batch_size <- 32
patience <- 20
real_data_results_list$NN1_MSE <- NNet_fit_stats(pooled_panel, realdata_timeSlices, 1, "mse", batch_size = batch_size, patience = patience)
real_data_results_list$NN1_MAE <- NNet_fit_stats(pooled_panel, realdata_timeSlices, 1, "mae", batch_size = batch_size, patience = patience)
real_data_results_list$NN2_MSE <- NNet_fit_stats(pooled_panel, realdata_timeSlices, 2, "mse", batch_size = batch_size, patience = patience)
real_data_results_list$NN2_MAE <- NNet_fit_stats(pooled_panel, realdata_timeSlices, 2, "mae", batch_size = batch_size, patience = patience)
real_data_results_list$NN3_MSE <- NNet_fit_stats(pooled_panel, realdata_timeSlices, 3, "mse", batch_size = batch_size, patience = patience)
real_data_results_list$NN3_MAE <- NNet_fit_stats(pooled_panel, realdata_timeSlices, 3, "mae", batch_size = batch_size, patience = patience)
real_data_results_list$NN4_MSE <- NNet_fit_stats(pooled_panel, realdata_timeSlices, 4, "mse", batch_size = batch_size, patience = patience)
real_data_results_list$NN4_MAE <- NNet_fit_stats(pooled_panel, realdata_timeSlices, 4, "mae", batch_size = batch_size, patience = patience)
real_data_results_list$NN5_MSE <- NNet_fit_stats(pooled_panel, realdata_timeSlices, 5, "mse", batch_size = batch_size, patience = patience)
real_data_results_list$NN5_MAE <- NNet_fit_stats(pooled_panel, realdata_timeSlices, 5, "mae", batch_size = batch_size, patience = patience)
}
real_data_results_list
}
## Load in results
LM_stats_mse <- readRDS("LM_stats_mse.rds")
LM_stats_mae <- readRDS("LM_stats_mae.rds")
ELN_stats_mse <- readRDS("ELN_stats_mse.rds")
ELN_stats_mae <- readRDS("ELN_stats_mae.rds")
RF_mse_stats <- readRDS("RF_mse_stats_1.5_train_valid.rds")
RF_mae_stats <- readRDS("RF_mae_stats_1.5_train_valid.rds")
################################################################
NNet_1_mse_stats <- readRDS("NNet_1_mse_stats_1.5.rds")
NNet_1_mae_stats <- readRDS("NNet_1_mae_stats_1.5.rds")
NNet_2_mse_stats <- readRDS("NNet_2_mse_stats_1.5.rds")
save.image("~/GitHub/Evaluation-of-Machine-Learning-in-Asset-Pricing/R/.RData")
