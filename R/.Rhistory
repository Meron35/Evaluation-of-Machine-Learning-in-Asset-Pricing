#Test
test_predict <- lstm_model %>% predict(X_array_test) %>%
as.vector()
LSTM_stats[[set]]$loss_stats$test_MAE <- mae(Y_array_test %>% as.vector(), test_predict)
LSTM_stats[[set]]$loss_stats$test_MSE <- mse(Y_array_test %>% as.vector(), test_predict)
LSTM_stats[[set]]$loss_stats$test_RMSE <- rmse(Y_array_test %>% as.vector(), test_predict)
LSTM_stats[[set]]$loss_stats$test_RSquare <- R2(test_predict, Y_array_test %>% as.vector(), form = "traditional")
#Forecasts
LSTM_stats[[set]]$forecasts <- test_predict
#Variable Importance
LSTM_stats[[set]]$variable_importance <- LSTM_variable_importance(X_array_test, Y_array_test, test_x, lstm_model)
#Clear Kera Session
k_clear_session()
}
LSTM_stats
}
knitr::opts_chunk$set(echo = TRUE)
################
##Load Libraries
################
library(tidyverse)
library(keras)
library(quantreg)
library(ggplot2)
library(forecast)
library(rlist)
library(Metrics)
library(ranger)
library(caret)
library(forcats)
library(xtable)
library(randomForestSRC)
library(xts)
library(uwot)
library(rugarch)
#Parallel Computing
library(foreach)
library(doFuture)
#Registering
registerDoFuture()
plan(multisession)
set.seed(27935248)
batch_process_range <- c(1:5)
g1_A1_nosv_0_LSTM_results_15 <- fit_all_models_lstm(g1_A1_nosv_0, batch_process_range)
saveRDS(g1_A1_nosv_0_LSTM_results_15, "~/GitHub/Evaluation-of-Machine-Learning-in-Asset-Pricing/R/Model_results_15/g1_A1_nosv_0_LSTM_results_15.rds")
View(g1_A1_nosv_0_LSTM_results_15)
saveRDS(g1_A1_nosv_0_LSTM_results_15, "~/GitHub/Evaluation-of-Machine-Learning-in-Asset-Pricing/R/Model_results_15/g1_A1_nosv_0_LSTM_results_15.rds")
saveRDS(g1_A1_nosv_0_LSTM_results_15, "~/GitHub/Evaluation-of-Machine-Learning-in-Asset-Pricing/R/Model_results/g1_A1_nosv_0_LSTM_results_15.rds")
g1_A1_nosv_0_LSTM_results_15[[1]]$LSTM_MSE[[1]]$loss_stats
g1_A1_nosv_0_LSTM_results_15[[1]]$LSTM_MSE[[2]]$loss_stats
g1_A1_nosv_0_LSTM_results_15[[1]]$LSTM_MSE[[3]]$loss_stats
g2_A1_nosv_0_LSTM_results_15 <- fit_all_models_lstm(g2_A1_nosv_0, batch_process_range)
k_clear_session()
################
##Load Libraries
################
library(speedglm)
library(tidyverse)
library(keras)
library(ggplot2)
library(forecast)
library(rlist)
library(Metrics)
library(tensorflow)
library(quantreg)
library(randomForestSRC)
library(hqreg)
library(caret)
library(tsfeatures)
## Vector arima/garch packages
library(xts)
library(uwot)
library(rugarch)
## SPecial xgboost instructions
# CLone my forked version of xgboost with multiclass fix
# Set working directoty the r pakcage folder of that repo
# setwd('C:/Users/Zeyu Zhong/Documents/GitHub/xgboost/R-package')
# install.packages('.', repos = NULL, type="source")
# Other alternative
library(xgboost)
#Parallel Computing
library(foreach)
library(doFuture)
#Registering
registerDoFuture()
plan(multisession)
LSTM_fit_stats <- function(pooled_panel, timeSlices, loss_function, batch_size, patience) {
## Initialize
LSTM_stats <- rep(list(0), 3)
## Loop over different timeslices
for (set in 1:3) {
## Important things
LSTM_stats[[set]] <- list(loss_stats = data.frame(train_MAE = 0, train_MSE = 0, train_RMSE = 0, train_RSquare = 0,
validation_MAE = 0, validation_MSE = 0, validation_RMSE = 0, validation_RSquare = 0,
test_MAE = 0, test_MSE = 0, test_RMSE = 0, test_RSquare = 0),
#Other useful things
# Keep forecasts here in for nnet objects to make sure they aren't doing something stupid
forecasts = 0,
forecast_resids = 0,
model = 0,
variable_importance = 0)
## Train, validation, test
## Scale Data first
train <- pooled_panel %>%
dplyr::filter(time %in% timeSlices[[set]]$train)
validation <- pooled_panel %>%
dplyr::filter(time %in% timeSlices[[set]]$validation)
test<- pooled_panel %>%
dplyr::filter(time %in% timeSlices[[set]]$test)
train_x <- train %>%
dplyr::select(-rt, -stock, -time)
train_x <- scale(train_x)
col_means_train <- attr(train_x, "scaled:center")
col_stddevs_train <- attr(train_x, "scaled:scale")
train_y <- train$rt
validation_x <- validation %>%
dplyr::select(-rt, -stock, -time)
validation_x <- scale(validation_x, center = col_means_train, scale = col_stddevs_train)
validation_y <- validation$rt
test_x <- test %>%
dplyr::select(-rt, -stock, -time)
test_x <- scale(test_x, center = col_means_train, scale = col_stddevs_train)
test_y <- test$rt
pooled_panel_trunc <- rbind(
cbind(rt = train_y, time = train$time, train_x),
cbind(rt = validation_y, time = validation$time, validation_x),
cbind(rt = test_y, time = test$time, test_x)
) %>%
data.frame
## Change this to array format for use with LSTMs
length <- max(timeSlices[[set]]$test) - 1
X_array <- array(data = 0, dim = c(length, 1, 80000))
Y_array <- array(data = 0, dim = c(length, 200))
for(t in 1:length) {
X_array[t, 1, ] <- pooled_panel_trunc %>%
dplyr::filter(time == t+1) %>%
dplyr::select(-rt, -time) %>%
as.matrix() %>%
as.vector()
}
for(t in 1:length) {
Y_array[t, ] <- pooled_panel_trunc %>%
dplyr::filter(time == t+1) %>%
dplyr::select(rt) %>%
as.matrix() %>%
as.vector()
}
X_array_train <- X_array[(timeSlices[[set]]$train - 1), , , drop = F]
X_array_validation <- X_array[(timeSlices[[set]]$validation - 1), , , drop = F]
X_array_test <- X_array[(timeSlices[[set]]$test - 1), , , drop = F]
Y_array_train <- Y_array[(timeSlices[[set]]$train - 1), ]
Y_array_validation <- Y_array[(timeSlices[[set]]$validation - 1), ]
Y_array_test <- Y_array[(timeSlices[[set]]$test - 1), ]
## Early stopping Callback
early_stop <- callback_early_stopping(monitor = "val_loss", patience = patience, restore_best_weights = TRUE)
print_dot_callback <- callback_lambda(
on_epoch_end = function(epoch, logs) {
if (epoch %% 50 == 0) cat("\n")
cat(".")
}
)
## Fit the LSTM Model
l1_penalty <- 0.001
lstm_model <- keras_model_sequential() %>%
layer_lstm(units = 4, input_shape = c(1, dim(X_array_train)[3]),
return_sequences = TRUE) %>%
layer_activation("tanh") %>%
layer_activity_regularization(l1 = l1_penalty) %>%
layer_batch_normalization() %>%
layer_dense(units = 200) %>%
layer_activation("linear") %>%
layer_flatten()
lstm_model %>% compile(loss = "mse",
optimizer = optimizer_adam(lr = 0.001, beta_1 = 0.9, beta_2 = 0.999),
metrics = list("mae", "mse"))
lstm_model_fit <- lstm_model %>%
fit(X_array_train, Y_array_train, epochs = 500,
validation_data = list(X_array_validation, Y_array_validation),
batch_size = 32, shuffle = FALSE, verbose = 0,
callbacks = list(early_stop, print_dot_callback))
## Save stuff to list object
#Train
train_predict <- lstm_model %>% predict(X_array_train) %>%
as.vector()
LSTM_stats[[set]]$loss_stats$train_MAE <- mae(Y_array_train %>% as.vector(), train_predict)
LSTM_stats[[set]]$loss_stats$train_MSE <- mse(Y_array_train %>% as.vector(), train_predict)
LSTM_stats[[set]]$loss_stats$train_RMSE <- rmse(Y_array_train %>% as.vector(), train_predict)
LSTM_stats[[set]]$loss_stats$train_RSquare <- R2(train_predict, Y_array_train %>% as.vector(), form = "traditional")
#Validation
validation_predict <- lstm_model %>% predict(X_array_validation) %>%
as.vector()
LSTM_stats[[set]]$loss_stats$validation_MAE <- mae(Y_array_validation %>% as.vector(), validation_predict)
LSTM_stats[[set]]$loss_stats$validation_MSE <- mse(Y_array_validation %>% as.vector(), validation_predict)
LSTM_stats[[set]]$loss_stats$validation_RMSE <- rmse(Y_array_validation %>% as.vector(), validation_predict)
LSTM_stats[[set]]$loss_stats$validation_RSquare <- R2(validation_predict, Y_array_validation %>% as.vector(), form = "traditional")
#Test
test_predict <- lstm_model %>% predict(X_array_test) %>%
as.vector()
LSTM_stats[[set]]$loss_stats$test_MAE <- mae(Y_array_test %>% as.vector(), test_predict)
LSTM_stats[[set]]$loss_stats$test_MSE <- mse(Y_array_test %>% as.vector(), test_predict)
LSTM_stats[[set]]$loss_stats$test_RMSE <- rmse(Y_array_test %>% as.vector(), test_predict)
LSTM_stats[[set]]$loss_stats$test_RSquare <- R2(test_predict, Y_array_test %>% as.vector(), form = "traditional")
#Forecasts
LSTM_stats[[set]]$forecasts <- test_predict
#Variable Importance
LSTM_stats[[set]]$variable_importance <- LSTM_variable_importance(X_array_test, Y_array_test, test_x, lstm_model)
#Clear Kera Session
k_clear_session()
}
LSTM_stats
}
knitr::opts_chunk$set(echo = TRUE)
################
##Load Libraries
################
library(tidyverse)
library(keras)
library(quantreg)
library(ggplot2)
library(forecast)
library(rlist)
library(Metrics)
library(ranger)
library(caret)
library(forcats)
library(xtable)
library(randomForestSRC)
library(xts)
library(uwot)
library(rugarch)
#Parallel Computing
library(foreach)
library(doFuture)
#Registering
registerDoFuture()
plan(multisession)
set.seed(27935248)
g2_A1_nosv_0_LSTM_results_15 <- fit_all_models_lstm(g2_A1_nosv_0, batch_process_range)
knitr::opts_chunk$set(echo = TRUE)
################
##Load Libraries
################
library(tidyverse)
library(keras)
library(quantreg)
library(ggplot2)
library(forecast)
library(rlist)
library(Metrics)
library(ranger)
library(caret)
library(forcats)
library(xtable)
library(randomForestSRC)
library(xts)
library(uwot)
library(rugarch)
#Parallel Computing
library(foreach)
library(doFuture)
#Registering
registerDoFuture()
plan(multisession)
set.seed(27935248)
g1_A1_sv_0.01 <- readRDS("~/GitHub/Evaluation-of-Machine-Learning-in-Asset-Pricing/R/Simulated_dataset/g1_A1_sv_0.01.RDS")
g2_A1_sv_0.01 <- readRDS("~/GitHub/Evaluation-of-Machine-Learning-in-Asset-Pricing/R/Simulated_dataset/g2_A1_sv_0.01.RDS")
g3_A1_sv_0.01 <- readRDS("~/GitHub/Evaluation-of-Machine-Learning-in-Asset-Pricing/R/Simulated_dataset/g3_A1_sv_0.01.RDS")
################
##Load Libraries
################
library(speedglm)
library(tidyverse)
library(keras)
library(ggplot2)
library(forecast)
library(rlist)
library(Metrics)
library(tensorflow)
library(quantreg)
library(randomForestSRC)
library(hqreg)
library(caret)
library(tsfeatures)
## Vector arima/garch packages
library(xts)
library(uwot)
library(rugarch)
## SPecial xgboost instructions
# CLone my forked version of xgboost with multiclass fix
# Set working directoty the r pakcage folder of that repo
# setwd('C:/Users/Zeyu Zhong/Documents/GitHub/xgboost/R-package')
# install.packages('.', repos = NULL, type="source")
# Other alternative
library(xgboost)
#Parallel Computing
library(foreach)
library(doFuture)
#Registering
registerDoFuture()
plan(multisession)
#Create Training + Test Sets
# Gu et al set a training, validation and test sample equal in length for their simulations. Not the most sensible idea
#$T = 180$ monthly periods corresponds to 15 years. The training sample was set to start from $T = 108$ or 9 years, a validation set 1 year in length. The last 3 years were reserved as a test set never to be used for validation or training.
#This means that there are only 3 sample periods to train on, yay
#Caret and other packages are not so good at dealing with custom training/validation/test sets. Have to specify resampling, tuning and testing manually (ugh)
# Change this to 9 + 3 + 3, maybe more stable procedure
customTimeSlices <- function(start, initialWindow, horizon, validation_size, test_size, set_no) {
time_slice <- list(train = 0, validation = 0, test = 0)
time_slices <- rep(list(time_slice), set_no)
for (t in 1:set_no) {
time_slice$train <- c(start:(initialWindow + (t-1) * horizon + 1))
time_slice$validation <- c((initialWindow + (t-1) * horizon + 2):((initialWindow + (t-1) * horizon) + validation_size + 1))
time_slice$test <- c((initialWindow + (t-1) * horizon) + validation_size + 2):((initialWindow + (t-1) * horizon) + validation_size + test_size + 1)
time_slices[[t]] <- time_slice
}
time_slices
}
#Create custom time slices
timeSlices <- customTimeSlices(start = 2, initialWindow = 108, horizon = 12, validation_size = 36, test_size = 12, set_no = 3)
#Formula Function, makes it easier for those packages with a formula interface
panel_formula <- function(panel){
#Remove the first 3 colNames, as these correspond to the return, time and stock id
panel_colnames <- colnames(panel)[-c(1:3)]
f <- as.formula(c("rt ~", paste(panel_colnames, collapse = "+")))
return(f)
}
## LSTM Fit all models
fit_all_models_lstm <- function(dataset_list, batch_process_range) {
# Initialize List
simulation_results_list <- rep(list(0), length(batch_process_range))
for (batch in (batch_process_range)) {
simulation_results_list[[batch]] <- list(
# Panel Statistics
Dataset_stats = 0,
# Models
LSTM_MSE = 0, LSTM_MAE = 0
)
# Load Dataset
pooled_panel <- dataset_list[[batch]]$panel
simulation_results_list[[batch]]$Dataset_stats <- dataset_list[[batch]]$statistics
simulation_results_list[[batch]]$returns <- pooled_panel$rt
timeSlices <- customTimeSlices(start = 2, initialWindow = 84, horizon = 12, validation_size = 60, test_size = 12, set_no = 3)
simulation_results_list[[batch]]$LSTM_MSE <- LSTM_fit_stats(pooled_panel, timeSlices, "mse", batch_size = 64, patience = 20)
simulation_results_list[[batch]]$LSTM_MAE <- LSTM_fit_stats(pooled_panel, timeSlices, "mae", batch_size = 64, patience = 20)
}
simulation_results_list
}
g1_A1_sv_0.01_LSTM_results_15 <- fit_all_models_lstm(g1_A1_sv_0.01, batch_process_range)
batch_process_range <- c(1:5)
g1_A1_sv_0.01_LSTM_results_15 <- fit_all_models_lstm(g1_A1_sv_0.01, batch_process_range)
## RNN and LSTM Neural Network Models
# Prepare the data into a format which RNNs and LSTM models from keras recognise and can work with
# We want a multipl einput (factor set), multiple output (multiple stocks) structure
# Annoyingly, this means we have to convert our dataset from tidy to array format
# Keras needs the X (input) to be an array of (sample no, time steps, no. of features) dimensions
# In our case, this corresponds to (length of training set, 1 as we are using 1 lag, 400 factors)
# Keras needs the Y output to be an array of (sample no, number of outputs per sample)
# In our case, this corresponds to (length of training set, no. of stocks/cross sectional units)
# Observations when fitting:
# LSTM parameter count is very explosive, usually having an LSTM unit with 8 units approaches GPU memory limit of 2GB already
# Stacking LSTM units doesn't seem to help too much
# Adding more vanilla layers also doesn't seem to help much
# Predictions seem to be well behaved compared to MLPs
# tanh seems to be better than ReLU, though as least ReLU works here
# Fitting wrt to MAE seems to be better
# Batch size seems to be best at around 32 or 64
## Variable Importance function for LSTM
LSTM_variable_importance <- function(X_array_test, Y_array_test, test_x, lstm_model) {
variable_importance_df <- foreach(i = (1:400), .combine = "rbind", .packages = c("keras", "tensorflow", "reticulate")) %do% {
index <- seq(1, 200, by = 1) + (i - 1)*200
X_array_test_zero <- X_array_test
for (t in 1:dim(X_array_test)[1]) {
X_array_test_zero[t, 1, index] <- 0
}
original_R2 <- R2(predict(lstm_model, X_array_test), Y_array_test %>% as.vector(), form = "traditional")
new_R2 <- R2(predict(lstm_model, X_array_test_zero), Y_array_test %>% as.vector(), form = "traditional")
variable_importance <- data.frame(variable = colnames(test_x)[i], importance = (original_R2 - new_R2))
variable_importance
}
variable_importance_df
}
## Function to fit LSTM Model
LSTM_fit_stats <- function(pooled_panel, timeSlices, loss_function, batch_size, patience) {
## Initialize
LSTM_stats <- rep(list(0), 3)
## Loop over different timeslices
for (set in 1:3) {
## Important things
LSTM_stats[[set]] <- list(loss_stats = data.frame(train_MAE = 0, train_MSE = 0, train_RMSE = 0, train_RSquare = 0,
validation_MAE = 0, validation_MSE = 0, validation_RMSE = 0, validation_RSquare = 0,
test_MAE = 0, test_MSE = 0, test_RMSE = 0, test_RSquare = 0),
#Other useful things
# Keep forecasts here in for nnet objects to make sure they aren't doing something stupid
forecasts = 0,
forecast_resids = 0,
model = 0,
variable_importance = 0)
## Train, validation, test
## Scale Data first
train <- pooled_panel %>%
dplyr::filter(time %in% timeSlices[[set]]$train)
validation <- pooled_panel %>%
dplyr::filter(time %in% timeSlices[[set]]$validation)
test<- pooled_panel %>%
dplyr::filter(time %in% timeSlices[[set]]$test)
train_x <- train %>%
dplyr::select(-rt, -stock, -time)
train_x <- scale(train_x)
col_means_train <- attr(train_x, "scaled:center")
col_stddevs_train <- attr(train_x, "scaled:scale")
train_y <- train$rt
validation_x <- validation %>%
dplyr::select(-rt, -stock, -time)
validation_x <- scale(validation_x, center = col_means_train, scale = col_stddevs_train)
validation_y <- validation$rt
test_x <- test %>%
dplyr::select(-rt, -stock, -time)
test_x <- scale(test_x, center = col_means_train, scale = col_stddevs_train)
test_y <- test$rt
pooled_panel_trunc <- rbind(
cbind(rt = train_y, time = train$time, train_x),
cbind(rt = validation_y, time = validation$time, validation_x),
cbind(rt = test_y, time = test$time, test_x)
) %>%
data.frame
## Change this to array format for use with LSTMs
length <- max(timeSlices[[set]]$test) - 1
X_array <- array(data = 0, dim = c(length, 1, 80000))
Y_array <- array(data = 0, dim = c(length, 200))
for(t in 1:length) {
X_array[t, 1, ] <- pooled_panel_trunc %>%
dplyr::filter(time == t+1) %>%
dplyr::select(-rt, -time) %>%
as.matrix() %>%
as.vector()
}
for(t in 1:length) {
Y_array[t, ] <- pooled_panel_trunc %>%
dplyr::filter(time == t+1) %>%
dplyr::select(rt) %>%
as.matrix() %>%
as.vector()
}
X_array_train <- X_array[(timeSlices[[set]]$train - 1), , , drop = F]
X_array_validation <- X_array[(timeSlices[[set]]$validation - 1), , , drop = F]
X_array_test <- X_array[(timeSlices[[set]]$test - 1), , , drop = F]
Y_array_train <- Y_array[(timeSlices[[set]]$train - 1), ]
Y_array_validation <- Y_array[(timeSlices[[set]]$validation - 1), ]
Y_array_test <- Y_array[(timeSlices[[set]]$test - 1), ]
## Early stopping Callback
early_stop <- callback_early_stopping(monitor = "val_loss", patience = patience, restore_best_weights = TRUE)
print_dot_callback <- callback_lambda(
on_epoch_end = function(epoch, logs) {
if (epoch %% 50 == 0) cat("\n")
cat(".")
}
)
## Fit the LSTM Model
l1_penalty <- 0.001
lstm_model <- keras_model_sequential() %>%
layer_lstm(units = 4, input_shape = c(1, dim(X_array_train)[3]),
return_sequences = TRUE) %>%
layer_activation("tanh") %>%
layer_activity_regularization(l1 = l1_penalty) %>%
layer_batch_normalization() %>%
layer_dense(units = 200) %>%
layer_activation("linear") %>%
layer_flatten()
lstm_model %>% compile(loss = "mse",
optimizer = optimizer_adam(lr = 0.001, beta_1 = 0.9, beta_2 = 0.999),
metrics = list("mae", "mse"))
lstm_model_fit <- lstm_model %>%
fit(X_array_train, Y_array_train, epochs = 500,
validation_data = list(X_array_validation, Y_array_validation),
batch_size = 32, shuffle = FALSE, verbose = 0,
callbacks = list(early_stop, print_dot_callback))
## Save stuff to list object
#Train
train_predict <- lstm_model %>% predict(X_array_train) %>%
as.vector()
LSTM_stats[[set]]$loss_stats$train_MAE <- mae(Y_array_train %>% as.vector(), train_predict)
LSTM_stats[[set]]$loss_stats$train_MSE <- mse(Y_array_train %>% as.vector(), train_predict)
LSTM_stats[[set]]$loss_stats$train_RMSE <- rmse(Y_array_train %>% as.vector(), train_predict)
LSTM_stats[[set]]$loss_stats$train_RSquare <- R2(train_predict, Y_array_train %>% as.vector(), form = "traditional")
#Validation
validation_predict <- lstm_model %>% predict(X_array_validation) %>%
as.vector()
LSTM_stats[[set]]$loss_stats$validation_MAE <- mae(Y_array_validation %>% as.vector(), validation_predict)
LSTM_stats[[set]]$loss_stats$validation_MSE <- mse(Y_array_validation %>% as.vector(), validation_predict)
LSTM_stats[[set]]$loss_stats$validation_RMSE <- rmse(Y_array_validation %>% as.vector(), validation_predict)
LSTM_stats[[set]]$loss_stats$validation_RSquare <- R2(validation_predict, Y_array_validation %>% as.vector(), form = "traditional")
#Test
test_predict <- lstm_model %>% predict(X_array_test) %>%
as.vector()
LSTM_stats[[set]]$loss_stats$test_MAE <- mae(Y_array_test %>% as.vector(), test_predict)
LSTM_stats[[set]]$loss_stats$test_MSE <- mse(Y_array_test %>% as.vector(), test_predict)
LSTM_stats[[set]]$loss_stats$test_RMSE <- rmse(Y_array_test %>% as.vector(), test_predict)
LSTM_stats[[set]]$loss_stats$test_RSquare <- R2(test_predict, Y_array_test %>% as.vector(), form = "traditional")
#Forecasts
LSTM_stats[[set]]$forecasts <- test_predict
#Variable Importance
LSTM_stats[[set]]$variable_importance <- LSTM_variable_importance(X_array_test, Y_array_test, test_x, lstm_model)
#Clear Kera Session
k_clear_session()
}
LSTM_stats
}
g1_A1_sv_0.01_LSTM_results_15 <- fit_all_models_lstm(g1_A1_sv_0.01, batch_process_range)
saveRDS(g1_A1_sv_0.01_LSTM_results_15, "~/GitHub/Evaluation-of-Machine-Learning-in-Asset-Pricing/R/Model_results/g1_A1_sv_0.01_LSTM_results_15.rds")
g2_A1_sv_0.01_LSTM_results_15 <- fit_all_models_lstm(g2_A1_sv_0.01, batch_process_range)
saveRDS(g2_A1_sv_0.01_LSTM_results_15, "~/GitHub/Evaluation-of-Machine-Learning-in-Asset-Pricing/R/Model_results/g2_A1_sv_0.01_LSTM_results_15.rds")
g3_A1_sv_0.01_LSTM_results_15 <- fit_all_models_lstm(g3_A1_sv_0.01, batch_process_range)
