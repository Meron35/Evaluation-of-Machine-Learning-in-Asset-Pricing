LM_stats[[set]]$forecast_resids <- test$rt - test_predict
}
return(LM_stats)
}
LM_stats_mse <- LM_fit_mse(pooled_panel, timeSlices, "mse")
#Linear model wrt MSE
LM_fit <- function(pooled_panel, timeSlices, loss_function) {
#Initialize Loss Function Statistics
LM_stats <- rep(list(0), 3)
for (set in 1:3) {
LM_stats[[set]] <- list(loss_stats = data.frame(train_MAE = 0, train_MSE = 0, train_RMSE = 0, train_RSquare = 0,
validation_MAE = 0, validation_MSE = 0, validation_RMSE = 0, validation_RSquare = 0,
test_MAE = 0, test_MSE = 0, test_RMSE = 0, test_RSquare = 0),
#Other useful things
forecasts = 0,
forecast_resids = 0,
model = 0)
#Load Training, validation and test sets
train <- pooled_panel %>%
filter(time %in% timeSlices[[set]]$train)
validation <- pooled_panel %>%
filter(time %in% timeSlices[[set]]$validation)
test <- pooled_panel %>%
filter(time %in% timeSlices[[set]]$test)
#Train Model on training set
#MSE case
if (loss_function == "mse") {
lm <- lm(f, data = train)
} else {
lm <- rq(f, data = train, tau = 0.5, method = "br")
}
LM_stats[[set]]$model <- lm
#No Tuning Needed
#Statistics
#Training Set
train_predict <- predict(lm)
LM_stats[[set]]$loss_stats$train_MAE <- mae(train$rt, train_predict)
LM_stats[[set]]$loss_stats$train_MSE <- mse(train$rt, train_predict)
LM_stats[[set]]$loss_stats$train_RMSE <- rmse(train$rt, train_predict)
LM_stats[[set]]$loss_stats$train_RSquare <- (1 - sse(train$rt, train_predict) / sum((train$rt - mean(train$rt))^2))
#Validation Set Statistics
validation_predict <- predict(lm, newdata = validation)
LM_stats[[set]]$loss_stats$validation_MAE <- mae(validation$rt, validation_predict)
LM_stats[[set]]$loss_stats$validation_MSE <- mse(validation$rt, validation_predict)
LM_stats[[set]]$loss_stats$validation_RMSE <- rmse(validation$rt, validation_predict)
LM_stats[[set]]$loss_stats$validation_RSquare <- (1 - sse(validation$rt, validation_predict) / sum((validation$rt - mean(validation$rt))^2))
#Test Set Statistics
test_predict <- predict(lm, newdata = test)
LM_stats[[set]]$loss_stats$test_MAE <- mae(test$rt, test_predict)
LM_stats[[set]]$loss_stats$test_MSE <- mse(test$rt, test_predict)
LM_stats[[set]]$loss_stats$test_RMSE <- rmse(test$rt, test_predict)
LM_stats[[set]]$loss_stats$test_RSquare <- (1 - sse(test$rt, test_predict) / sum((test$rt - mean(test$rt))^2))
#Forecasts
LM_stats[[set]]$forecasts <- test_predict
#Forecast Residuals
LM_stats[[set]]$forecast_resids <- test$rt - test_predict
}
return(LM_stats)
}
LM_stats_mse <- LM_fit(pooled_panel, timeSlices, "mse")
LM_stats_mae <- LM_fit(pooled_panel, timeSlices, "mae")
summary(LM_stats_mse[[1]]$model)
LM_stats_mse[[1]]$loss_stats
LM_stats_mse[[2]]$loss_stats
LM_stats_mse[[3]]$loss_stats
summary(LM_stats_mae[[1]]$model)
LM_stats_mae[[1]]$loss_stats
LM_stats_mae[[2]]$loss_stats
LM_stats_mae[[3]]$loss_stats
summary(LM_stats_mae[[1]]$model)
RF_MSE <- RF_fit_stats(f, pooled_panel, RF_grid, timeSlices, "mse")
#Random Forest
#Random Forest should also be fairly straightforward, only two tuning parameters mtry and ntree (usually)
#The vanilla randomForest package is very basic and too slow
#We will use the ranger package, which is apparently a C++ implementation of the random forest algorithm and should be faster
#Another alternative is to use XGBoost (which is very very fast) and set boosting rounds = 1, because in this case it should be a standard random forest
#Compute power is a real struggle, and the aim is just to get the code basis working for the moment. Very conservative grids are therefore used for now
#Check out randomForestSRC for an implementation of randomForests that allows for quantile regression trees. Quantile regression use a check loss function, which is a superset of absolute error
#forestr package doesn't seem to want to function on honours lab computers
#Default ntree = 500
#Default mtry = count(predictors/3) = 400/3 = 133
RF_grid <- expand.grid(
#ntree usually isn't tuned. Just set to max of computationally feasible
ntree = 5,
mtry = seq(10, round(ncol(pooled_panel[4:ncol(pooled_panel)])/3), 10),
nodesize = seq(2, 14, 2)
# nodedepth recommended not to be changed
#nodedepth = 1,
)
# Fit an RF model over the entire grid of hyperparameters
# Ideally, we would save all the models as we computed them
# This is very memory intensive, hence they will NOT be saved
# Downside is that the optimal model will have to be recomputed from the optimal hyperparameters
# Relevant lines have been commented out for now
RF_fit_model_grid <- function(f, train, validation, RF_grid, loss_function) {
#Initialize List
RF_model_grid <- rep(list(0), nrow(RF_grid))
for (i in 1:nrow(RF_grid)) {
RF_model_grid[[i]] <- list(RF_grid = 0, model = 0)
#MSE Case
if (loss_function == "mse") {
RF <- rfsrc(f, train,
#Hyperparameters
ntree = RF_grid$ntree[i],
mtry = RF_grid$mtry[i],
nodesize = RF_grid$nodesize[i],
splitrule = "mse"
)
#RF_model_grid[[i]]$model <- RF
RF_model_grid[[i]]$RF_grid <- cbind(RF_grid[i, ],
#Train Loss
train_loss = mse(train$rt, predict(RF)$predicted),
#Validation Loss
validation_loss = mse(validation$rt, predict(RF, newdata = validation)$predicted)
)
} else {
#MAE Case
RF <- rfsrc(f, train,
#Hyperparameters
ntree = RF_grid$ntree[i],
mtry = RF_grid$mtry[i],
nodesize = RF_grid$nodesize[i],
splitrule = "quantile.regr",
prob = 0.5
)
#RF_model_grid[[i]]$model <- RF
RF_model_grid[[i]]$RF_grid <- cbind(RF_grid[i, ],
#Train Loss
train_loss = mae(train$rt, predict(RF)$predicted),
#Validation Loss
validation_loss = mae(validation$rt, predict(RF, newdata = validation)$predicted)
)
}
}
return(RF_model_grid)
}
#Returns the dataframe row containing the "best" hyperparameters
get_RF_best_tune <- function(RF_model_grid) {
RF_tune_grid <- RF_model_grid[[1]]$RF_grid
for (i in 2:length(RF_model_grid)) {
RF_tune_grid <- rbind(RF_tune_grid, RF_model_grid[[i]]$RF_grid)
}
return(RF_tune_grid[which.min(RF_tune_grid$validation_loss), ])
}
RF_fit_stats <- function(f, pooled_panel, RF_grid, timeSlices, loss_function) {
#Initialize
RF_stats <- rep(list(0), 3)
#Load training, validation and test sets
for (set in 1:3) {
RF_stats[[set]] <- list(loss_stats = data.frame(train_MAE = 0, train_MSE = 0, train_RMSE = 0, train_RSquare = 0,
validation_MAE = 0, validation_MSE = 0, validation_RMSE = 0, validation_RSquare = 0,
test_MAE = 0, test_MSE = 0, test_RMSE = 0, test_RSquare = 0),
#Other useful things
forecasts = 0,
forecast_resids = 0,
model = 0)
train <- pooled_panel %>%
filter(time %in% timeSlices[[set]]$train)
validation <- pooled_panel %>%
filter(time %in% timeSlices[[set]]$validation)
test <- pooled_panel %>%
filter(time %in% timeSlices[[set]]$test)
#Fit on training Set over grid of hyperparameters
model_grid <- RF_fit_model_grid(f, train, validation, RF_grid, loss_function)
#Get the best hyperparameters
best_model_params <- get_RF_best_tune(model_grid)
#Compute the optimal model
if (loss_function == "mse") {
model <- rfsrc(f, train,
#Hyperparameters
ntree = best_model_params$ntree,
mtry = best_model_params$mtry,
nodesize = best_model_params$nodesize,
splitrule = "mse"
)
} else {
model <- rfsrc(f, train,
#Hyperparameters
ntree = best_model_params$ntree,
mtry = best_model_params$mtry,
nodesize = best_model_params$nodesize,
splitrule = "quantile.regr",
prob = 0.5
)
}
#Train
train_predict <- predict(model, train_x, alpha = best_model_params$alpha, lambda = best_model_params$lambda)
ELN_stats[[set]]$loss_stats$train_MAE <- mae(train_y, train_predict)
ELN_stats[[set]]$loss_stats$train_MSE <- mse(train_y, train_predict)
ELN_stats[[set]]$loss_stats$train_RMSE <- rmse(train_y, train_predict)
ELN_stats[[set]]$loss_stats$train_RSquare <- (1 - sse(train_y, train_predict) / sum((train_y - mean(train_y))^2))
#Validation
valid_predict <- predict(model, validation_x, alpha = best_model_params$alpha, lambda = best_model_params$lambda)
ELN_stats[[set]]$loss_stats$validation_MAE <- mae(validation_y, valid_predict)
ELN_stats[[set]]$loss_stats$validation_MSE <- mse(validation_y, valid_predict)
ELN_stats[[set]]$loss_stats$validation_RMSE <- rmse(validation_y, valid_predict)
ELN_stats[[set]]$loss_stats$validation_RSquare <- (1 - sse(validation_y, valid_predict) / sum((validation_y - mean(validation_y))^2))
#Test
test_predict <- predict(model, test_x, alpha = best_model_params$alpha, lambda = best_model_params$lambda)
ELN_stats[[set]]$loss_stats$test_MAE <- mae(test_y, test_predict)
ELN_stats[[set]]$loss_stats$test_MSE <- mse(test_y, test_predict)
ELN_stats[[set]]$loss_stats$test_RMSE <- rmse(test_y, test_predict)
ELN_stats[[set]]$loss_stats$test_RSquare <- (1 - sse(test_y, test_predict) / sum((test_y - mean(test_y))^2))
#Forecasts
ELN_stats[[set]]$forecasts <- predict(model, test_x, alpha = best_model_params$alpha, lambda = best_model_params$lambda)
#Forecast residuals
ELN_stats[[set]]$forecast_resids <- test_y - predict(model, test_x, alpha = best_model_params$alpha, lambda = best_model_params$lambda)
}
return(RF_stats)
}
RF_MSE <- RF_fit_stats(f, pooled_panel, RF_grid, timeSlices, "mse")
?rfsrc
RF_fit_stats <- function(f, pooled_panel, RF_grid, timeSlices, loss_function) {
#Initialize
RF_stats <- rep(list(0), 3)
#Load training, validation and test sets
for (set in 1:3) {
RF_stats[[set]] <- list(loss_stats = data.frame(train_MAE = 0, train_MSE = 0, train_RMSE = 0, train_RSquare = 0,
validation_MAE = 0, validation_MSE = 0, validation_RMSE = 0, validation_RSquare = 0,
test_MAE = 0, test_MSE = 0, test_RMSE = 0, test_RSquare = 0),
#Other useful things
forecasts = 0,
forecast_resids = 0,
model = 0)
train <- pooled_panel %>%
filter(time %in% timeSlices[[set]]$train)
validation <- pooled_panel %>%
filter(time %in% timeSlices[[set]]$validation)
test <- pooled_panel %>%
filter(time %in% timeSlices[[set]]$test)
#Fit on training Set over grid of hyperparameters
model_grid <- RF_fit_model_grid(f, train, validation, RF_grid, loss_function)
#Get the best hyperparameters
best_model_params <- get_RF_best_tune(model_grid)
#Compute the optimal model
if (loss_function == "mse") {
model <- rfsrc(f, train,
#Hyperparameters
ntree = best_model_params$ntree,
mtry = best_model_params$mtry,
nodesize = best_model_params$nodesize,
splitrule = "mse"
)
} else {
model <- rfsrc(f, train,
#Hyperparameters
ntree = best_model_params$ntree,
mtry = best_model_params$mtry,
nodesize = best_model_params$nodesize,
splitrule = "quantile.regr",
prob = 0.5
)
}
#Train
train_predict <- predict(model, train, alpha = best_model_params$alpha, lambda = best_model_params$lambda)
ELN_stats[[set]]$loss_stats$train_MAE <- mae(train$rt, train_predict)
ELN_stats[[set]]$loss_stats$train_MSE <- mse(train$rt, train_predict)
ELN_stats[[set]]$loss_stats$train_RMSE <- rmse(train$rt, train_predict)
ELN_stats[[set]]$loss_stats$train_RSquare <- (1 - sse(train$rt, train_predict) / sum((train$rt - mean(train$rt))^2))
#Validation
valid_predict <- predict(model, validation, alpha = best_model_params$alpha, lambda = best_model_params$lambda)
ELN_stats[[set]]$loss_stats$validation_MAE <- mae(validation$rt, valid_predict)
ELN_stats[[set]]$loss_stats$validation_MSE <- mse(validation$rt, valid_predict)
ELN_stats[[set]]$loss_stats$validation_RMSE <- rmse(validation$rt, valid_predict)
ELN_stats[[set]]$loss_stats$validation_RSquare <- (1 - sse(validation$rt, valid_predict) / sum((validation$rt - mean(validation$rt))^2))
#Test
test_predict <- predict(model, test, alpha = best_model_params$alpha, lambda = best_model_params$lambda)
ELN_stats[[set]]$loss_stats$test_MAE <- mae(test$rt, test_predict)
ELN_stats[[set]]$loss_stats$test_MSE <- mse(test$rt, test_predict)
ELN_stats[[set]]$loss_stats$test_RMSE <- rmse(test$rt, test_predict)
ELN_stats[[set]]$loss_stats$test_RSquare <- (1 - sse(test$rt, test_predict) / sum((test$rt - mean(test$rt))^2))
#Forecasts
ELN_stats[[set]]$forecasts <- test_predict
#Forecast residuals
ELN_stats[[set]]$forecast_resids <- test$rt - test_predict
}
return(RF_stats)
}
RF_MSE <- RF_fit_stats(f, pooled_panel, RF_grid, timeSlices, "mse")
?predict.rfsrc
RF_fit_stats <- function(f, pooled_panel, RF_grid, timeSlices, loss_function) {
#Initialize
RF_stats <- rep(list(0), 3)
#Load training, validation and test sets
for (set in 1:3) {
RF_stats[[set]] <- list(loss_stats = data.frame(train_MAE = 0, train_MSE = 0, train_RMSE = 0, train_RSquare = 0,
validation_MAE = 0, validation_MSE = 0, validation_RMSE = 0, validation_RSquare = 0,
test_MAE = 0, test_MSE = 0, test_RMSE = 0, test_RSquare = 0),
#Other useful things
forecasts = 0,
forecast_resids = 0,
model = 0)
train <- pooled_panel %>%
filter(time %in% timeSlices[[set]]$train)
validation <- pooled_panel %>%
filter(time %in% timeSlices[[set]]$validation)
test <- pooled_panel %>%
filter(time %in% timeSlices[[set]]$test)
#Fit on training Set over grid of hyperparameters
model_grid <- RF_fit_model_grid(f, train, validation, RF_grid, loss_function)
#Get the best hyperparameters
best_model_params <- get_RF_best_tune(model_grid)
#Compute the optimal model
if (loss_function == "mse") {
model <- rfsrc(f, train,
#Hyperparameters
ntree = best_model_params$ntree,
mtry = best_model_params$mtry,
nodesize = best_model_params$nodesize,
splitrule = "mse"
)
} else {
model <- rfsrc(f, train,
#Hyperparameters
ntree = best_model_params$ntree,
mtry = best_model_params$mtry,
nodesize = best_model_params$nodesize,
splitrule = "quantile.regr",
prob = 0.5
)
}
#Train
train_predict <- predict(model, train, alpha = best_model_params$alpha, lambda = best_model_params$lambda)$predicted
ELN_stats[[set]]$loss_stats$train_MAE <- mae(train$rt, train_predict)
ELN_stats[[set]]$loss_stats$train_MSE <- mse(train$rt, train_predict)
ELN_stats[[set]]$loss_stats$train_RMSE <- rmse(train$rt, train_predict)
ELN_stats[[set]]$loss_stats$train_RSquare <- (1 - sse(train$rt, train_predict) / sum((train$rt - mean(train$rt))^2))
#Validation
valid_predict <- predict(model, newdata = validation, alpha = best_model_params$alpha, lambda = best_model_params$lambda)$predicted
ELN_stats[[set]]$loss_stats$validation_MAE <- mae(validation$rt, valid_predict)
ELN_stats[[set]]$loss_stats$validation_MSE <- mse(validation$rt, valid_predict)
ELN_stats[[set]]$loss_stats$validation_RMSE <- rmse(validation$rt, valid_predict)
ELN_stats[[set]]$loss_stats$validation_RSquare <- (1 - sse(validation$rt, valid_predict) / sum((validation$rt - mean(validation$rt))^2))
#Test
test_predict <- predict(model, newdata = test, alpha = best_model_params$alpha, lambda = best_model_params$lambda)$predicted
ELN_stats[[set]]$loss_stats$test_MAE <- mae(test$rt, test_predict)
ELN_stats[[set]]$loss_stats$test_MSE <- mse(test$rt, test_predict)
ELN_stats[[set]]$loss_stats$test_RMSE <- rmse(test$rt, test_predict)
ELN_stats[[set]]$loss_stats$test_RSquare <- (1 - sse(test$rt, test_predict) / sum((test$rt - mean(test$rt))^2))
#Forecasts
ELN_stats[[set]]$forecasts <- test_predict
#Forecast residuals
ELN_stats[[set]]$forecast_resids <- test$rt - test_predict
}
return(RF_stats)
}
RF_MSE <- RF_fit_stats(f, pooled_panel, RF_grid, timeSlices, "mse")
#Random Forest
#Random Forest should also be fairly straightforward, only two tuning parameters mtry and ntree (usually)
#The vanilla randomForest package is very basic and too slow
#We will use the ranger package, which is apparently a C++ implementation of the random forest algorithm and should be faster
#Another alternative is to use XGBoost (which is very very fast) and set boosting rounds = 1, because in this case it should be a standard random forest
#Compute power is a real struggle, and the aim is just to get the code basis working for the moment. Very conservative grids are therefore used for now
#Check out randomForestSRC for an implementation of randomForests that allows for quantile regression trees. Quantile regression use a check loss function, which is a superset of absolute error
#forestr package doesn't seem to want to function on honours lab computers
#Default ntree = 500
#Default mtry = count(predictors/3) = 400/3 = 133
RF_grid <- expand.grid(
#ntree usually isn't tuned. Just set to max of computationally feasible
ntree = 5,
mtry = seq(10, round(ncol(pooled_panel[4:ncol(pooled_panel)])/3), 10),
nodesize = seq(2, 14, 2)
# nodedepth recommended not to be changed
#nodedepth = 1,
)
# Fit an RF model over the entire grid of hyperparameters
# Ideally, we would save all the models as we computed them
# This is very memory intensive, hence they will NOT be saved
# Downside is that the optimal model will have to be recomputed from the optimal hyperparameters
# Relevant lines have been commented out for now
RF_fit_model_grid <- function(f, train, validation, RF_grid, loss_function) {
#Initialize List
RF_model_grid <- rep(list(0), nrow(RF_grid))
for (i in 1:nrow(RF_grid)) {
RF_model_grid[[i]] <- list(RF_grid = 0, model = 0)
#MSE Case
if (loss_function == "mse") {
RF <- rfsrc(f, train,
#Hyperparameters
ntree = RF_grid$ntree[i],
mtry = RF_grid$mtry[i],
nodesize = RF_grid$nodesize[i],
splitrule = "mse"
)
#RF_model_grid[[i]]$model <- RF
RF_model_grid[[i]]$RF_grid <- cbind(RF_grid[i, ],
#Train Loss
train_loss = mse(train$rt, predict(RF)$predicted),
#Validation Loss
validation_loss = mse(validation$rt, predict(RF, newdata = validation)$predicted)
)
} else {
#MAE Case
RF <- rfsrc(f, train,
#Hyperparameters
ntree = RF_grid$ntree[i],
mtry = RF_grid$mtry[i],
nodesize = RF_grid$nodesize[i],
splitrule = "quantile.regr",
prob = 0.5
)
#RF_model_grid[[i]]$model <- RF
RF_model_grid[[i]]$RF_grid <- cbind(RF_grid[i, ],
#Train Loss
train_loss = mae(train$rt, predict(RF)$predicted),
#Validation Loss
validation_loss = mae(validation$rt, predict(RF, newdata = validation)$predicted)
)
}
}
return(RF_model_grid)
}
#Returns the dataframe row containing the "best" hyperparameters
get_RF_best_tune <- function(RF_model_grid) {
RF_tune_grid <- RF_model_grid[[1]]$RF_grid
for (i in 2:length(RF_model_grid)) {
RF_tune_grid <- rbind(RF_tune_grid, RF_model_grid[[i]]$RF_grid)
}
return(RF_tune_grid[which.min(RF_tune_grid$validation_loss), ])
}
RF_fit_stats <- function(f, pooled_panel, RF_grid, timeSlices, loss_function) {
#Initialize
RF_stats <- rep(list(0), 3)
#Load training, validation and test sets
for (set in 1:3) {
RF_stats[[set]] <- list(loss_stats = data.frame(train_MAE = 0, train_MSE = 0, train_RMSE = 0, train_RSquare = 0,
validation_MAE = 0, validation_MSE = 0, validation_RMSE = 0, validation_RSquare = 0,
test_MAE = 0, test_MSE = 0, test_RMSE = 0, test_RSquare = 0),
#Other useful things
forecasts = 0,
forecast_resids = 0,
model = 0)
train <- pooled_panel %>%
filter(time %in% timeSlices[[set]]$train)
validation <- pooled_panel %>%
filter(time %in% timeSlices[[set]]$validation)
test <- pooled_panel %>%
filter(time %in% timeSlices[[set]]$test)
#Fit on training Set over grid of hyperparameters
model_grid <- RF_fit_model_grid(f, train, validation, RF_grid, loss_function)
#Get the best hyperparameters
best_model_params <- get_RF_best_tune(model_grid)
#Compute the optimal model
if (loss_function == "mse") {
model <- rfsrc(f, train,
#Hyperparameters
ntree = best_model_params$ntree,
mtry = best_model_params$mtry,
nodesize = best_model_params$nodesize,
splitrule = "mse"
)
} else {
model <- rfsrc(f, train,
#Hyperparameters
ntree = best_model_params$ntree,
mtry = best_model_params$mtry,
nodesize = best_model_params$nodesize,
splitrule = "quantile.regr",
prob = 0.5
)
}
#Train
train_predict <- predict(model, train, alpha = best_model_params$alpha, lambda = best_model_params$lambda)$predicted
RF_stats[[set]]$loss_stats$train_MAE <- mae(train$rt, train_predict)
RF_stats[[set]]$loss_stats$train_MSE <- mse(train$rt, train_predict)
RF_stats[[set]]$loss_stats$train_RMSE <- rmse(train$rt, train_predict)
RF_stats[[set]]$loss_stats$train_RSquare <- (1 - sse(train$rt, train_predict) / sum((train$rt - mean(train$rt))^2))
#Validation
valid_predict <- predict(model, newdata = validation, alpha = best_model_params$alpha, lambda = best_model_params$lambda)$predicted
RF_stats[[set]]$loss_stats$validation_MAE <- mae(validation$rt, valid_predict)
RF_stats[[set]]$loss_stats$validation_MSE <- mse(validation$rt, valid_predict)
RF_stats[[set]]$loss_stats$validation_RMSE <- rmse(validation$rt, valid_predict)
RF_stats[[set]]$loss_stats$validation_RSquare <- (1 - sse(validation$rt, valid_predict) / sum((validation$rt - mean(validation$rt))^2))
#Test
test_predict <- predict(model, newdata = test, alpha = best_model_params$alpha, lambda = best_model_params$lambda)$predicted
RF_stats[[set]]$loss_stats$test_MAE <- mae(test$rt, test_predict)
RF_stats[[set]]$loss_stats$test_MSE <- mse(test$rt, test_predict)
RF_stats[[set]]$loss_stats$test_RMSE <- rmse(test$rt, test_predict)
RF_stats[[set]]$loss_stats$test_RSquare <- (1 - sse(test$rt, test_predict) / sum((test$rt - mean(test$rt))^2))
#Forecasts
RF_stats[[set]]$forecasts <- test_predict
#Forecast residuals
RF_stats[[set]]$forecast_resids <- test$rt - test_predict
}
return(RF_stats)
}
RF_MSE <- RF_fit_stats(f, pooled_panel, RF_grid, timeSlices, "mse")
RF_MSE[[1]]$loss_stats
RF_MSE[[2]]$loss_stats
RF_MSE[[3]]$loss_stats
RF_MAE <- RF_fit_stats(f, pooled_panel, RF_grid, timeSlices, "mae")
RF_MAE[[1]]$loss_stats
RF_MAE[[2]]$loss_stats
RF_MAE[[3]]$loss_stats
RF_grid <- expand.grid(
#ntree usually isn't tuned. Just set to max of computationally feasible
ntree = 100,
mtry = seq(10, round(ncol(pooled_panel[4:ncol(pooled_panel)])/3), 10),
nodesize = seq(2, 14, 2)
# nodedepth recommended not to be changed
#nodedepth = 1,
)
RF_MSE <- RF_fit_stats(f, pooled_panel, RF_grid, timeSlices, "mse")
RF_MAE <- RF_fit_stats(f, pooled_panel, RF_grid, timeSlices, "mae")
RF_MAE[[1]]$loss_stats
RF_MAE[[2]]$loss_stats
RF_MSE[[2]]$loss_stats
RF_MSE[[3]]$loss_stats
RF_MAE[[3]]$loss_stats
RF_MSE[[3]]$loss_stats
RF_MSE[[3]]$loss_stats
RF_MAE[[3]]$loss_stats
