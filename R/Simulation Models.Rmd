---
title: "Simulation Models"
author: "Ze Yu Zhong"
date: "19 June 2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

################
##Load Libraries
################

library(glmnet)
library(tidyverse)
library(keras)
library(ggplot2)
library(caret)
library(forecast)
library(rlist)
library(plm)
library(Metrics)
library(ranger)

set.seed(27935248)
```

```{r train_valid_test}
#Create Training + Test Sets

#$T = 180$ monthly periods corresponds to 15 years. The training sample was set to start from $T = 108$ or 9 years, a validation set 1 year in length. The last 3 years were reserved as a test set never to be used for validation or training.

#This means that there are only 3 sample periods to train on, yay

#Caret and other packages are not so good at dealing with custom training/validation/test sets. Have to specify resampling, tuning and testing manually (ugh)

time_slices <- createTimeSlices(1:(Time-36), initialWindow = 108, horizon = 12, fixedWindow = FALSE, skip = 11)

true_test <- list(c(121:180), c(133:180), c(145:180))

time_slices <- list.append(time_slices, true_test)

names(time_slices) <- c("train", "validation", "test")

```

```{r pooled_ols}
#POLS

#Pooled OLS does not have anything hyperparameters to tune, easy example to start off with

#Expand formula manually this way because PLM is dumb and gets confused by y ~ .

plm_formula <- function(panel){
  colnames <- colnames(pooled_panel)
  colnames <- colnames[-c(1:3)]
  f <- as.formula(c("return ~ ", paste(colnames, collapse = "+")))
  return(f)
}

f <- plm_formula(pooled_panel)

#Initialize Loss Function Statistics

POLS_stats <- data.frame(
  set = c(1:3),
  #Training
  train_MAE = c(1:3),
  train_MSE = c(1:3),
  train_RMSE = c(1:3),
  train_R2 = c(1:3),
  #Validation
  valid_MAE = c(1:3),
  valid_MSE = c(1:3),
  valid_RMSE = c(1:3),
  valid_R2 = c(1:3),
  #Test
  test_MAE = c(1:3),
  test_MSE = c(1:3),
  test_RMSE = c(1:3),
  test_R2 = c(1:3)
)

#Initialize Forecasts

POLS_forecasts <- rep(list(0), 3)

#Initialize Models

POLS_models <- rep(list(0), 3)

for (set in 1:3) {
  #Load training, validation and test sets
    train <- pooled_panel %>%
      filter(time %in% unlist(time_slices$train[set]))
    validation <- pooled_panel %>%
      filter(time %in% unlist(time_slices$validation[set]))
    test <- pooled_panel %>%
      filter(time %in% unlist(time_slices$test[set]))
    
  #Initialize forecasts
    POLS_forecasts[[set]] <- data.frame(
      time = test$time,
      forecast = rep(0, length(test$time)),
      forecast_error = rep(0, length(test$time))
    )

  #Train model on training set
  
    pols <- plm(f, data = train, model = "pooling", index = c("time", "stock"))
    POLS_models[[set]] <- pols
    
  #Usually Tuning via Validation set would be here, but POLS does not need tuning
  
  #Training Set Statistics
    POLS_stats$train_MAE[set] <- mae(train$return, predict(pols))
    POLS_stats$train_MSE[set] <- mse(train$return, predict(pols))
    POLS_stats$train_RMSE[set] <- rmse(train$return, predict(pols))
    error <- (train$return - predict(pols))
    SSR <- t(error) %*% (error)
    SST <- t(train$return - mean(train$return)) %*% (train$return - mean(train$return))
    POLS_stats$train_R2[set] <- (1 - SSR/SST)
    
  #Validation Set Statistics
    POLS_stats$valid_MAE[set] <- mae(validation$return, predict(pols, newdata = validation))
    POLS_stats$valid_MSE[set] <- mse(validation$return, predict(pols, newdata = validation))
    POLS_stats$valid_RMSE[set] <- rmse(validation$return, predict(pols, newdata = validation))
    
    error <- (validation$return - predict(pols, newdata = validation))
    SSR <- t(error) %*% (error)
    SST <- t(validation$return - mean(validation$return)) %*% (validation$return - mean(validation$return))
    POLS_stats$valid_R2[set] <- (1 - SSR/SST)
  
  #Test Set Statistics
    POLS_stats$test_MAE[set] <- mae(test$return, predict(pols, newdata = test))
    POLS_stats$test_MSE[set] <- mse(test$return, predict(pols, newdata = test))
    POLS_stats$test_RMSE[set] <- rmse(test$return, predict(pols, newdata = test))
    
    error <- (test$return - predict(pols, newdata = test))
    SSR <- t(error) %*% (error)
    SST <- t(test$return - mean(test$return)) %*% (test$return - mean(test$return))
    POLS_stats$test_R2[set] <- (1 - SSR/SST)
    
  #Forecasts
    POLS_forecasts[[set]]$forecast <- predict(pols, newdata = test)
    POLS_forecasts[[set]]$forecast_error <- test$return - predict(pols, newdata = test)
}

POLS_stats

summary(POLS_models[[3]])


```

```{r elastic_net}
#Elasticnet

#Elasticnet only has alpha to tune, thank god

elasticnet_grid <- expand.grid(
  alpha = seq(0, 1, 0.01)
)

#Initialize Loss Function Statistics

elasticnet_stats <- data.frame(
  set = c(1:3),
  #Training
  train_MAE = c(1:3),
  train_MSE = c(1:3),
  train_RMSE = c(1:3),
  train_R2 = c(1:3),
  #Validation
  valid_MAE = c(1:3),
  valid_MSE = c(1:3),
  valid_RMSE = c(1:3),
  valid_R2 = c(1:3),
  #Test
  test_MAE = c(1:3),
  test_MSE = c(1:3),
  test_RMSE = c(1:3),
  test_R2 = c(1:3)
)

#Initialize Forecasts

elasticnet_forecasts <- rep(list(0), 3)

#Initialize Models

elasticnet_models <- rep(list(0), 3)

for (set in 1:3) {
  #Load training, validation and test sets
    train <- pooled_panel %>%
      filter(time %in% unlist(time_slices$train[set]))
    train_x <- train %>%
      select(-return, -time, -stock)
    train_y <- train %>%
      select(return)
    
    validation <- pooled_panel %>%
      filter(time %in% unlist(time_slices$validation[set]))
    validation_x <- validation %>%
      select(-return, -time, -stock)
    validation_y <- validation %>%
      select(return)
    
    test <- pooled_panel %>%
      filter(time %in% unlist(time_slices$test[set]))
    test_x <- test %>%
      select(-return, -time, -stock)
    test_y <- test %>%
      select(return)
    
  #Initialize forecasts
    elasticnet_forecasts[[set]] <- data.frame(
      time = test$time,
      forecast = rep(0, length(test$time)),
      forecast_error = rep(0, length(test$time))
    )

  #Train model on training set
  
    elasticnet <- glmnet(x = train_x, y = train_y)
    POLS_models[[set]] <- pols
    
  #Usually Tuning via Validation set would be here, but POLS does not need tuning
  
  #Training Set Statistics
    POLS_stats$train_MAE[set] <- mae(train$return, predict(pols))
    POLS_stats$train_MSE[set] <- mse(train$return, predict(pols))
    POLS_stats$train_RMSE[set] <- rmse(train$return, predict(pols))
    error <- (train$return - predict(pols))
    SSR <- t(error) %*% (error)
    SST <- t(train$return - mean(train$return)) %*% (train$return - mean(train$return))
    POLS_stats$train_R2[set] <- (1 - SSR/SST)
    
  #Validation Set Statistics
    POLS_stats$valid_MAE[set] <- mae(validation$return, predict(pols, newdata = validation))
    POLS_stats$valid_MSE[set] <- mse(validation$return, predict(pols, newdata = validation))
    POLS_stats$valid_RMSE[set] <- rmse(validation$return, predict(pols, newdata = validation))
    
    error <- (validation$return - predict(pols, newdata = validation))
    SSR <- t(error) %*% (error)
    SST <- t(validation$return - mean(validation$return)) %*% (validation$return - mean(validation$return))
    POLS_stats$valid_R2[set] <- (1 - SSR/SST)
  
  #Test Set Statistics
    POLS_stats$test_MAE[set] <- mae(test$return, predict(pols, newdata = test))
    POLS_stats$test_MSE[set] <- mse(test$return, predict(pols, newdata = test))
    POLS_stats$test_RMSE[set] <- rmse(test$return, predict(pols, newdata = test))
    
    error <- (test$return - predict(pols, newdata = test))
    SSR <- t(error) %*% (error)
    SST <- t(test$return - mean(test$return)) %*% (test$return - mean(test$return))
    POLS_stats$test_R2[set] <- (1 - SSR/SST)
    
  #Forecasts
    POLS_forecasts[[set]]$forecast <- predict(pols, newdata = test)
    POLS_forecasts[[set]]$forecast_error <- test$return - predict(pols, newdata = test)
}

```

```{r random_forest}
#Random Forest

#Random Forest should also be fairly straightforward, only two tuning parameters mtry and ntree (usually)

#The vanilla randomForest package is very basic and too slow
#We will use the ranger package, which is apparently a C++ implementation of the random forest algorithm and should be faster
#Another alternative is to use XGBoost (which is very very fast) and set boosting rounds = 1, because in this case it should be a standard random forest

#COmpute power is a real struggle, and the aim is just to get the code basis working for the moment. Very conservative grids are therefore used for now

#Default ntree = 500
#Default mtry = count(predictors/3) = 400/3 = 133

random_forest_grid <- expand.grid(
  ntree = seq(10, 50, 10),
  mtry = seq(10, 100, 10)
)

#Generate formula, actually same code from PLM earlier

rf_formula <- function(panel){
  colnames <- colnames(pooled_panel)
  colnames <- colnames[-c(1:3)]
  f <- as.formula(c("return ~ ", paste(colnames, collapse = "+")))
  return(f)
}

f <- rf_formula(pooled_panel)

#Initialize Loss Function Statistics

RF_stats <- data.frame(
  set = c(1:3),
  #Training
  train_MAE = c(1:3),
  train_MSE = c(1:3),
  train_RMSE = c(1:3),
  train_R2 = c(1:3),
  #Validation
  valid_MAE = c(1:3),
  valid_MSE = c(1:3),
  valid_RMSE = c(1:3),
  valid_R2 = c(1:3),
  #Test
  test_MAE = c(1:3),
  test_MSE = c(1:3),
  test_RMSE = c(1:3),
  test_R2 = c(1:3)
)

#Initialize Forecasts

RF_forecasts <- rep(list(0), 3)

#Initialize Models

RF_models <- rep(list(0), 3)

for (set in 1:3) {
  #Load training, validation and test sets
    train <- pooled_panel %>%
      filter(time %in% unlist(time_slices$train[set]))
    validation <- pooled_panel %>%
      filter(time %in% unlist(time_slices$validation[set]))
    test <- pooled_panel %>%
      filter(time %in% unlist(time_slices$test[set]))
    
  #Initialize forecasts
    RF_forecasts[[set]] <- data.frame(
      time = test$time,
      forecast = rep(0, length(test$time)),
      forecast_error = rep(0, length(test$time))
    )

  #Train model on training set
  
    rf <- ranger(f, data = train, model = "pooling", index = c("stock", "time"))
    RF_models[[set]] <- rf
    
  #Usually Tuning via Validation set would be here, but POLS does not need tuning
  
  #Training Set Statistics
    RF_stats$train_MAE[set] <- mae(train$return, predict(rf))
    RF_stats$train_MSE[set] <- mse(train$return, predict(rf))
    RF_stats$train_RMSE[set] <- rmse(train$return, predict(rf))
    #R squared still pending, haven't figured out how to do it yet fully
    
  #Validation Set Statistics
    RF_stats$valid_MAE[set] <- mae(validation$return, predict(rf, newdata = validation))
    RF_stats$valid_MSE[set] <- mse(validation$return, predict(rf, newdata = validation))
    RF_stats$valid_RMSE[set] <- rmse(validation$return, predict(rf, newdata = validation))
    #R squared still pending, haven't figured out how to do it yet fully
  
  #Test Set Statistics
    RF_stats$test_MAE[set] <- mae(test$return, predict(rf, newdata = test))
    RF_stats$test_MSE[set] <- mse(test$return, predict(rf, newdata = test))
    RF_stats$test_RMSE[set] <- rmse(test$return, predict(rf, newdata = test))
    #R squared still pending, haven't figured out how to do it yet fully
    
  #Forecasts
    RF_forecasts[[set]]$forecast <- predict(rf, newdata = test)
    RF_forecasts[[set]]$forecast_error <- test$return - predict(rf, newdata = test)
    
}

```

```{r}
#Train RF Model + tune
rf <- ranger(f, data = train, 
             #Hyperparameters
             
             #Tuning parameters
              num.trees = 10,
              mtry = 10,
             
             #No Tuning
              #Default Min node size
              min.node.size = 5,
             
             #Other
             write.forest = TRUE,
             importance = "impurity"
             )

importance(rf)

```


```{r neural_networks}
#Neural Network

```

