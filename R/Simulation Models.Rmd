---
title: "Simulation Models"
author: "Ze Yu Zhong"
date: "19 June 2019"
output: html_document
---

```{r setup, include=FALSE}
################
##Load Libraries
################

library(speedglm)
library(tidyverse)
library(keras)
library(ggplot2)
library(forecast)
library(rlist)
library(Metrics)
library(tensorflow)
library(quantreg)
library(randomForestSRC)
library(hqreg)
library(caret)
library(tsfeatures)

## Vector arima/garch packages

library(xts)
library(rmgarch)
library(marima)
library(xgboost)

#Parallel Computing
library(foreach)
library(doFuture)
#Registering
registerDoFuture()
plan(multisession)
```

```{r train_valid_test}
#Create Training + Test Sets

# Gu et al set a training, validation and test sample equal in length for their simulations. Not the most sensible idea

#$T = 180$ monthly periods corresponds to 15 years. The training sample was set to start from $T = 108$ or 9 years, a validation set 1 year in length. The last 3 years were reserved as a test set never to be used for validation or training.

#This means that there are only 3 sample periods to train on, yay

#Caret and other packages are not so good at dealing with custom training/validation/test sets. Have to specify resampling, tuning and testing manually (ugh)

# Change this to 9 + 3 + 3, maybe more stable procedure

customTimeSlices <- function(start, initialWindow, horizon, validation_size, test_size, set_no) {
  
  time_slice <- list(train = 0, validation = 0, test = 0)
  time_slices <- rep(list(time_slice), set_no)
  
  for (t in 1:set_no) {
    time_slice$train <- c(start:(initialWindow + (t-1) * horizon + 1))
    time_slice$validation <- c((initialWindow + (t-1) * horizon + 2):((initialWindow + (t-1) * horizon) + validation_size + 1))
    time_slice$test <- c((initialWindow + (t-1) * horizon) + validation_size + 2):((initialWindow + (t-1) * horizon) + validation_size + test_size + 1)
    time_slices[[t]] <- time_slice
  }
  time_slices
}

#Create custom time slices
# These parameters give you the following train/validation/test splits (in terms of years)
# 6 4 3
# 7 4 3
# 8 4 3
timeSlices <- customTimeSlices(start = 2, initialWindow = 84, horizon = 12, validation_size = 60, test_size = 12, set_no = 3)

#Formula Function, makes it easier for those packages with a formula interface

panel_formula <- function(panel){
  #Remove the first 3 colNames, as these correspond to the return, time and stock id
  panel_colnames <- colnames(panel)[-c(1:3)]
  f <- as.formula(c("rt ~", paste(panel_colnames, collapse = "+")))
  return(f)
}
```

```{r, eval = FALSE}
g1_A1_nosv_0 <- readRDS("~/GitHub/Evaluation-of-Machine-Learning-in-Asset-Pricing/R/Simulated_dataset/g1_A1_nosv_0.RDS")

pooled_panel <- g1_A1_nosv_0[[1]]$panel

g1_A1_sv_0.1 <- readRDS("~/GitHub/Evaluation-of-Machine-Learning-in-Asset-Pricing/R/Simulated_dataset/g1_A1_sv_0.1.RDS")

pooled_panel <- g1_A1_sv_0.1[[1]]$panel

f <- panel_formula(pooled_panel)
```

```{r variable_importance_functions_foreach}
# Foreach parallel implementation of previous section
# There are a LOT of predictors to inerate over and often this is the bottleneck for the model fitting procedure
# Therefore, use these parallel implementations if possible
# Also, the code is noticeably neater

# Variable Importance Metric Functions
# Given a single model and test set, return a variable importance dataframe (don't bother sorting them now)
# Note that a training set is NOT needed because this is to be called AFTER the model is fit
# These functions were originally written after the actul fitting functions were
# These are used by the fitting functions, so they need to be declared before them

# Similarly, different functions are needed for all model types because all the model objects have slightly different predict methods

################################################
# Linear Model
LM_variable_importance <- function(test, lm_model) {
  test_x <- test[4:ncol(test)]
  
  # Specify .packages = "quantreg" here as it seems it isn't supported and therefore is missed by doFuture
  variable_importance_df <- foreach(i = (1:ncol(test_x)), .combine = "rbind", .packages = c("speedglm", "quantreg")) %dopar% {
    test_x_zero <- test_x
    test_x_zero[, i] <- 0
    
    original_R2 <- R2(predict(lm_model, newdata = test_x), test$rt, form = "traditional")
    
    new_R2 <- R2(predict(lm_model, newdata = test_x_zero), test$rt, form = "traditional")
    
    variable_importance <- data.frame(variable = colnames(test_x)[i], importance = (original_R2 - new_R2))
    
    variable_importance
  }
  variable_importance_df
}

# Penalized Linear Model

ELN_variable_importance <- function(test, eln_model, alpha, lambda) {
  test_x <- as.matrix(test[4:ncol(test)])
  
  variable_importance_df <- foreach(i = (1:ncol(test_x)), .combine = "rbind") %dopar% {
    test_x_zero <- test_x
    test_x_zero[, i] <- 0
    
    original_R2 <- R2(predict(eln_model, test_x, alpha = alpha, lambda = lambda), test$rt, form = "traditional")
    
    new_R2 <- R2(predict(eln_model, test_x_zero, alpha = alpha, lambda = lambda), test$rt, form = "traditional")
    
    variable_importance <- data.frame(variable = colnames(test_x)[i], importance = (original_R2 - new_R2))
    
    variable_importance
  }
  variable_importance_df
}

# Random Forest

RF_variable_importance <- function(test, rf_model) {
  test_x <- test[4:ncol(test)]
  
  variable_importance_df <- foreach(i = (1:ncol(test_x)), .combine = "rbind", .packages = "randomForestSRC") %do% {
    test_x_zero <- test_x
    test_x_zero[, i] <- 0
    
    original_R2 <- R2(predict(rf_model, newdata = test_x)$predicted, test$rt, form = "traditional")
    
    new_R2 <- R2(predict(rf_model, newdata = test_x_zero)$predicted, test$rt, form = "traditional")
    
    variable_importance <- data.frame(variable = colnames(test_x)[i], importance = (original_R2 - new_R2))
                                      
    variable_importance
  }
variable_importance_df
}

# Neural Network
# Unfortunately it seems that the tensorflow/keras backend does not work with foreach, so a standard for loop is needed instead

NNet_variable_importance <- function(test, nnet_model) {

  test_x <- test[4:ncol(test)]

  variable_importance_df <- foreach(i = (1:ncol(test_x)), .combine = "rbind", .packages = c("keras", "tensorflow", "reticulate")) %do% {
    test_x_zero <- test_x
    test_x_zero[, i] <- 0

    original_R2 <- R2(predict(nnet_model, as.matrix(test_x)), test$rt, form = "traditional")

    new_R2 <- R2(predict(nnet_model, as.matrix(test_x_zero)), test$rt, form = "traditional")

    variable_importance <- data.frame(variable = colnames(test_x)[i], importance = (original_R2 - new_R2))

    variable_importance
  }
variable_importance_df
}
```

```{r cross_sectional_average_residuals}
## Function to calculate cross sectional average residuals, used for Diebold Mariano Tests later
## Don't really believe in this, mostly done because its popular in literature
# Similarly, different functions are needed for each type of model object as they all have slightly different predict methods

## Linear Model
# Given a an lm object and a test dataset, return the cross sectional average forecast errors

lm_ave_forecast_resids <- function(lm_model, test) {
  
  time_periods <- unique(test$time)
  
  # Set .combine to "c" to return a vector of results, which is what we want
  ave_forecast_resids_vector <- foreach(t = 1:length(time_periods), .combine = "c", .packages = c("speedglm", "quantreg")) %do% {
    
    test_cross_section <- test %>%
      filter(time == time_periods[t])
    
    residuals <- test_cross_section$rt - predict(lm_model, newdata = test_cross_section)
    
    mean(residuals)
  }
  ave_forecast_resids_vector
}

## Elastic Net

eln_ave_forecast_resids <- function(eln_model, test, alpha, lambda) {
  
  time_periods <- unique(test$time)
  
  # Set .combine to "c" to return a vector of results, which is what we want
  ave_forecast_resids_vector <- foreach(t = 1:length(time_periods), .combine = "c") %do% {
    
    test_cross_section <- test %>%
      filter(time == time_periods[t])

    test_cross_section_x <- as.matrix(test_cross_section[4:ncol(test_cross_section)])
    
    residuals <- test_cross_section$rt - predict(eln_model, test_cross_section_x,
                                                 alpha = alpha, lambda = lambda)
    
    mean(residuals)
  }
  ave_forecast_resids_vector
}

## Random Forest

rf_ave_forecast_resids <- function(rf_model, test) {
  
  time_periods <- unique(test$time)
  
  # Set .combine to "c" to return a vector of results, which is what we want
  ave_forecast_resids_vector <- foreach(t = 1:length(time_periods), .combine = "c") %do% {
    
    test_cross_section <- test %>%
      filter(time == time_periods[t])
    
    residuals <- test_cross_section$rt - predict(rf_model, newdata = test_cross_section)$predicted
    
    mean(residuals)
  }
  ave_forecast_resids_vector
}

## Neural Networks

nnet_ave_forecast_resids <- function(nnet_model, test) {
  
  time_periods <- unique(test$time)
  
  # Set .combine to "c" to return a vector of results, which is what we want
  ave_forecast_resids_vector <- foreach(t = 1:length(time_periods), .combine = "c") %do% {
    
    test_cross_section <- test %>%
      filter(time == time_periods[t])
    
    test_cross_section_x <- test_cross_section[4:ncol(test_cross_section)]
    
    residuals <- test_cross_section$rt - (nnet_model %>% predict(as.matrix(test_cross_section_x)))
    
    mean(residuals)
  }
  ave_forecast_resids_vector
}

```

```{r pooled_ols}
LM_fit <- function(pooled_panel, timeSlices, loss_function, f) {
  #Initialize Loss Function Statistics
  LM_stats <- rep(list(0), 3)
  
  for (set in 1:3) {
    LM_stats[[set]] <- list(loss_stats = data.frame(train_MAE = 0, train_MSE = 0, train_RMSE = 0, train_RSquare = 0, 
                                                    validation_MAE = 0, validation_MSE = 0, validation_RMSE = 0, validation_RSquare = 0, 
                                                    test_MAE = 0, test_MSE = 0, test_RMSE = 0, test_RSquare = 0),
                            #Other useful things
                            forecast_resids = 0,
                            model = 0,
                            #Variable Importance
                            variable_importance = 0)
    
    #Load Training, validation and test sets
    train <- pooled_panel %>%
      filter(time %in% timeSlices[[set]]$train)
    validation <- pooled_panel %>%
      filter(time %in% timeSlices[[set]]$validation)
    test <- pooled_panel %>%
      filter(time %in% timeSlices[[set]]$test)
    
    #Train Model on training set
    
    # Set model = FALSE so that it won't save a copy of the training data in the model object
    # Doen for memory efficiency
    #MSE case
    if (loss_function == "mse") {
      lm <- lm(f, data = train, model = FALSE, y = FALSE)
    } else {
      # Use pfn as method here for much faster computation
      lm <- rq(f, data = train, tau = 0.5, method = "pfn")
    }
    
    #LM_stats[[set]]$model <- lm
    
    #No Tuning Needed
    
    #Statistics
    #Training Set
    train_predict <- predict(lm, newdata = train)
    LM_stats[[set]]$loss_stats$train_MAE <- mae(train$rt, train_predict)
    LM_stats[[set]]$loss_stats$train_MSE <- mse(train$rt, train_predict)
    LM_stats[[set]]$loss_stats$train_RMSE <- rmse(train$rt, train_predict)
    LM_stats[[set]]$loss_stats$train_RSquare <- R2(train_predict, train$rt, form = "traditional")
    
    #Validation Set Statistics
    validation_predict <- predict(lm, newdata = validation)
    LM_stats[[set]]$loss_stats$validation_MAE <- mae(validation$rt, validation_predict)
    LM_stats[[set]]$loss_stats$validation_MSE <- mse(validation$rt, validation_predict)
    LM_stats[[set]]$loss_stats$validation_RMSE <- rmse(validation$rt, validation_predict)
    LM_stats[[set]]$loss_stats$validation_RSquare <- R2(validation_predict, validation$rt, form = "traditional")
      
    #Test Set Statistics
    test_predict <- predict(lm, newdata = test)
    LM_stats[[set]]$loss_stats$test_MAE <- mae(test$rt, test_predict)
    LM_stats[[set]]$loss_stats$test_MSE <- mse(test$rt, test_predict)
    LM_stats[[set]]$loss_stats$test_RMSE <- rmse(test$rt, test_predict)
    LM_stats[[set]]$loss_stats$test_RSquare <- R2(test_predict, test$rt, form = "traditional")

    #Forecast Residuals
    LM_stats[[set]]$forecast_resids <- lm_ave_forecast_resids(lm_model = lm, test = test)
    #Variable Importance
    LM_stats[[set]]$variable_importance <- LM_variable_importance(test, lm)
  }
  return(LM_stats)
}
```

```{r, eval = FALSE}

LM_stats_mse <- LM_fit(pooled_panel, timeSlices, "mse", f)
# 
LM_stats_mae <- LM_fit(pooled_panel, timeSlices, "mae", f)
# 
LM_stats_mse[[1]]$loss_stats
LM_stats_mse[[2]]$loss_stats
LM_stats_mse[[3]]$loss_stats
# 
LM_stats_mse[[1]]$variable_importance %>% arrange(desc(importance))
LM_stats_mse[[2]]$variable_importance %>% arrange(desc(importance))
LM_stats_mse[[3]]$variable_importance %>% arrange(desc(importance))
# 
summary(LM_stats_mse[[1]]$model)
# 
LM_stats_mae[[1]]$loss_stats
LM_stats_mae[[2]]$loss_stats
LM_stats_mae[[3]]$loss_stats

summary(LM_stats_mae[[1]]$model)
# 
LM_stats_mae[[1]]$variable_importance %>% arrange(desc(importance))
LM_stats_mae[[2]]$variable_importance %>% arrange(desc(importance))
LM_stats_mae[[3]]$variable_importance %>% arrange(desc(importance))
# 

LM_stats_mse[[3]]$forecast_resids
LM_stats_mae[[3]]$forecast_resids

```

```{r elastic_net, eval = FALSE}
## DO NOT RUN THIS CHUNK. ITS IMPLEMENTATION IS VERY INTENSIVE AND SLOW. NOTE EVAL = FALSE FLAG
# KEPT HERE MAINLY FOR LEGACY PURPOSES

#Elasticnet WRT MSE

# Can't seem to find anything online that tunes ELN for you with specified validation sets, instead of straight-forward cross validation
# nfold and foldid require that ELN be performed with at least 3 fold validation - not possible in our context

# Elasticnet only has alpha and lambda to tune

# ELN is rather finicky and likes to generate its own sequence of lambda values during the actual fitting process. Really doesn't like specifying a singular lambda value
# Solution: follow what the documentation says and let it find its own lambda sequence
# Then, when supply custom grid values of lambda when actually predicting values. This can be either through exact = TRUE (refits the model with the additional lambda gridpoint), or exact = FALSE, which just uses linear interpolation
# According to documentation, linear interpolation is very fast and is generally "good enough" anyway

# Just keep the grid small for now to make sure everything is running
ELN_grid <- expand.grid(
  alpha = seq(0, 1, 0.01),
  lambda = 10^seq(-2, 2, length.out = 50)
)

######################################
## Functions to tune an ELN model
######################################

# Fit the model over the defined grid of hyperparameters, and return it as a list
# Takes a defined grid of hyperparameters, a training set, and a validation set, fits the model over the grid, and returns the models + loss statistics as a list
# Works with both loss functions of MSE and MAE, need to specify these as "mse" or "mae" respectively

# Ideally, we would store all the model fitted over the grid. This takes up way too much memory, and because ELN is very efficiently fit anyway it's better not to store them. Relevant lines are commented out

ELN_model_grid <- function(ELN_grid, train_x, train_y, validation_x, validation_y, loss_function, nlambda) {
  ELN_model_grid <- rep(list(0), nrow(ELN_grid))
  
  for (i in 1:nrow(ELN_grid)) {
    
    ELN_model_grid[[i]] <- list(ELN_grid = 0, model = 0)
    
    # MSE Case
    if (loss_function == "mse") {
      ELN <- hqreg(train_x, train_y, method = "ls", alpha = ELN_grid$alpha[i], nlambda = nlambda)
      
      #ELN_model_grid[[i]]$model <- ELN
      
      ELN_model_grid[[i]]$ELN_grid <- cbind(alpha = ELN_grid$alpha[i], lambda = ELN_grid$lambda[i],
                                            train_loss = mse(train_y, predict(ELN, train_x, lambda = ELN_grid$lambda[i])),
                                            validation_loss = mse(validation_y, predict(ELN, validation_x, lambda = ELN_grid$lambda[i]))
                                            )
    } else {
      # MAE Case
      ELN <- hqreg(train_x, train_y, method = "quantile", tau = 0.5, alpha = ELN_grid$alpha[i], nlambda = nlambda)
      #ELN_model_grid[[i]]$model <- ELN

      ELN_model_grid[[i]]$ELN_grid <- cbind(alpha = ELN_grid$alpha[i], lambda = ELN_grid$lambda[i],
                                            train_loss = mae(train_y, predict(ELN, train_x, lambda = ELN_grid$lambda[i])),
                                            validation_loss = mae(validation_y, predict(ELN, validation_x, lambda = ELN_grid$lambda[i]))
                                            )
    }
  }
  return(ELN_model_grid)
}

get_ELN_best_tune <- function(model_grid) {
  ELN_tune_dataframe <- cbind(model_grid[[1]]$ELN_grid, list_index = 1)
  for (i in 2:length(model_grid)) {
    ELN_tune_dataframe <- rbind(ELN_tune_dataframe, cbind(model_grid[[i]]$ELN_grid, list_index = i))
  }
  ELN_tune_dataframe <- data.frame(ELN_tune_dataframe)
  return(ELN_tune_dataframe[which.min(ELN_tune_dataframe$validation_loss), ])
}

# Final big wrapper function that gives an ELN_stats list with everything desired

ELN_fit_stats <- function(ELN_grid, nlambda, timeSlices, pooled_panel, loss_function) {
  ELN_stats <- rep(list(0), 3)
  
  for (set in 1:3) {
    #Load Training, validation and test sets
    train <- pooled_panel %>%
      filter(time %in% timeSlices[[set]]$train)
    validation <- pooled_panel %>%
      filter(time %in% timeSlices[[set]]$validation)
    test <- pooled_panel %>%
      filter(time %in% timeSlices[[set]]$test)
    
    train_x <- as.matrix(train[4:ncol(train)])
    train_y <- as.matrix(train$rt)
    validation_x <- as.matrix(validation[4:ncol(validation)])
    validation_y <- as.matrix(validation$rt)
    test_x <- as.matrix(test[4:ncol(test)])
    test_y <- as.matrix(test$rt)
      
    #Get models fit over the grid of hyperparameters
    ELN_model_grid <- ELN_model_grid(ELN_grid, train_x, train_y, validation_x, validation_y, loss_function, nlambda = nlambda)
        
    #Get the best tuning parameters
    best_model_params <- get_ELN_best_tune(ELN_model_grid)
    
    #Initialize stats list
    ELN_stats[[set]] <- list(loss_stats = data.frame(train_MAE = 0, train_MSE = 0, train_RMSE = 0, train_RSquare = 0, 
                                                     validation_MAE = 0, validation_MSE = 0, validation_RMSE = 0, validation_RSquare = 0, 
                                                     test_MAE = 0, test_MSE = 0, test_RMSE = 0, test_RSquare = 0),
                             #Other useful things
                             forecasts = 0,
                             forecast_resids = 0,
                             model = 0)
      
    #Model
    if (loss_function == "mse") {
      model <- hqreg(train_x, train_y, method = "ls", alpha = best_model_params$alpha, nlambda = nlambda)
    } else {
      model <- hqreg(train_x, train_y, method = "quantile", tau = 0.5, alpha = best_model_params$alpha, nlambda = nlambda)
    }
    
    #ELN_stats[[set]]$model <- model
      
    #Loss Stats Dataframe
    #Train
    train_predict <- predict(model, train_x, alpha = best_model_params$alpha, lambda = best_model_params$lambda)
    ELN_stats[[set]]$loss_stats$train_MAE <- mae(train_y, train_predict)
    ELN_stats[[set]]$loss_stats$train_MSE <- mse(train_y, train_predict)
    ELN_stats[[set]]$loss_stats$train_RMSE <- rmse(train_y, train_predict)
    ELN_stats[[set]]$loss_stats$train_RSquare <- (1 - sse(train_y, train_predict) / sum((train_y - mean(train_y))^2))
        
    #Validation
    valid_predict <- predict(model, validation_x, alpha = best_model_params$alpha, lambda = best_model_params$lambda)
    ELN_stats[[set]]$loss_stats$validation_MAE <- mae(validation_y, valid_predict)
    ELN_stats[[set]]$loss_stats$validation_MSE <- mse(validation_y, valid_predict)
    ELN_stats[[set]]$loss_stats$validation_RMSE <- rmse(validation_y, valid_predict)
    ELN_stats[[set]]$loss_stats$validation_RSquare <- (1 - sse(validation_y, valid_predict) / sum((validation_y - mean(validation_y))^2))
    
    #Test
    test_predict <- predict(model, test_x, alpha = best_model_params$alpha, lambda = best_model_params$lambda)
    ELN_stats[[set]]$loss_stats$test_MAE <- mae(test_y, test_predict)
    ELN_stats[[set]]$loss_stats$test_MSE <- mse(test_y, test_predict)
    ELN_stats[[set]]$loss_stats$test_RMSE <- rmse(test_y, test_predict)
    ELN_stats[[set]]$loss_stats$test_RSquare <- (1 - sse(test_y, test_predict) / sum((test_y - mean(test_y))^2))
      
    #Forecasts
    ELN_stats[[set]]$forecasts <- test_predict
    
    #Forecast residuals
    ELN_stats[[set]]$forecast_resids <- test_y - test_predict
  }
  return(ELN_stats)
}
```

```{r elastic_net}
# Alternative implementation for ELN
# This is much, much faster
# DO NOT USE THE PREVIOUS IMPLEMENTATION
# Do not specify a grid for lambda. Instead, let the algorithm determine its own sequence of lambda
# Method: specify only a grid for alpha. For each grid value of alpha, fit an ELN model and let LARS determine a suitable path of lambdas. Produce validation error statistics for each alpha and lambda value. Use the best combination for the final model
# 

alpha_grid <- seq(0, 1, 0.01)

# Foreach implementation of above function
# This is much much faster

ELN_model_grid <- function(alpha_grid, train_x, train_y, validation_x, validation_y, loss_function, nlamb) {
  
  ELN_model_grid_list <- foreach(i = (1:length(alpha_grid))) %dopar% {
    ELN_model_grid <- list(ELN_grid = 0, model = 0)
    
    #MSE Case
    if (loss_function == "mse") {
      ELN <- hqreg(train_x, train_y, method = "ls", alpha = alpha_grid[i], nlambda = nlamb)
      ELN_model_grid$model <- ELN
    
      ELN_model_grid$ELN_grid <- cbind(alpha = alpha_grid[i], lambda = ELN$lambda,
                                       train_loss = apply(predict(ELN, train_x), 2, mse, actual = train_y), 
                                       validation_loss = apply(predict(ELN, validation_x), 2, mse, actual = validation_y))
    } else {
      #MAE Case
      ELN <- hqreg(train_x, train_y, method = "quantile", tau = 0.5, alpha = alpha_grid[i], nlambda = nlamb)
      ELN_model_grid$model <- ELN
    
      ELN_model_grid$ELN_grid <- cbind(alpha = alpha_grid[i], lambda = ELN$lambda,
                                       train_loss = apply(predict(ELN, train_x), 2, mse, actual = train_y), 
                                       validation_loss = apply(predict(ELN, validation_x), 2, mae, actual = validation_y))
    }
    ELN_model_grid
  }
  ELN_model_grid_list
}

# Given a list containing the model grid, return the "best" model according to validation loss
# Use in conjunction with previous function

get_ELN_best_tune <- function(model_grid) {
  ELN_tune_dataframe <- cbind(model_grid[[1]]$ELN_grid, list_index = 1)
  for (i in 2:length(model_grid)) {
    ELN_tune_dataframe <- rbind(ELN_tune_dataframe, cbind(model_grid[[i]]$ELN_grid, list_index = i))
  }
  ELN_tune_dataframe <- data.frame(ELN_tune_dataframe)
  return(ELN_tune_dataframe[which.min(ELN_tune_dataframe$validation_loss), ])
}

# Final big wrapper function that gives an ELN_stats list with everything desired

ELN_fit_stats <- function(alpha_grid, nlamb, timeSlices, pooled_panel, loss_function) {
  ELN_stats <- rep(list(0), 3)
  
  for (set in 1:3) {
    #Load Training, validation and test sets
    train <- pooled_panel %>%
      filter(time %in% timeSlices[[set]]$train)
    validation <- pooled_panel %>%
      filter(time %in% timeSlices[[set]]$validation)
    test <- pooled_panel %>%
      filter(time %in% timeSlices[[set]]$test)
    
    train_x <- as.matrix(train[4:ncol(train)])
    train_y <- as.matrix(train$rt)
    validation_x <- as.matrix(validation[4:ncol(validation)])
    validation_y <- as.matrix(validation$rt)
    test_x <- as.matrix(test[4:ncol(test)])
    test_y <- as.matrix(test$rt)
      
    #Get models fit over the grid of hyperparameters
    ELN_model_grid <- ELN_model_grid(alpha_grid, train_x, train_y, validation_x, validation_y, loss_function, nlamb = nlamb)
        
    #Get the best tuning parameters
    best_model_params <- get_ELN_best_tune(ELN_model_grid)
    
    #Initialize stats list
    ELN_stats[[set]] <- list(loss_stats = data.frame(train_MAE = 0, train_MSE = 0, train_RMSE = 0, train_RSquare = 0, 
                                                     validation_MAE = 0, validation_MSE = 0, validation_RMSE = 0, validation_RSquare = 0, 
                                                     test_MAE = 0, test_MSE = 0, test_RMSE = 0, test_RSquare = 0),
                             #Other useful things
                             forecasts = 0,
                             forecast_resids = 0,
                             model = 0,
                             hyperparameters = 0,
                             variable_importance = 0)
      
    #Model
    model <- ELN_model_grid[[best_model_params$list_index]]$model
    
    #ELN_stats[[set]]$model <- model
    
    #Hyperparameters
    ELN_stats[[set]]$hyperparameters <- best_model_params
      
    #Loss Stats Dataframe
    #Train
    train_predict <- predict(model, train_x, alpha = best_model_params$alpha, lambda = best_model_params$lambda)
    ELN_stats[[set]]$loss_stats$train_MAE <- mae(train_y, train_predict)
    ELN_stats[[set]]$loss_stats$train_MSE <- mse(train_y, train_predict)
    ELN_stats[[set]]$loss_stats$train_RMSE <- rmse(train_y, train_predict)
    ELN_stats[[set]]$loss_stats$train_RSquare <- (1 - sse(train_y, train_predict) / sum((train_y - mean(train_y))^2))
        
    #Validation
    valid_predict <- predict(model, validation_x, alpha = best_model_params$alpha, lambda = best_model_params$lambda)
    ELN_stats[[set]]$loss_stats$validation_MAE <- mae(validation_y, valid_predict)
    ELN_stats[[set]]$loss_stats$validation_MSE <- mse(validation_y, valid_predict)
    ELN_stats[[set]]$loss_stats$validation_RMSE <- rmse(validation_y, valid_predict)
    ELN_stats[[set]]$loss_stats$validation_RSquare <- (1 - sse(validation_y, valid_predict) / sum((validation_y - mean(validation_y))^2))
    
    #Test
    test_predict <- predict(model, test_x, alpha = best_model_params$alpha, lambda = best_model_params$lambda)
    ELN_stats[[set]]$loss_stats$test_MAE <- mae(test_y, test_predict)
    ELN_stats[[set]]$loss_stats$test_MSE <- mse(test_y, test_predict)
    ELN_stats[[set]]$loss_stats$test_RMSE <- rmse(test_y, test_predict)
    ELN_stats[[set]]$loss_stats$test_RSquare <- (1 - sse(test_y, test_predict) / sum((test_y - mean(test_y))^2))
    
    #Forecasts
    ELN_stats[[set]]$forecasts <- test_predict
    
    #Forecast residuals
    ELN_stats[[set]]$forecast_resids <- eln_ave_forecast_resids(eln_model = model, test, 
                                                                # Hyperparameters
                                                                alpha = best_model_params$alpha, 
                                                                lambda = best_model_params$lambda)
    
    #Variable Importance
    ELN_stats[[set]]$variable_importance <- ELN_variable_importance(test, model, alpha = best_model_params$alpha, lambda = best_model_params$lambda)
  }
  return(ELN_stats)
}
```

```{r, eval = FALSE}

#Testing if function works

ELN_stats_mae <- ELN_fit_stats(alpha_grid, nlamb = 100, timeSlices, pooled_panel, loss_function = "mae")

ELN_stats_mse <- ELN_fit_stats(alpha_grid, nlamb = 100, timeSlices, pooled_panel, loss_function = "mse")

ELN_stats_mse[[1]]$loss_stats
ELN_stats_mse[[1]]$model$alpha

ELN_stats_mse[[2]]$loss_stats
ELN_stats_mse[[2]]$model$alpha

ELN_stats_mse[[3]]$loss_stats
ELN_stats_mse[[3]]$model$alpha

ELN_stats_mse[[1]]$variable_importance %>% arrange(desc(importance))
ELN_stats_mse[[2]]$variable_importance %>% arrange(desc(importance))
ELN_stats_mse[[3]]$variable_importance %>% arrange(desc(importance))

ELN_stats_mae[[1]]$loss_stats
ELN_stats_mae[[1]]$model$alpha

ELN_stats_mae[[2]]$loss_stats
ELN_stats_mae[[2]]$model$alpha

ELN_stats_mae[[3]]$loss_stats
ELN_stats_mae[[3]]$model$alpha

ELN_stats_mae[[1]]$variable_importance %>% arrange(desc(importance))
ELN_stats_mae[[2]]$variable_importance %>% arrange(desc(importance))
ELN_stats_mae[[3]]$variable_importance %>% arrange(desc(importance))

# Notes
# Mostly consistent results with the previous implementation, very minor differences in optimal alpha values chosen
# Similarly, optimal alpha values are highly unstable
# Does not seem to particularly different between MAE and MSE versions

# Moving from A1 to A2
# LM struggles greatly with more complex A matrix specifications
# However, ELN actually does quite well in spite of this
# It actuallyhas higher R squared values than the true r squared, presumbaly indicating that it is somehow capturing the SV error structure
# In general, it seems that ELN is choosing very parsimonious representations (look at variable importance metrics that are at 0)
```

```{r random_forest}
#Random Forest

#Random Forest should also be fairly straightforward, only two tuning parameters mtry and ntree (usually)

#Check out randomForestSRC for an implementation of randomForests that allows for quantile regression trees. Quantile regression use a check loss function, which is a superset of absolute error
#forestr package doesn't seem to want to function on honours lab computers

#Default ntree = 500
#Default mtry = count(predictors/3) = 400/3 = 133

# Fit an RF model over the entire grid of hyperparameters
# Ideally, we would save all the models as we computed them
# This is very memory intensive, hence they will NOT be saved
# Downside is that the optimal model will have to be recomputed from the optimal hyperparameters

# Special Note: variable importance using a standard for loop (not parallelized) here is very computationally intensive
# ALWAYS make sure that the parallelized version fo RF variable importance is used

RF_fit_model_grid <- function(f, train, validation, RF_grid, loss_function) {
  #Initialize List
  RF_model_grid <- rep(list(0), nrow(RF_grid))
  
  for (i in 1:nrow(RF_grid)) {
    
    RF_model_grid[[i]] <- list(RF_grid = 0, model = 0)
    
    #MSE Case
    if (loss_function == "mse") {
      RF <- rfsrc(f, train, 
                  #Hyperparameters
                  ntree = RF_grid$ntree[i],
                  mtry = RF_grid$mtry[i],
                  nodesize = RF_grid$nodesize[i],
                  splitrule = "mse"
                  )
      
      #RF_model_grid[[i]]$model <- RF
      RF_model_grid[[i]]$RF_grid <- cbind(RF_grid[i, ],
                                          #Train Loss
                                          train_loss = mse(train$rt, predict(RF)$predicted),
                                          #Validation Loss
                                          validation_loss = mse(validation$rt, predict(RF, newdata = validation)$predicted)
                                          )
      
    } else {
      #MAE Case
      RF <- rfsrc(f, train, 
                  #Hyperparameters
                  ntree = RF_grid$ntree[i],
                  mtry = RF_grid$mtry[i],
                  nodesize = RF_grid$nodesize[i],
                  splitrule = "quantile.regr",
                  prob = 0.5
                  )
      
      #RF_model_grid[[i]]$model <- RF
      RF_model_grid[[i]]$RF_grid <- cbind(RF_grid[i, ],
                                          #Train Loss
                                          train_loss = mae(train$rt, predict(RF)$predicted),
                                          #Validation Loss
                                          validation_loss = mae(validation$rt, predict(RF, newdata = validation)$predicted)
                                          )
    }
  }
  return(RF_model_grid)
}

#Returns the dataframe row containing the "best" hyperparameters

get_RF_best_tune <- function(RF_model_grid) {
  RF_tune_grid <- RF_model_grid[[1]]$RF_grid
  for (i in 2:length(RF_model_grid)) {
    RF_tune_grid <- rbind(RF_tune_grid, RF_model_grid[[i]]$RF_grid)
  }
  return(RF_tune_grid[which.min(RF_tune_grid$validation_loss), ])
}

RF_fit_stats <- function(pooled_panel, RF_grid, timeSlices, loss_function, f) {
  #Initialize
  RF_stats <- rep(list(0), 3)
  
  #Load training, validation and test sets
  
  for (set in 1:3) {
    
    RF_stats[[set]] <- list(loss_stats = data.frame(train_MAE = 0, train_MSE = 0, train_RMSE = 0, train_RSquare = 0, 
                                                    validation_MAE = 0, validation_MSE = 0, validation_RMSE = 0, validation_RSquare = 0, 
                                                    test_MAE = 0, test_MSE = 0, test_RMSE = 0, test_RSquare = 0),
                            #Other useful things
                            forecast_resids = 0,
                            model = 0,
                            hyperparameters = 0,
                            variable_importance = 0)
    
    train <- pooled_panel %>%
      filter(time %in% timeSlices[[set]]$train)
    validation <- pooled_panel %>%
      filter(time %in% timeSlices[[set]]$validation)
    test <- pooled_panel %>%
      filter(time %in% timeSlices[[set]]$test)
    
    #Fit on training Set over grid of hyperparameters
    
    model_grid <- RF_fit_model_grid(f, train, validation, RF_grid, loss_function)
    
    #Get the best hyperparameters
    
    best_model_params <- get_RF_best_tune(model_grid)
    RF_stats[[set]]$hyperparameters <- best_model_params
    
    #Compute the optimal model
    
    if (loss_function == "mse") {
      model <- rfsrc(f, train, 
                     #Hyperparameters
                     ntree = best_model_params$ntree,
                     mtry = best_model_params$mtry,
                     nodesize = best_model_params$nodesize,
                     splitrule = "mse",
                     bootstrap = "by.root", samptype = "swr"
                     )
    } else {
      model <- rfsrc(f, train, 
                     #Hyperparameters
                     ntree = best_model_params$ntree,
                     mtry = best_model_params$mtry,
                     nodesize = best_model_params$nodesize,
                     splitrule = "quantile.regr",
                     prob = 0.5,
                     bootstrap = "by.root", samptype = "swr"
                     )
    }
    
    #RF_stats[[set]]$model <- model
    
    #Train
    train_predict <- predict(model, train)$predicted
    RF_stats[[set]]$loss_stats$train_MAE <- mae(train$rt, train_predict)
    RF_stats[[set]]$loss_stats$train_MSE <- mse(train$rt, train_predict)
    RF_stats[[set]]$loss_stats$train_RMSE <- rmse(train$rt, train_predict)
    RF_stats[[set]]$loss_stats$train_RSquare <- (1 - sse(train$rt, train_predict) / sum((train$rt - mean(train$rt))^2))
        
    #Validation
    valid_predict <- predict(model, newdata = validation)$predicted
    RF_stats[[set]]$loss_stats$validation_MAE <- mae(validation$rt, valid_predict)
    RF_stats[[set]]$loss_stats$validation_MSE <- mse(validation$rt, valid_predict)
    RF_stats[[set]]$loss_stats$validation_RMSE <- rmse(validation$rt, valid_predict)
    RF_stats[[set]]$loss_stats$validation_RSquare <- (1 - sse(validation$rt, valid_predict) / sum((validation$rt - mean(validation$rt))^2))
    
    #Test
    test_predict <- predict(model, newdata = test)$predicted
    RF_stats[[set]]$loss_stats$test_MAE <- mae(test$rt, test_predict)
    RF_stats[[set]]$loss_stats$test_MSE <- mse(test$rt, test_predict)
    RF_stats[[set]]$loss_stats$test_RMSE <- rmse(test$rt, test_predict)
    RF_stats[[set]]$loss_stats$test_RSquare <- (1 - sse(test$rt, test_predict) / sum((test$rt - mean(test$rt))^2))
    
    #Forecast residuals
    RF_stats[[set]]$forecast_resids <- rf_ave_forecast_resids(rf_model = model, test = test)
    
    #Variable Importance
    RF_stats[[set]]$variable_importance <- RF_variable_importance(test, model)
  }
  return(RF_stats)
}
```

```{r, eval = FALSE}

RF_grid <- expand.grid(
      #ntree usually isn't tuned. Just set to max of computationally feasible
      ntree = 100,
      mtry = seq(20, round(ncol(pooled_panel[4:ncol(pooled_panel)])/3), 20)
      # nodesize = seq(2, 14, 2)
      # nodedepth recommended not to be changed
      #nodedepth = 1
      )

RF_MSE <- RF_fit_stats(pooled_panel, RF_grid, timeSlices, "mse")

RF_MAE <- RF_fit_stats(pooled_panel, RF_grid, timeSlices, "mae")

RF_MSE[[1]]$loss_stats
RF_MSE[[2]]$loss_stats
RF_MSE[[3]]$loss_stats

RF_MSE[[1]]$variable_importance %>% arrange(desc(importance))

RF_MAE[[1]]$loss_stats
RF_MAE[[2]]$loss_stats
RF_MAE[[3]]$loss_stats

RF_MAE[[1]]$variable_importance %>% arrange(desc(importance))
RF_MAE[[1]]$forecast_resids

dm.test(RF_MSE[[1]]$forecast_resids, RF_MAE[[1]]$forecast_resids)
```

```{r neural_networks}
# Neural Networks

set.seed(27935248)

#Build neural networks with neuron numbers according to geometric pyramid rule, and ReLU activation function for all layers
# IE Input layer > 32 neurons > 16 neurons > 8 neurons > 4 neurons > 2 neurons > output

##Neural Network generalized

NNet_fit_stats <- function(pooled_panel, timeSlices, hidden_layers, loss_function, batch_size, patience) {
  #Initialize
  
  NNet_stats <- rep(list(0), 3)
  
  for (set in 1:3) {
    NNet_stats[[set]] <- list(loss_stats = data.frame(train_MAE = 0, train_MSE = 0, train_RMSE = 0, train_RSquare = 0, 
                                                      validation_MAE = 0, validation_MSE = 0, validation_RMSE = 0, validation_RSquare = 0, 
                                                      test_MAE = 0, test_MSE = 0, test_RMSE = 0, test_RSquare = 0),
                              #Other useful things
                              # Keep forecasts here in for nnet objects to make sure they aren't doing something stupid
                              forecasts = 0,
                              forecast_resids = 0,
                              model = 0,
                              variable_importance = 0)
      
    #Load Training, validation and test sets
    train <- pooled_panel %>%
      filter(time %in% timeSlices[[set]]$train)
    validation <- pooled_panel %>%
      filter(time %in% timeSlices[[set]]$validation)
    test<- pooled_panel %>%
      filter(time %in% timeSlices[[set]]$test)
      
    train_x <- train[4:ncol(train)]
    train_x <- scale(train_x)
    col_means_train <- attr(train_x, "scaled:center") 
    col_stddevs_train <- attr(train_x, "scaled:scale")
    train_y <- train$rt
    
    validation_x <- validation[4:ncol(validation)]
    validation_x <- scale(validation_x, center = col_means_train, scale = col_stddevs_train)
    validation_y <- validation$rt
      
    test_x <- test[4:ncol(test)]
    test_x <- scale(test_x, center = col_means_train, scale = col_stddevs_train)
    test_y <- test$rt
      
    # Fit the model
    # The patience parameter is the amount of epochs to check for improvement.
    # Gu et al don't say what their early stopping parameter p is
    early_stop <- callback_early_stopping(monitor = "val_loss", patience = patience, restore_best_weights = TRUE)
    
    print_dot_callback <- callback_lambda(
      on_epoch_end = function(epoch, logs) {
        if (epoch %% 50 == 0) cat("\n")
        cat(".")
      }
    ) 
    
    l1_penalty <- 0.01
    
    build_NN <- function(hidden_layers, loss_function) {
        
      if (hidden_layers == 1) {
        
        model <- keras_model_sequential() %>%
        
        # Layer 1
        layer_dense(units = 32, input_shape = ncol(train_x),
                    bias_initializer = initializer_zeros(),
                    kernel_initializer = initializer_truncated_normal(mean = 0, stddev = 0.05, seed = NULL)) %>%
          layer_activation("tanh") %>%
          layer_activity_regularization(l1 = l1_penalty) %>%
          layer_batch_normalization() %>%
        # Output Layer
          layer_dense(units = 1, activation = "linear")
        
      } else if (hidden_layers == 2) {
        
        model <- keras_model_sequential() %>%
          
        # Layer 1
        layer_dense(units = 32, input_shape = ncol(train_x),
                    bias_initializer = initializer_zeros(),
                    kernel_initializer = initializer_truncated_normal(mean = 0, stddev = 0.05, seed = NULL)) %>%
          layer_activation("tanh") %>%
          layer_activity_regularization(l1 = l1_penalty) %>%
          layer_batch_normalization() %>%
        # Layer 2
        layer_dense(units = 16,
                    bias_initializer = initializer_zeros(),
                    kernel_initializer = initializer_truncated_normal(mean = 0, stddev = 0.05, seed = NULL)) %>%
          layer_activation("tanh") %>%
          layer_activity_regularization(l1 = l1_penalty) %>%
          layer_batch_normalization() %>%
        # Output Layer
        layer_dense(units = 1, activation = "linear")
          
      } else if (hidden_layers == 3) {
          
        model <- keras_model_sequential() %>%
          
        # Layer 1
        layer_dense(units = 32, input_shape = ncol(train_x),
                    bias_initializer = initializer_zeros(),
                    kernel_initializer = initializer_truncated_normal(mean = 0, stddev = 0.05, seed = NULL)) %>%
          layer_activation("tanh") %>%
          layer_activity_regularization(l1 = l1_penalty) %>%
          layer_batch_normalization() %>%
        # Layer 2
        layer_dense(units = 16,
                    bias_initializer = initializer_zeros(),
                    kernel_initializer = initializer_truncated_normal(mean = 0, stddev = 0.05, seed = NULL)) %>%
          layer_activation("tanh") %>%
          layer_activity_regularization(l1 = l1_penalty) %>%
          layer_batch_normalization() %>%
        #Layer 3
        layer_dense(units = 8,
                    bias_initializer = initializer_zeros(),
                    kernel_initializer = initializer_truncated_normal(mean = 0, stddev = 0.05, seed = NULL)) %>%
          layer_activation("tanh") %>%
          layer_activity_regularization(l1 = l1_penalty) %>%
          layer_batch_normalization() %>%
        # Output Layer
        layer_dense(units = 1, activation = "linear")
          
      } else if (hidden_layers == 4) {
          
        model <- keras_model_sequential() %>%
        
        # Layer 1
        layer_dense(units = 32, input_shape = ncol(train_x),
                    bias_initializer = initializer_zeros(),
                    kernel_initializer = initializer_truncated_normal(mean = 0, stddev = 0.05, seed = NULL)) %>%
          layer_activation("tanh") %>%
          layer_activity_regularization(l1 = l1_penalty) %>%
          layer_batch_normalization() %>%
        # Layer 2
        layer_dense(units = 16,
                    bias_initializer = initializer_zeros(),
                    kernel_initializer = initializer_truncated_normal(mean = 0, stddev = 0.05, seed = NULL)) %>%
          layer_activation("tanh") %>%
          layer_activity_regularization(l1 = l1_penalty) %>%
          layer_batch_normalization() %>%
        #Layer 3
        layer_dense(units = 8,
                    bias_initializer = initializer_zeros(),
                    kernel_initializer = initializer_truncated_normal(mean = 0, stddev = 0.05, seed = NULL)) %>%
          layer_activation("tanh") %>%
          layer_activity_regularization(l1 = l1_penalty) %>%
          layer_batch_normalization() %>%
        #Layer 4
        layer_dense(units = 4,
                    bias_initializer = initializer_zeros(),
                    kernel_initializer = initializer_truncated_normal(mean = 0, stddev = 0.05, seed = NULL)) %>%
          layer_activation("tanh") %>%
          layer_activity_regularization(l1 = l1_penalty) %>%
          layer_batch_normalization() %>%
        # Output Layer
        layer_dense(units = 1, activation = "linear")
          
      } else {
        
        model <- keras_model_sequential() %>%
        
        # Layer 1
        layer_dense(units = 32, input_shape = ncol(train_x),
                    bias_initializer = initializer_zeros(),
                    kernel_initializer = initializer_truncated_normal(mean = 0, stddev = 0.05, seed = NULL)) %>%
          layer_activation("tanh") %>%
          layer_activity_regularization(l1 = l1_penalty) %>%
          layer_batch_normalization() %>%
        # Layer 2
        layer_dense(units = 16,
                    bias_initializer = initializer_zeros(),
                    kernel_initializer = initializer_truncated_normal(mean = 0, stddev = 0.05, seed = NULL)) %>%
          layer_activation("tanh") %>%
          layer_activity_regularization(l1 = l1_penalty) %>%
          layer_batch_normalization() %>%
        #Layer 3
        layer_dense(units = 8,
                    bias_initializer = initializer_zeros(),
                    kernel_initializer = initializer_truncated_normal(mean = 0, stddev = 0.05, seed = NULL)) %>%
          layer_activation("tanh") %>%
          layer_activity_regularization(l1 = l1_penalty) %>%
          layer_batch_normalization() %>%
        #Layer 4
        layer_dense(units = 4,
                    bias_initializer = initializer_zeros(),
                    kernel_initializer = initializer_truncated_normal(mean = 0, stddev = 0.05, seed = NULL)) %>%
          layer_activation("tanh") %>%
          layer_activity_regularization(l1 = l1_penalty) %>%
          layer_batch_normalization() %>%
        #Layer 5
        layer_dense(units = 2,
                    bias_initializer = initializer_zeros(),
                    kernel_initializer = initializer_truncated_normal(mean = 0, stddev = 0.05, seed = NULL)) %>%
          layer_activation("tanh") %>%
          layer_activity_regularization(l1 = l1_penalty) %>%
          layer_batch_normalization() %>%
        # Output Layer
        layer_dense(units = 1, activation = "linear")
          
      }
      model %>% compile(
        loss = loss_function,
        optimizer = optimizer_adam(lr = 0.001, beta_1 = 0.9, beta_2 = 0.999,
                                   epsilon = NULL, clipnorm = NULL,
                                   clipvalue = NULL),
        metrics = list("mae", "mse")
      )
      model
    }
      
    neural_network <- build_NN(hidden_layers, loss_function)
    
    # Other options used throughout the neural network fitting process are specified here
    # Namely, batch size
    # In general, larger batch sizes result in faster progress in training, but don't always converge as fast. Smaller batch sizes train slower, but can converge faster.
    # Default batch size is 32
    neural_network %>% fit(as.matrix(train_x), as.matrix(train_y), 
                           batch_size = batch_size, epochs = 500, verbose = 0, 
                           validation_data = list(as.matrix(validation_x), as.matrix(validation_y)),
                           callbacks = list(early_stop, print_dot_callback))
    
    #Model
    #NNet_stats[[set]]$model <- neural_network
    
    #Train
    train_predict <- neural_network %>% predict(as.matrix(train_x))
    
    NNet_stats[[set]]$loss_stats$train_MAE <- mae(train$rt, train_predict)
    NNet_stats[[set]]$loss_stats$train_MSE <- mse(train$rt, train_predict)
    NNet_stats[[set]]$loss_stats$train_RMSE <- rmse(train$rt, train_predict)
    NNet_stats[[set]]$loss_stats$train_RSquare <- R2(train_predict, train$rt, form = "traditional")
          
    #Validation
    validation_predict <- neural_network %>% predict(as.matrix(validation_x))
    
    NNet_stats[[set]]$loss_stats$validation_MAE <- mae(validation$rt, validation_predict)
    NNet_stats[[set]]$loss_stats$validation_MSE <- mse(validation$rt, validation_predict)
    NNet_stats[[set]]$loss_stats$validation_RMSE <- rmse(validation$rt, validation_predict)
    NNet_stats[[set]]$loss_stats$validation_RSquare <- R2(validation_predict, validation$rt, form = "traditional")
      
    #Test
    test_predict <- neural_network %>% predict(as.matrix(test_x))
    
    NNet_stats[[set]]$loss_stats$test_MAE <- mae(test$rt, test_predict)
    NNet_stats[[set]]$loss_stats$test_MSE <- mse(test$rt, test_predict)
    NNet_stats[[set]]$loss_stats$test_RMSE <- rmse(test$rt, test_predict)
    NNet_stats[[set]]$loss_stats$test_RSquare <- R2(test_predict, test$rt, form = "traditional")
        
    #Forecasts
    #NNet_stats[[set]]$forecasts <- test_predict
      
    #Forecast residuals
    NNet_stats[[set]]$forecast_resids <- nnet_ave_forecast_resids(nnet_model = neural_network, test = test)
    
    #Variable Importance
    NNet_stats[[set]]$variable_importance <- NNet_variable_importance(test, neural_network)
    
  }
  #Clear Kera Session
  k_clear_session()
  NNet_stats
}

```

```{r, eval = FALSE}

NNet_1_mse_stats <- NNet_fit_stats(pooled_panel, timeSlices, 1, "mse", batch_size = 32, 40)
NNet_1_mae_stats <- NNet_fit_stats(pooled_panel, timeSlices, 1, "mae", batch_size = 128, 40)

# # 
NNet_1_mse_stats[[1]]$loss_stats
NNet_1_mse_stats[[2]]$loss_stats
NNet_1_mse_stats[[3]]$loss_stats

NNet_1_mse_stats[[1]]$forecasts
NNet_1_mse_stats[[2]]$forecasts
NNet_1_mse_stats[[3]]$forecasts

NNet_1_mse_stats[[1]]$variable_importance %>% arrange(desc(importance))
# 
NNet_1_mae_stats[[1]]$loss_stats
NNet_1_mae_stats[[2]]$loss_stats
NNet_1_mae_stats[[3]]$loss_stats

NNet_1_mae_stats[[1]]$forecasts
NNet_1_mae_stats[[2]]$forecasts
NNet_1_mae_stats[[3]]$forecasts

NNet_1_mae_stats[[1]]$variable_importance %>% arrange(desc(importance))
# 
# ##
# 
# NNet_2_mse_stats <- NNet_fit_stats(pooled_panel, timeSlices, 2, "mse", batch_size = 512, 10)
# NNet_2_mae_stats <- NNet_fit_stats(pooled_panel, timeSlices, 2, "mae", batch_size = 512, 10)
# 
# NNet_2_mse_stats[[1]]$loss_stats
# NNet_2_mse_stats[[2]]$loss_stats
# NNet_2_mse_stats[[3]]$loss_stats
# 
# NNet_2_mae_stats[[1]]$loss_stats
# NNet_2_mae_stats[[2]]$loss_stats
# NNet_2_mae_stats[[3]]$loss_stats
# 
# ##
# 
# NNet_3_mse_stats <- NNet_fit_stats(pooled_panel, timeSlices, 3, "mse", batch_size = 512, 10)
# NNet_3_mae_stats <- NNet_fit_stats(pooled_panel, timeSlices, 3, "mae", batch_size = 512, 10)
# 
# NNet_3_mse_stats[[1]]$loss_stats
# NNet_3_mse_stats[[2]]$loss_stats
# NNet_3_mse_stats[[3]]$loss_stats
# 
# NNet_3_mae_stats[[1]]$loss_stats
# NNet_3_mae_stats[[2]]$loss_stats
# NNet_3_mae_stats[[3]]$loss_stats
# 
# ##
# 
# NNet_4_mse_stats <- NNet_fit_stats(pooled_panel, timeSlices, 4, "mse", batch_size = 32, 30)
# NNet_4_mae_stats <- NNet_fit_stats(pooled_panel, timeSlices, 4, "mae", batch_size = 32, 10)
# 
# NNet_4_mse_stats[[1]]$loss_stats
# NNet_4_mse_stats[[2]]$loss_stats
# NNet_4_mse_stats[[3]]$loss_stats
# 
# NNet_4_mae_stats[[1]]$loss_stats
# NNet_4_mae_stats[[2]]$loss_stats
# NNet_4_mae_stats[[3]]$loss_stats
# 
# ##
# 
NNet_5_mse_stats <- NNet_fit_stats(pooled_panel, timeSlices, 5, "mse", batch_size = 32, 10)
NNet_5_mae_stats <- NNet_fit_stats(pooled_panel, timeSlices, 5, "mae", batch_size = 32, 10)

NNet_5_mse_stats[[1]]$loss_stats
NNet_5_mse_stats[[2]]$loss_stats
NNet_5_mse_stats[[3]]$loss_stats

NNet_5_mse_stats[[1]]$forecasts

NNet_5_mse_stats[[1]]$variable_importance %>%
  arrange(desc(importance))

NNet_5_mae_stats[[1]]$loss_stats
NNet_5_mae_stats[[2]]$loss_stats
NNet_5_mae_stats[[3]]$loss_stats

NNet_5_mae_stats[[1]]$forecasts
NNet_5_mae_stats[[2]]$forecasts
NNet_5_mae_stats[[3]]$forecasts

NNet_5_mae_stats[[1]]$variable_importance %>%
  arrange(desc(importance))

# Other general observations
# Increasing the number of layers is NOT good. Neural Network 5 has validation loss graphs that do not look right at all

# Neural Networks WRT MAE are not good for highly non-linear specifications
# Very unstable out of sample r squared values
```

```{r}
## RNN and LSTM Neural Network Models

# Prepare the data into a format which RNNs and LSTM models from keras recognise and can work with
# We want a multipl einput (factor set), multiple output (multiple stocks) structure
# Annoyingly, this means we have to convert our dataset from tidy to array format

# Keras needs the X (input) to be an array of (sample no, time steps, no. of features) dimensions
# In our case, this corresponds to (length of training set, 1 as we are using 1 lag, 400 factors)

# Keras needs the Y output to be an array of (sample no, number of outputs per sample)
# In our case, this corresponds to (length of training set, no. of stocks/cross sectional units)

# Set the training length to be the entire smaple for now
train_length <- 180

X_array <- array(data = 0, dim = c(train_length, 1, 80000))

Y_array <- array(data = 0, dim = c(train_length, 200))

for(t in 1:train_length) {
  X_array[t, 1, ] <- pooled_panel %>%
  filter(time == t+1) %>%
  dplyr::select(-rt, -time, -stock) %>%
  as.matrix() %>%
  as.vector()
}

for(t in 1:train_length) {
  Y_array[t, ] <- pooled_panel %>%
    filter(time == t+1) %>%
    dplyr::select(rt) %>%
    as.matrix() %>%
    as.vector()
}

X_array_train <- X_array[(timeSlices[[1]]$train - 1), , ,drop = F]
X_array_validation <- X_array[(timeSlices[[1]]$validation - 1), , ,drop = F]
X_array_test <- X_array[(timeSlices[[1]]$test - 1), , ,drop = F]

Y_array_train <- Y_array[(timeSlices[[1]]$train - 1), ]
Y_array_validation <- Y_array[(timeSlices[[1]]$validation - 1), ]
Y_array_test <- Y_array[(timeSlices[[1]]$test - 1), ]

## Start building the model

l1_penalty <- 0.001

lstm_model <- keras_model_sequential() %>%
  layer_lstm(units = 4, input_shape = c(1, dim(X_array_train)[3]),
             return_sequences = TRUE) %>%
  layer_activation("tanh") %>%
  layer_activity_regularization(l1 = l1_penalty) %>%
  layer_batch_normalization() %>%
  
  layer_lstm(units = 2) %>%
  layer_activation("tanh") %>%
  layer_activity_regularization(l1 = l1_penalty) %>%
  layer_batch_normalization() %>%
  
  layer_dense(units = 200) %>%
  layer_activation("linear")

lstm_model %>% compile(loss = "mse",
                       optimizer = optimizer_adam(lr = 0.001, beta_1 = 0.9, beta_2 = 0.999),
                       metrics = list("mae", "mse"))

lstm_model %>% summary()

lstm_model_fit <- lstm_model %>% 
  fit(X_array_train, Y_array_train, epochs = 500,
      validation_data = list(X_array_validation, Y_array_validation), 
      batch_size = 32, shuffle = FALSE, verbose = 1)

lstm_predictions <- lstm_model %>% predict(X_array_test) %>%
  as.vector()

mse(Y_array_test %>% as.vector(), lstm_predictions)
mae(Y_array_test %>% as.vector(), lstm_predictions)
R2(lstm_predictions, Y_array_test %>% as.vector(), form = "traditional")
```


```{r}
#######################################################
## FFORMA
#######################################################

## FFORMA constituent models

## FFORMA requires a univariate model to be explicitly estimated for each individual time series
## This is not possible given the number of factors we have
## Solution: apply some dimensional reduction technique, such as t-SNE, and then apply univariate time series models such as ARIMA/GARCH
## univariate approaches

stock_1 <- pooled_panel %>%
  filter(stock == "stock_1")

stock_1_train <- stock_1 %>%
  filter(time %in% timeSlices[[1]]$train)
stock_1_validation <- stock_1 %>%
  filter(time %in% timeSlices[[1]]$validation)
stock_1_test <- stock_1 %>%
  filter(time %in% timeSlices[[1]]$test)

## Calculate principal components first
# Use tol = 0 to eliminate essentially constant components. CHosen because this is a somewhat systematic way of choosing principal components
stock_1_train_pca <- prcomp(stock_1_train[, -(1:3)], center = TRUE, scale. = TRUE, tol = 0)
summary(stock_1_train_pca)
plot(stock_1_train_pca)

train_x <- predict(stock_1_train_pca, newdata = stock_1_train[, -(1:3)])
train_x <- as.xts(train_x, order.by = as.Date(stock_1_train$time))

validation_x <- predict(stock_1_train_pca, newdata = stock_1_validation[, -(1:3)])
validation_x <- as.xts(validation_x, order.by = as.Date(stock_1_validation$time))

test_x <- predict(stock_1_train_pca, newdata = stock_1_test[, -(1:3)])
test_x <- as.xts(test_x, order.by = as.Date(stock_1_test$time))

stock_1_pca <- prcomp(stock_1[, -(1:3)], center = TRUE, scale. = TRUE, tol = 0.5)
summary(stock_1_pca)

stock_1_x <- predict(stock_1_pca, newdata = stock_1[, -(1:3)])

stock_1_true_x <- as.xts(stock_1[, (4:6)], order.by = as.Date(stock_1$time))

## Use these with ugarch models

ugarch_spec <- ugarchspec(variance.model = list(model = "sGARCH",
                                                         garchOrder = c(1, 1), 
                                                         submodel = NULL, 
                                                         external.regressors = stock_1_x, 
                                                         variance.targeting = FALSE), 
                          mean.model     = list(armaOrder = c(0, 0), 
                                                include.mean = TRUE,
                                                external.regressors = NULL),
                          distribution.model = "std")

rolling_forecasts <- ugarchroll(ugarch_spec, data = xts(stock_1$rt, order.by = as.Date(stock_1$time)), 
                                n.ahead = 1, n.start = 144, 
                                refit.every = 12, refit.window = "recursive")

plot(rolling_forecasts, which = 3)

as.data.frame(rolling_forecasts)$Mu

mae(actual = stock_1_test$rt, 
    as.data.frame(rolling_forecasts)$Mu)

pooled_panel %>%
  filter(time %in% timeSlices[[1]]$test | time %in% timeSlices[[2]]$test | time %in% timeSlices[[3]]$test) %>%
  filter(stock == "stock_1") %>%
  mutate(forecasts = as.data.frame(rolling_forecasts)$Mu) %>%
  select(time, stock, rt, forecasts) %>%
  gather("type", "value", -stock, -time) %>%
  ggplot() +
  geom_line(aes(x = time, y = value, colour = type))

ELN_stats_mse <- ELN_fit_stats(alpha_grid, nlamb = 100, timeSlices, pooled_panel, loss_function = "mse")

pooled_panel %>%
  filter(time %in% timeSlices[[1]]$test | time %in% timeSlices[[2]]$test | time %in% timeSlices[[3]]$test) %>%
  mutate(forecasts = rbind(ELN_stats_mse[[1]]$forecasts, ELN_stats_mse[[2]]$forecasts, ELN_stats_mse[[3]]$forecasts)) %>%
  select(time, stock, rt, forecasts) %>%
  filter(stock == "stock_1") %>%
  gather("type", "value", -stock, -time) %>%
  ggplot() +
  geom_line(aes(x = time, y = value, colour = type))

mae(actual = stock_1_test$rt, 
    rbind(ELN_stats_mse[[1]]$forecasts, ELN_stats_mse[[2]]$forecasts, ELN_stats_mse[[3]]$forecasts))

mae(actual = stock_1_test$rt, 
    0)

ELN_stats_mse[[1]]$forecasts
```

```{r}
## FFORMA implementation, pretending that each stock return series is an individual time series with no external regressors

# Get univariate time series training sample from cross section

stock_id <- unique(pooled_panel$stock)

univariate_time_series <- pooled_panel %>%
  filter(stock == stock_id[1]) %>%
  filter(time %in% timeSlices[[1]]$train | time %in% timeSlices[[1]]$validation) %>%
  select(rt) %>%
  ts()

univariate_time_series_test <- pooled_panel %>%
  filter(stock == stock_id[1]) %>%
  filter(time %in% timeSlices[[1]]$test) %>%
  select(rt) %>%
  ts()

## Calculating set of features

fforma_features <- function(time_series) {
  cbind(
    # Series length
    length = length(time_series),
    # nperiods, seasonal_period, trend, seasonality, linearity, curvature, spikiness e_acr1 and e_acf10
    tsfeatures(time_series, features = "stl_features"),
    # Stability
    tsfeatures(time_series, features = "stability"),
    # Lumpiness
    tsfeatures(time_series, features = "lumpiness"),
    # Hurst
    tsfeatures(time_series, features = "hurst"),
    # Nonlinearity
    tsfeatures(time_series, features = "nonlinearity"),
    # Holt alpha and beta
    tsfeatures(time_series, features = "holt_parameters"),
    # HW alpha and beta, only applies for seasonal time series
    #tsfeatures(time_series, features = "hw_parameters"),
    # Unitroot statistics
    tsfeatures(time_series, features = c("unitroot_kpss", "unitroot_pp")),
    # ACF features
    tsfeatures(time_series, features = "acf_features"),
    # PACF features
    tsfeatures(time_series, features = "pacf_features"),
    # Crossing point
    tsfeatures(time_series, features = "crossing_points"),
    # flat_spots
    tsfeatures(time_series, features = "flat_spots"),
    # arch_lm
    tsfeatures(time_series, features = "arch_stat"),
    # Heterogeneity
    tsfeatures(time_series, features = "heterogeneity")
  )
}

# Test, works perfectly
fforma_features(univariate_time_series)
```

```{r}
### Constituent models, and their corresponding losses over the test set
## Note that we can specify any loss function we want
## Going ahead with mse for now

# Naive
naive_model <- naive(univariate_time_series, h = 12)
mse(as.vector(univariate_time_series_test), predict(naive_model)$mean)

# Random walk with drift
random_walk_model <- rwf(univariate_time_series, h = 12, drift = TRUE)
mse(as.vector(univariate_time_series_test), predict(random_walk_model)$mean)

# Seasonal Naive, this ends up being the same as naive
seasonal_naive_model <- snaive(univariate_time_series, h = 12)
mse(as.vector(univariate_time_series_test), predict(seasonal_naive_model)$mean)

# Theta Method
theta_model <- thetaf(univariate_time_series, h = 12)
mse(as.vector(univariate_time_series_test), predict(theta_model)$mean)

## Auto arima
auto_arima_model <- auto.arima(univariate_time_series)
mse(as.vector(univariate_time_series_test), forecast(auto_arima_model, h = 12)$mean)

# Auto ETS
auto_ets_model <- ets(univariate_time_series)
mse(as.vector(univariate_time_series_test), forecast(auto_ets_model, h = 12)$mean)

# TBATS
tbats_model <- tbats(univariate_time_series)
mse(as.vector(univariate_time_series_test), forecast(tbats_model, h = 12)$mean)

# STLM-AR, does not work because our data is not seasonal
#stlm_ar_model <- stlm(univariate_time_series, modelfunction = "ar")

# Neural Network time series forecasts
nnetar_model <- nnetar(univariate_time_series)
mse(as.vector(univariate_time_series_test), forecast(nnetar_model, h = 12)$mean)

## Other constituent models that may perform well

# Naive model with 0 forecasts
# Neat idea, but practically speaking ends up being the sam eas auto arima's 0, 0, 0 model most of the time
# naive0_model <- rep(0, 12)
# mse(as.vector(univariate_time_series_test), naive0_model)

# ARMA 1, 1 with GARCH 1, 1 gaussian errors
ugarch_spec <- ugarchspec(variance.model = list(model = "sGARCH",
                                                  garchOrder = c(1, 1), 
                                                  submodel = NULL, 
                                                  external.regressors = NULL, 
                                                  variance.targeting = FALSE), 
                            mean.model     = list(armaOrder = c(1, 1), 
                                                  include.mean = TRUE,
                                                  external.regressors = NULL),
                            distribution.model = "norm")

ugarch_spec_fit <- ugarchfit(ugarch_spec, data = univariate_time_series, out.sample = 12)
ugarch_forecast <- ugarchforecast(ugarch_spec_fit, n.ahead = 12, n.roll = 11)
fitted(ugarch_forecast)[1, ]
plot(ugarch_forecast, which = 2)

# ARMA 1, 1 with GARCH 1, 1 student t errors
ugarch_spec_t <- ugarchspec(variance.model = list(model = "sGARCH",
                                                  garchOrder = c(1, 1), 
                                                  submodel = NULL, 
                                                  external.regressors = NULL, 
                                                  variance.targeting = FALSE), 
                            mean.model     = list(armaOrder = c(1, 1), 
                                                  include.mean = TRUE,
                                                  external.regressors = NULL),
                            distribution.model = "std")

ugarch_spec_t_fit <- ugarchfit(ugarch_spec_t, data = univariate_time_series, out.sample = 12)
ugarch_t_forecast <- ugarchforecast(ugarch_spec_t_fit, n.ahead = 12, n.roll = 11)
fitted(ugarch_t_forecast)[1, ]
plot(ugarch_t_forecast, which = 2)

## Stochastic volatility model

stochvol_model <- svlsample(univariate_time_series, draws = 10000, burnin = 1000, designmatrix = "ar1", 
                            priormu = c(0, 100), priorphi = c(5, 1.5), priorsigma = 1,
                            priorrho = c(4, 4), priorbeta = c(0, 10000), thinpara = 1,
                            thinlatent = 1, thintime = NULL, keeptime = "all", quiet = FALSE)

mse(stock_1_test$rt, apply(X = predict(stochvol_model, steps = 12)$y, MARGIN = 2, FUN = mean))

####################
## Getting the data into the right format for use with xgboost and its custom loss function
## FFORMA requires a list with the entries:
# Data - a matrix with the features extracted via tsfeatures, one series per row
# Errors - a matrix of the errors (losses) produced by the forecasting methods, one series per row
# Labels - a numeric vector from 0 to nclass -1, representing the method with the lowest forecast error

### Data matrix
# Just use fforma_features function from before, easy
data <- foreach(i = 1:length(unique(pooled_panel$stock)), .combine = "rbind") %dopar% {
  stock_id <- unique(pooled_panel$stock)

  univariate_time_series <- pooled_panel %>%
    filter(stock == stock_id[i]) %>%
    filter(time %in% timeSlices[[1]]$train | time %in% timeSlices[[1]]$validation) %>%
    select(rt) %>%
    ts()
  
  univariate_time_series_test <- pooled_panel %>%
    filter(stock == stock_id[i]) %>%
    filter(time %in% timeSlices[[1]]$test) %>%
    select(rt) %>%
    ts()
  
  fforma_features(univariate_time_series)
}

errors <- foreach(i = 1:length(unique(pooled_panel$stock)), .combine = "rbind") %dopar% {
  ## Filter Data first
  stock_id <- unique(pooled_panel$stock)

  univariate_time_series <- pooled_panel %>%
    filter(stock == stock_id[i]) %>%
    filter(time %in% timeSlices[[1]]$train | time %in% timeSlices[[1]]$validation) %>%
    select(rt) %>%
    ts()

  univariate_time_series_test <- pooled_panel %>%
    filter(stock == stock_id[i]) %>%
    filter(time %in% timeSlices[[1]]$test) %>%
    select(rt) %>%
    ts()
  
  ## Estimation of consituent models
  
  # Naive
  naive_model <- naive(univariate_time_series, h = 12)
  
  # Random walk with drift
  random_walk_model <- rwf(univariate_time_series, h = 12, drift = TRUE)
  
  # Seasonal Naive, this ends up being the same as naive
  seasonal_naive_model <- snaive(univariate_time_series, h = 12)
  
  # Theta Method
  theta_model <- thetaf(univariate_time_series, h = 12)
  
  ## Auto arima
  auto_arima_model <- auto.arima(univariate_time_series)
  
  # Auto ETS
  auto_ets_model <- ets(univariate_time_series)
  
  # TBATS
  tbats_model <- tbats(univariate_time_series)
  
  # Neural Network time series forecasts
  nnetar_model <- nnetar(univariate_time_series)
  
  # Naive model that forecasts 0
  # Neat idea, but practically speaking ends up being the sam eas auto arima's 0, 0, 0 model most of the time
  # naive0_model <- rep(0, 12)
  # mse(as.vector(univariate_time_series_test), naive0_model)
  
  # ARMA 1, 1 with GARCH 1, 1 gaussian errors
  ugarch_spec <- ugarchspec(variance.model = list(model = "sGARCH",
                                                    garchOrder = c(1, 1), 
                                                    submodel = NULL, 
                                                    external.regressors = NULL, 
                                                    variance.targeting = FALSE), 
                              mean.model     = list(armaOrder = c(1, 1), 
                                                    include.mean = TRUE,
                                                    external.regressors = NULL),
                              distribution.model = "norm")
  
  ugarch_spec_fit <- ugarchfit(ugarch_spec, data = univariate_time_series, out.sample = 12,
                               solver = "hybrid")
  ugarch_forecast <- ugarchforecast(ugarch_spec_fit, n.ahead = 12, n.roll = 11)
  
  ## Stochastic volatility model
  ## Theoretically the most correctly specified model, but very computatioally intensive, and may be infeasible
  ## Takes ~ 14 seconds to estimate
  
  #stochvol_model <- svlsample(univariate_time_series, draws = 10000, burnin = 1000, designmatrix = "ar1", 
                              #priormu = c(0, 100), priorphi = c(5, 1.5), priorsigma = 1,
                              #priorrho = c(4, 4), priorbeta = c(0, 10000), thinpara = 1,
                              #thinlatent = 1, thintime = NULL, keeptime = "all", quiet = TRUE)
  
  ## Put errors into a single row
  errors <- data.frame(naive = mse(as.vector(univariate_time_series_test), predict(naive_model)$mean),
                     
                       random_walk = mse(as.vector(univariate_time_series_test), predict(random_walk_model)$mean), 
                       
                       theta = mse(as.vector(univariate_time_series_test), predict(theta_model)$mean),
                       
                       auto_arima = mse(as.vector(univariate_time_series_test), 
                                        forecast(auto_arima_model, h = 12)$mean),
                       
                       ets = mse(as.vector(univariate_time_series_test), forecast(auto_ets_model, h = 12)$mean),
                       
                       tbats = mse(as.vector(univariate_time_series_test), forecast(tbats_model, h = 12)$mean),
                       
                       nnetar = mse(as.vector(univariate_time_series_test), forecast(nnetar_model, h = 12)$mean),
                       
                       garch_11 = mse(as.vector(univariate_time_series_test), fitted(ugarch_forecast)[1, ]))
                       
                       #stochvol = mse(as.vector(univariate_time_series_test), 
                                      #apply(X = predict(stochvol_model, steps = 12)$y, MARGIN = 2, FUN = mean)))
  
  errors
}

## Create labels
# Look like there's a healthy mix of optimal forecasting methods, yay

labels <- foreach(i = 1:nrow(errors), .combine = "rbind") %dopar% {
  data.frame(label = which.min(errors[i, ]) - 1)
}

## Create final list with everything needed in FFORMA
## Looks pretty good
train_list <- list(
  data = data,
  errors = errors,
  labels = labels
)
```

```{r}
## Training of meta learning using features and errors

## Need to supply custom loss function
## This is because the vanilla softmax loss function does not take into account of the actual errors associated with each forecasting method
# A vanilla softmax xgboost model will simply to try to use the features to identify the best performing model, not accounting for how similarly performing the models may be

softmax_transform <- function(x) {
  exp(x) / sum(exp(x))
}

error_softmax_obj <- function(preds, dtrain) {
  labels <- xgboost::getinfo(dtrain, "label")
  errors <- attr(dtrain, "errors")
  
  for (n in 1:nrow(preds)) {
    preds[n, ] <- softmax_transform(preds[n, ])
  }
  
  rowsumerrors <- rowSums(preds * errors)

  grad <- preds*(errors - rowsumerrors)
  hess <- errors*preds*(1.0 - preds) - grad*preds
  #hess <- grad*(1.0 - 2.0*preds)
  #hess <- pmax(hess, 1e-16)
  #the true hessian should be grad*(1.0 - 2.0*preds) but it produces numerical problems
  #what we use here is a upper bound

  #print(mean(rowSums(preds*errors)))
  return(list(grad = t(grad), hess = t(hess)))
}

error_softmax_obj <- function(preds, dtrain) {
  labels <- xgboost::getinfo(dtrain, "label")
  errors <- attr(dtrain, "errors")

  preds <- exp(preds)
  sp <- rowSums(preds)
  preds <- preds / replicate(ncol(preds), sp)
  rowsumerrors <- replicate(ncol(preds), rowSums(preds * errors))

  grad <- preds*(errors - rowsumerrors)
  hess <- errors*preds*(1.0-preds) - grad*preds
  #hess <- grad*(1.0 - 2.0*preds)
  #hess <- pmax(hess, 1e-16)
  #the true hessian should be grad*(1.0 - 2.0*preds) but it produces numerical problems
  #what we use here is a upper bound

  #print(mean(rowSums(preds*errors)))
  return(list(grad = t(grad), hess = t(hess)))
}

#########################################

param <- list(booster = "gbtree",
              max_depth = 6, eta = 0.3, nthread = 12,
                  num_class = ncol(errors),
              objective = error_softmax_obj,
                  subsample = 1,
                  colsample_bytree = 1)

dtrain <- xgb.DMatrix(data = as.matrix(train_list$data), 
                      label = train_list$labels[, 1])
  
attr(dtrain, "errors") <- train_list$errors
  
bst <- xgboost::xgb.train(param, dtrain, 94,
                          verbose = 1)
bst
pred <- predict(bst, newdata = xgboost::xgb.DMatrix(as.matrix(train_list$data)), outputmargin = TRUE, reshape=TRUE)
pred <- t(apply(pred, 1, softmax_transform))
pred

xgb.importance(model = bst)
xgb.ggplot.importance(bst)
```

```{r}
## Newdata needs to be the feature matrix, one row per series
predict_selection_ensemble <- function(model, newdata) {
  pred <- stats::predict(model, newdata, outputmargin = TRUE, reshape=TRUE)
  pred <- t(apply( pred, 1, softmax_transform))
  pred
}

ensemble_forecast <- function(predictions, dataset, clamp_zero=TRUE) {
  for (i in 1:length(dataset)) {
    weighted_ff <- as.vector(t(predictions[i,]) %*% dataset[[i]]$ff)
    if (clamp_zero) {
      weighted_ff[weighted_ff < 0] <- 0
    }
    dataset[[i]]$y_hat <- weighted_ff
  }
  dataset
}


```


```{r}
## Messing around with TSNE
#######

## Running tsne on each individual time series in the panel just for fun
## Analysis seems to reveal that tsne results are quite sensitive to perplexity parameters
## T-SNE was also originally designed for visualization of high dimensional data, and is not really suitable for use in feature extraction unfortunately

stock_id <- unique(pooled_panel$stock)

stock_series_tsne <- foreach(i = (1:(length(stock_id)/40)), .combine = "rbind") %dopar% {
  stock_series <- pooled_panel %>%
    filter(stock == stock_id[i])
  
  # Apply tsne
  stock_series_tsne <- tsne(stock_series[, -(1:3)], perplexity = 100)
  
  stock_series_tsne <- cbind(stock = stock_id[i], stock_series_tsne)
  
  stock_series_tsne
}

stock_series_tsne <- as.data.frame(stock_series_tsne)

stock_series_tsne %>%
  mutate(V2 = (V2 - mean(V2))/sd(V2)) %>%
  mutate(V3 = (V3 - mean(V3))/sd(V3)) %>%
  mutate(stock = as.factor(stock)) %>%
  ggplot() +
  geom_point(aes(x = V2, y = V3, colour = stock)) +
  theme(legend.position = "none")
```

```{r}
## mgarch models

dcc_spec <- dccspec(uspec = multispec(replicate(2, ugarch_spec)),
                    VAR = FALSE, robust = FALSE, lag = 1, 
                    external.regressors = NULL,
                    dccOrder = c(1, 1), model = c("DCC"),
                    distribution = c("mvt")
)

dcc_filter <- dccfilter(dcc_spec, data = )

```



