---
title: "Simulation Model Results"
author: "Ze Yu Zhong"
date: "21/07/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
################
##Load Libraries
################

library(tidyverse)
library(keras)
library(quantreg)
library(ggplot2)
library(forecast)
library(rlist)
library(plm)
library(Metrics)
library(ranger)
library(caret)
library(forcats)

#Parallel Computing
library(foreach)
library(doFuture)
#Registering
registerDoFuture()
plan(multisession)

set.seed(27935248)
```

```{r}
# Load Datasets

# Set how many realizations of datasets you wish to process
# Going with 30 for now as this is the rule of thumb for a sufficiently large sample

#batch_process_range <- c(1:30)
batch_process_range <- c(1:1)

# Actual Function
# This function takes a dataset and:
# Runs all possible models across mse and mae loss functions, does NOT save the models (for memory efficiency), and saves the loss statistics, actual forecasts, forecast residuals, and variable importance metrics into a list object
# Fairly straightforward, as the model_fit functions from before do everything for you already
# Ie acts as a very big wrapper/for loop
# Note that a lot number of the models are not multithreaded, so a foreach loop will be used so that batches of realizations can be run

fit_all_models <- function(dataset_list, batch_process_range,
                           # Logical arguments specifying which models you want to fit
                           # This is useful if you don't want to fit some of the most intensive methods such as RF and Neural Networks
                           LM, ELN, RF, NNet) {
  # Initialize List
  simulation_results_list <- rep(list(0), length(batch_process_range))
  
  for (batch in (batch_process_range)) {
    
    simulation_results_list[[batch]] <- list(
      # Panel Statistics
      Dataset_stats = 0, 
      # Y Values (used for Diebold Mariano Tests later)
      returns = 0,
      # Models
      LM_MSE = 0, LM_MAE = 0,
      ELN_MSE = 0, ELN_MAE = 0,
      RF_MSE = 0, RF_MAE = 0,
      # Neural Networks
      NN1_MSE = 0, NN1_MAE = 0,
      NN2_MSE = 0, NN2_MAE = 0,
      NN3_MSE = 0, NN3_MAE = 0,
      NN4_MSE = 0, NN4_MAE = 0,
      NN5_MSE = 0, NN5_MAE = 0
    )
    
    # Load Dataset
    pooled_panel <- dataset_list[[batch]]$panel
    simulation_results_list[[batch]]$Dataset_stats <- dataset_list[[batch]]$statistics
    simulation_results_list[[batch]]$returns <- pooled_panel$rt
    
    timeSlices <- customTimeSlices(start = 2, end = 181, initialWindow = 108, horizon = 12, test_size = 36)
    
    f <- panel_formula(pooled_panel)
    
    if (LM == 1) {
      # Linear Models
      simulation_results_list[[batch]]$LM_MSE <- LM_fit(pooled_panel, timeSlices, "mse")
      simulation_results_list[[batch]]$LM_MAE <- LM_fit(pooled_panel, timeSlices, "mae")
    }
    
    if (ELN == 1) {
      # Penalized Linear Models
    
      alpha_grid <- seq(0, 1, 0.01)
      
      simulation_results_list[[batch]]$ELN_MAE <- ELN_fit_stats(alpha_grid, nlamb = 100, timeSlices, pooled_panel, loss_function = "mae")
      simulation_results_list[[batch]]$ELN_MSE <- ELN_fit_stats(alpha_grid, nlamb = 100, timeSlices, pooled_panel, loss_function = "mse")
    }
    
    if (RF == 1) {
      # Random Forests
      RF_grid <- expand.grid(
      #ntree usually isn't tuned. Just set to max of computationally feasible
      ntree = 50,
      mtry = seq(10, round(ncol(pooled_panel[4:ncol(pooled_panel)])/3), 20)
      # nodesize = seq(2, 14, 2)
      # nodedepth recommended not to be changed
      #nodedepth = 1
      )
    
      simulation_results_list[[batch]]$RF_MSE <- RF_fit_stats(f, pooled_panel, RF_grid, timeSlices, "mse")
      simulation_results_list[[batch]]$RF_MAE <- RF_fit_stats(f, pooled_panel, RF_grid, timeSlices, "mae")
    }
      
    if (NNet == 1) {
      # Neural Networks
      # Commented for now because honours lab computers don't have keras/tensorflow
      
      batch_size <- 32
      patience <- 30
      
      simulation_results_list[[batch]]$NN1_MSE <- NNet_fit_stats(pooled_panel, timeSlices, 1, "mse", batch_size = batch_size, patience = patience)
      simulation_results_list[[batch]]$NN1_MAE <- NNet_fit_stats(pooled_panel, timeSlices, 1, "mae", batch_size = batch_size, patience = patience)
    
      simulation_results_list[[batch]]$NN2_MSE <- NNet_fit_stats(pooled_panel, timeSlices, 2, "mse", batch_size = batch_size, patience = patience)
      simulation_results_list[[batch]]$NN2_MAE <- NNet_fit_stats(pooled_panel, timeSlices, 2, "mae", batch_size = batch_size, patience = patience)
    
      simulation_results_list[[batch]]$NN3_MSE <- NNet_fit_stats(pooled_panel, timeSlices, 3, "mse", batch_size = batch_size, patience = patience)
      simulation_results_list[[batch]]$NN3_MAE <- NNet_fit_stats(pooled_panel, timeSlices, 3, "mae", batch_size = batch_size, patience = patience)
    
      simulation_results_list[[batch]]$NN4_MSE <- NNet_fit_stats(pooled_panel, timeSlices, 4, "mse", batch_size = batch_size, patience = patience)
      simulation_results_list[[batch]]$NN4_MAE <- NNet_fit_stats(pooled_panel, timeSlices, 4, "mae", batch_size = batch_size, patience = patience)
    
      simulation_results_list[[batch]]$NN5_MSE <- NNet_fit_stats(pooled_panel, timeSlices, 5, "mse", batch_size = batch_size, patience = patience)
      simulation_results_list[[batch]]$NN5_MAE <- NNet_fit_stats(pooled_panel, timeSlices, 5, "mae", batch_size = batch_size, patience = patience)
    }
  }
  simulation_results_list
}

g1_A1 <- fit_all_models(g1_A1_panel, batch_process_range, LM = 1, ELN = 0, RF = 0, NNet = 0)
g1_A2 <- fit_all_models(g1_A2_panel, batch_process_range, LM = 1, ELN = 0, RF = 0, NNet = 0)
g1_A3 <- fit_all_models(g1_A3_panel, batch_process_range, LM = 1, ELN = 0, RF = 0, NNet = 0)

g2_A1 <- fit_all_models(g2_A1_panel, batch_process_range, LM = 1, ELN = 0, RF = 0, NNet = 0)
g2_A2 <- fit_all_models(g2_A2_panel, batch_process_range, LM = 1, ELN = 0, RF = 0, NNet = 0)
g2_A3 <- fit_all_models(g2_A3_panel, batch_process_range, LM = 1, ELN = 0, RF = 0, NNet = 0)

g3_A1 <- fit_all_models(g3_A1_panel, batch_process_range, LM = 1, ELN = 0, RF = 0, NNet = 1)
g3_A2 <- fit_all_models(g3_A2_panel, batch_process_range, LM = 1, ELN = 0, RF = 0, NNet = 0)
g3_A3 <- fit_all_models(g3_A3_panel, batch_process_range, LM = 1, ELN = 0, RF = 0, NNet = 0)
```

```{r}
# Functions to process the raw loss statistics from earlier and create neat tables

# Functions to visualize variable importance metrics from earlier

combined_importance <- data.frame(
  rbind(data.frame(cbind(g1_A1[[1]]$LM_MSE[[1]]$variable_importance, model = "LM_MSE")),
        data.frame(cbind(g1_A1[[1]]$LM_MSE[[1]]$variable_importance, model = "LM_MAE")),
        # Elastic Net
        #data.frame(cbind(g1_A1[[1]]$LM_MSE[[1]]$variable_importance, model = "ELN_MAE")),
        #data.frame(cbind(g1_A1[[1]]$LM_MSE[[1]]$variable_importance, model = "ELN_MSE")),
        # Neural Networks
        data.frame(cbind(g1_A1[[1]]$NN1_MSE[[1]]$variable_importance, model = "NN1_MSE")),
        data.frame(cbind(g1_A1[[1]]$NN1_MAE[[1]]$variable_importance, model = "NN1_MAE")),
        data.frame(cbind(g1_A1[[1]]$NN2_MSE[[1]]$variable_importance, model = "NN2_MSE")),
        data.frame(cbind(g1_A1[[1]]$NN2_MAE[[1]]$variable_importance, model = "NN2_MAE")),
        data.frame(cbind(g1_A1[[1]]$NN3_MSE[[1]]$variable_importance, model = "NN3_MSE")),
        data.frame(cbind(g1_A1[[1]]$NN3_MAE[[1]]$variable_importance, model = "NN3_MAE")),
        data.frame(cbind(g1_A1[[1]]$NN4_MSE[[1]]$variable_importance, model = "NN4_MSE")),
        data.frame(cbind(g1_A1[[1]]$NN4_MAE[[1]]$variable_importance, model = "NN4_MAE")),
        data.frame(cbind(g1_A1[[1]]$NN5_MSE[[1]]$variable_importance, model = "NN5_MSE")),
        data.frame(cbind(g1_A1[[1]]$NN5_MAE[[1]]$variable_importance, model = "NN5_MAE"))
)) %>%
  # Calculates the relative importance of each factor within each model
  group_by(model) %>%
  mutate(importance = importance/sum(importance))

ggplot(combined_importance %>% filter(importance > 0.04) %>% mutate(variable = fct_reorder(variable, importance))) +
  geom_tile(aes(x = model, y = variable, alpha = importance))

combined_loss_stats <- data.frame(
  rbind(data.frame(cbind(g1_A1[[1]]$LM_MSE[[1]]$loss_stats, model = "LM_MSE")),
        data.frame(cbind(g1_A1[[1]]$LM_MAE[[1]]$loss_stats, model = "LM_MAE")),
        # Neural Networks
        data.frame(cbind(g1_A1[[1]]$NN1_MSE[[1]]$loss_stats, model = "NN1_MSE")),
        data.frame(cbind(g1_A1[[1]]$NN1_MAE[[1]]$loss_stats, model = "NN1_MAE")),
        data.frame(cbind(g1_A1[[1]]$NN2_MSE[[1]]$loss_stats, model = "NN2_MSE")),
        data.frame(cbind(g1_A1[[1]]$NN2_MAE[[1]]$loss_stats, model = "NN2_MAE")),
        data.frame(cbind(g1_A1[[1]]$NN3_MSE[[1]]$loss_stats, model = "NN3_MSE")),
        data.frame(cbind(g1_A1[[1]]$NN3_MAE[[1]]$loss_stats, model = "NN3_MAE")),
        data.frame(cbind(g1_A1[[1]]$NN4_MSE[[1]]$loss_stats, model = "NN4_MSE")),
        data.frame(cbind(g1_A1[[1]]$NN4_MAE[[1]]$loss_stats, model = "NN4_MAE")),
        data.frame(cbind(g1_A1[[1]]$NN5_MSE[[1]]$loss_stats, model = "NN5_MSE")),
        data.frame(cbind(g1_A1[[1]]$NN5_MAE[[1]]$loss_stats, model = "NN5_MAE"))
  )
)

combined_loss_stats

# Functions to do Diebold Mariano Tests using forecast residuals from earlier


```

```{r}
g1_A1[[1]]$LM_MSE[[1]]$loss_stats
g1_A2[[1]]$LM_MSE[[1]]$loss_stats
g1_A3[[1]]$LM_MSE[[1]]$loss_stats

g2_A1[[1]]$LM_MSE[[1]]$loss_stats
g2_A2[[1]]$LM_MSE[[1]]$loss_stats
g2_A3[[1]]$LM_MSE[[1]]$loss_stats

g3_A1[[1]]$LM_MSE[[1]]$loss_stats
g3_A2[[1]]$LM_MSE[[1]]$loss_stats
g3_A3[[1]]$LM_MSE[[1]]$loss_stats
```

