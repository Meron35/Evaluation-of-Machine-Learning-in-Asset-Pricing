---
title: "Simulation Models"
author: "Ze Yu Zhong"
date: "19 June 2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

################
##Load Libraries
################

library(tidyverse)
library(keras)
library(ggplot2)
library(forecast)
library(rlist)
library(Metrics)
library(tensorflow)
library(quantreg)
library(randomForestSRC)
library(hqreg)
library(caret)

#Parallel Computing
library(foreach)
library(doFuture)
#Registering
registerDoFuture()
plan(multisession)
```

```{r train_valid_test}
#Create Training + Test Sets

#$T = 180$ monthly periods corresponds to 15 years. The training sample was set to start from $T = 108$ or 9 years, a validation set 1 year in length. The last 3 years were reserved as a test set never to be used for validation or training.

#This means that there are only 3 sample periods to train on, yay

#Caret and other packages are not so good at dealing with custom training/validation/test sets. Have to specify resampling, tuning and testing manually (ugh)

customTimeSlices <- function(start, end, initialWindow, horizon, test_size) {
  total <- c(start:end)
  offset <- start - 1
  set_num <- (end - start + offset - test_size - initialWindow) / horizon
  
  time_slice <- list(train = 0, validation = 0, test = 0)
  time_slices <- rep(list(time_slice), 3)

  for (t in 1:set_num) {
    time_slice$train <- c(start:(initialWindow + (t-1) * horizon + offset))
    time_slice$validation <- c((initialWindow + (t-1) * horizon + offset + 1):(initialWindow + (t) * horizon + offset))
    time_slice$test <- c((initialWindow + (t) * horizon + offset + 1):end)
    
    time_slices[[t]] <- time_slice
  }
  return(time_slices)
}

#Create custom time slices
timeSlices <- customTimeSlices(start = 2, end = 181, initialWindow = 108, horizon = 12, test_size = 36)

#Formula Function, makes it easier for those packages with a formula interface

panel_formula <- function(panel){
  #Remove the first 3 colNames, as these correspond to the return, time and stock id
  panel_colnames <- colnames(panel)[-c(1:3)]
  f <- as.formula(c("rt ~", paste(panel_colnames, collapse = "+")))
  return(f)
}

# Change data specifcation here

pooled_panel <- g1_A1_panel[[1]]$panel
pooled_panel <- g1_A2_panel[[1]]$panel

pooled_panel <- g2_A1_panel[[1]]$panel

pooled_panel <- g3_A1_panel[[1]]$panel

pooled_panel <- gu_et_al_g1[[1]]$panel

pooled_panel <- gu_et_al_g2[[1]]$panel

f <- panel_formula(pooled_panel)
```

```{r variable_importance_functions, eval = FALSE}
# Variable Importance Metric Functions
# Given a single model and test set, return a variable importance dataframe (don't bother sorting them now)
# Note that a training set is NOT needed because this is to be called AFTER the model is fit
# These functions were originally written after the actul fitting functions were
# These are used by the fitting functions, so they need to be declared before them

# Similarly, different functions are needed for all model types because all the model objects have slightly different predict methods

################################################
# Linear Model
LM_variable_importance <- function(test, timeSlices, lm_model) {
  test_x <- test[4:ncol(test)]
  
  test_x_zero <- test_x
  test_x_zero[1] <- 0
    
  original_R2 <- R2(predict(lm_model, newdata = test_x), test$rt, form = "traditional")
    
  new_R2 <- R2(predict(lm_model, newdata = test_x_zero), test$rt, form = "traditional")
      
  variable_importance <- data.frame(variable = colnames(test_x)[1], importance = (original_R2 - new_R2))
    
  for (i in 2:ncol(test_x)) {
    test_x_zero <- test_x
    test_x_zero[i] <- 0
      
    original_R2 <- R2(predict(lm_model, newdata = test_x), test$rt, form = "traditional")
    
    new_R2 <- R2(predict(lm_model, newdata = test_x_zero), test$rt, form = "traditional")
      
    variable_importance <- rbind(variable_importance, 
                                 data.frame(variable = colnames(test_x)[i], 
                                            importance = (original_R2 - new_R2)
                                            )
                                 )
      
  }
  return(variable_importance)
}

# Penalized Linear Model

ELN_variable_importance <- function(test, timeSlices, eln_model, alpha, lambda) {
  test_x <- as.matrix(test[4:ncol(test)])
  test_x_zero <- test_x
  test_x_zero[, 1] <- 0
  
  test_x <- as.matrix(test_x)
  text_x_zero <- as.matrix(test_x_zero)
      
  original_R2 <- R2(predict(eln_model, test_x, alpha = alpha, lambda = lambda), test$rt, form = "traditional")
    
  new_R2 <- R2(predict(eln_model, test_x_zero, alpha = alpha, lambda = lambda), test$rt, form = "traditional")
    
  variable_importance <- data.frame(variable = colnames(test_x)[1], importance = (original_R2 - new_R2))
    
  for (i in 2:ncol(test_x)) {
    test_x_zero <- test_x
    test_x_zero[, i] <- 0
      
    original_R2 <- R2(predict(eln_model, test_x, alpha = alpha, lambda = lambda), test$rt, form = "traditional")
    
    new_R2 <- R2(predict(eln_model, test_x_zero, alpha = alpha, lambda = lambda), test$rt, form = "traditional")
      
    variable_importance <- rbind(variable_importance, 
                                 data.frame(variable = colnames(test_x)[i], 
                                            importance = (original_R2 - new_R2))
                                 )
  }
  return(variable_importance)
}

# Random Forest

RF_variable_importance <- function(test, timeSlices, rf_model) {
  test_x <- test[4:ncol(test)]
    
  test_x_zero <- test_x
  test_x_zero[1] <- 0
    
  original_R2 <- R2(predict(rf_model, newdata = test_x)$predicted, test$rt, form = "traditional")
    
  new_R2 <- R2(predict(rf_model, newdata = test_x_zero)$predicted, test$rt, form = "traditional")
      
  variable_importance <- data.frame(variable = colnames(test_x)[1], importance = (original_R2 - new_R2))
    
  for (i in 2:ncol(test_x)) {
    test_x_zero <- test_x
    test_x_zero[i] <- 0
      
    original_R2 <- R2(predict(rf_model, newdata = test_x)$predicted, test$rt, form = "traditional")
    
    new_R2 <- R2(predict(rf_model, newdata = test_x_zero)$predicted, test$rt, form = "traditional")
      
    variable_importance <- rbind(variable_importance, 
                                 data.frame(variable = colnames(test_x)[i], 
                                            importance = (original_R2 - new_R2)
                                )
    )
  }
  return(variable_importance)
}

# Neural Network

NNet_variable_importance <- function(test, timeSlices, nnet_model) {
  test_x <- as.matrix(test[4:ncol(test)])
  test_x_zero <- test_x
  test_x_zero[, 1] <- 0
  
  test_x <- as.matrix(test_x)
  text_x_zero <- as.matrix(test_x_zero)
    
  original_R2 <- R2(predict(nnet_model, as.matrix(test_x)), test$rt, form = "traditional")
    
  new_R2 <- R2(predict(nnet_model, as.matrix(test_x_zero)), test$rt, form = "traditional")
      
  variable_importance <- data.frame(variable = colnames(test_x)[1], importance = (original_R2 - new_R2))
    
  for (i in 2:ncol(test_x)) {
    test_x_zero <- test_x
    test_x_zero[, i] <- 0
      
    original_R2 <- R2(predict(nnet_model, as.matrix(test_x)), test$rt, form = "traditional")
    
    new_R2 <- R2(predict(nnet_model, as.matrix(test_x_zero)), test$rt, form = "traditional")
      
    variable_importance <- rbind(variable_importance, 
                                 data.frame(variable = colnames(test_x)[i], 
                                            importance = (original_R2 - new_R2)
                                            )
                                 )
      
  }
  return(variable_importance)
}
```

```{r variable_importance_functions_foreach}
# Foreach parallel implementation of previous section
# There are a LOT of predictors to inerate over and often this is the bottleneck for the model fitting procedure
# Therefore, use these parallel implementations if possible
# Also, the code is noticeably neater

# Variable Importance Metric Functions
# Given a single model and test set, return a variable importance dataframe (don't bother sorting them now)
# Note that a training set is NOT needed because this is to be called AFTER the model is fit
# These functions were originally written after the actul fitting functions were
# These are used by the fitting functions, so they need to be declared before them

# Similarly, different functions are needed for all model types because all the model objects have slightly different predict methods

################################################
# Linear Model
LM_variable_importance <- function(test, lm_model) {
  test_x <- test[4:ncol(test)]
  
  # Specify .packages = "quantreg" here as it seems it isn't supported and therefore is missed by doFuture
  variable_importance_df <- foreach(i = (1:ncol(test_x)), .combine = "rbind", .packages = "quantreg") %dopar% {
    test_x_zero <- test_x
    test_x_zero[, i] <- 0
    
    original_R2 <- R2(predict(lm_model, newdata = test_x), test$rt, form = "traditional")
    
    new_R2 <- R2(predict(lm_model, newdata = test_x_zero), test$rt, form = "traditional")
    
    variable_importance <- data.frame(variable = colnames(test_x)[i], importance = (original_R2 - new_R2))
    
    variable_importance
  }
  variable_importance_df
}

# Penalized Linear Model

ELN_variable_importance <- function(test, eln_model, alpha, lambda) {
  test_x <- as.matrix(test[4:ncol(test)])
  
  variable_importance_df <- foreach(i = (1:ncol(test_x)), .combine = "rbind") %dopar% {
    test_x_zero <- test_x
    test_x_zero[, i] <- 0
    
    original_R2 <- R2(predict(eln_model, test_x, alpha = alpha, lambda = lambda), test$rt, form = "traditional")
    
    new_R2 <- R2(predict(eln_model, test_x_zero, alpha = alpha, lambda = lambda), test$rt, form = "traditional")
    
    variable_importance <- data.frame(variable = colnames(test_x)[i], importance = (original_R2 - new_R2))
    
    variable_importance
  }
  variable_importance_df
}

# Random Forest

RF_variable_importance <- function(test, rf_model) {
  test_x <- test[4:ncol(test)]
  
  variable_importance_df <- foreach(i = (1:ncol(test_x)), .combine = "rbind") %dopar% {
    test_x_zero <- test_x
    test_x_zero[, i] <- 0
    
    original_R2 <- R2(predict(rf_model, newdata = test_x)$predicted, test$rt, form = "traditional")
    
    new_R2 <- R2(predict(rf_model, newdata = test_x_zero)$predicted, test$rt, form = "traditional")
    
    variable_importance <- data.frame(variable = colnames(test_x)[i], importance = (original_R2 - new_R2))
                                      
    variable_importance
  }
variable_importance_df
}

# Neural Network

NNet_variable_importance <- function(test, nnet_model) {
  test_x <- as.matrix(test[4:ncol(test)])
  test_x_zero <- test_x
  test_x_zero[, 1] <- 0
  
  test_x <- as.matrix(test_x)
  text_x_zero <- as.matrix(test_x_zero)
    
  original_R2 <- R2(predict(nnet_model, as.matrix(test_x)), test$rt, form = "traditional")
    
  new_R2 <- R2(predict(nnet_model, as.matrix(test_x_zero)), test$rt, form = "traditional")
      
  variable_importance <- data.frame(variable = colnames(test_x)[1], importance = (original_R2 - new_R2))
    
  for (i in 2:ncol(test_x)) {
    test_x_zero <- test_x
    test_x_zero[, i] <- 0
      
    original_R2 <- R2(predict(nnet_model, as.matrix(test_x)), test$rt, form = "traditional")
    
    new_R2 <- R2(predict(nnet_model, as.matrix(test_x_zero)), test$rt, form = "traditional")
      
    variable_importance <- rbind(variable_importance, 
                                 data.frame(variable = colnames(test_x)[i], 
                                            importance = (original_R2 - new_R2)
                                            )
                                 )
      
  }
  return(variable_importance)
}
```

```{r pooled_ols}
#Linear model wrt MSE

LM_fit <- function(pooled_panel, timeSlices, loss_function) {
  #Initialize Loss Function Statistics
  LM_stats <- rep(list(0), 3)
  
  for (set in 1:3) {
    LM_stats[[set]] <- list(loss_stats = data.frame(train_MAE = 0, train_MSE = 0, train_RMSE = 0, train_RSquare = 0, 
                                                    validation_MAE = 0, validation_MSE = 0, validation_RMSE = 0, validation_RSquare = 0, 
                                                    test_MAE = 0, test_MSE = 0, test_RMSE = 0, test_RSquare = 0),
                            #Other useful things
                            forecasts = 0,
                            forecast_resids = 0,
                            model = 0,
                            #Variable Importance
                            variable_importance = 0)
    
    #Load Training, validation and test sets
    train <- pooled_panel %>%
      filter(time %in% timeSlices[[set]]$train)
    validation <- pooled_panel %>%
      filter(time %in% timeSlices[[set]]$validation)
    test <- pooled_panel %>%
      filter(time %in% timeSlices[[set]]$test)
    
    #Train Model on training set
    
    #MSE case
    if (loss_function == "mse") {
      lm <- lm(f, data = train)
    } else {
      # Use pfn as method here for much faster computation
      lm <- rq(f, data = train, tau = 0.5, method = "pfn")
    }
    
    LM_stats[[set]]$model <- lm
    
    #No Tuning Needed
    
    #Statistics
    #Training Set
    train_predict <- predict(lm, newdata = train)
    LM_stats[[set]]$loss_stats$train_MAE <- mae(train$rt, train_predict)
    LM_stats[[set]]$loss_stats$train_MSE <- mse(train$rt, train_predict)
    LM_stats[[set]]$loss_stats$train_RMSE <- rmse(train$rt, train_predict)
    LM_stats[[set]]$loss_stats$train_RSquare <- (1 - sse(train$rt, train_predict) / sum((train$rt - mean(train$rt))^2))
    
    #Validation Set Statistics
    validation_predict <- predict(lm, newdata = validation)
    LM_stats[[set]]$loss_stats$validation_MAE <- mae(validation$rt, validation_predict)
    LM_stats[[set]]$loss_stats$validation_MSE <- mse(validation$rt, validation_predict)
    LM_stats[[set]]$loss_stats$validation_RMSE <- rmse(validation$rt, validation_predict)
    LM_stats[[set]]$loss_stats$validation_RSquare <- (1 - sse(validation$rt, validation_predict) / sum((validation$rt - mean(validation$rt))^2))
      
    #Test Set Statistics
    test_predict <- predict(lm, newdata = test)
    LM_stats[[set]]$loss_stats$test_MAE <- mae(test$rt, test_predict)
    LM_stats[[set]]$loss_stats$test_MSE <- mse(test$rt, test_predict)
    LM_stats[[set]]$loss_stats$test_RMSE <- rmse(test$rt, test_predict)
    LM_stats[[set]]$loss_stats$test_RSquare <- (1 - sse(test$rt, test_predict) / sum((test$rt - mean(test$rt))^2))
        
    #Forecasts
    LM_stats[[set]]$forecasts <- test_predict
    #Forecast Residuals
    LM_stats[[set]]$forecast_resids <- test$rt - test_predict
    #Variable Importance
    LM_stats[[set]]$variable_importance <- LM_variable_importance(test, lm)
  }
  return(LM_stats)
}

LM_stats_mse <- LM_fit(pooled_panel, timeSlices, "mse")

LM_stats_mae <- LM_fit(pooled_panel, timeSlices, "mae")

LM_stats_mse[[1]]$loss_stats
LM_stats_mse[[2]]$loss_stats
LM_stats_mse[[3]]$loss_stats

LM_stats_mse[[1]]$variable_importance %>% arrange(desc(importance))
LM_stats_mse[[2]]$variable_importance %>% arrange(desc(importance))
LM_stats_mse[[3]]$variable_importance %>% arrange(desc(importance))

summary(LM_stats_mse[[1]]$model)

LM_stats_mae[[1]]$loss_stats
LM_stats_mae[[2]]$loss_stats
LM_stats_mae[[3]]$loss_stats

LM_stats_mae[[1]]$variable_importance %>% arrange(desc(importance))
LM_stats_mae[[2]]$variable_importance %>% arrange(desc(importance))
LM_stats_mae[[3]]$variable_importance %>% arrange(desc(importance))

summary(LM_stats_mae[[1]]$model)

# Observations

```

```{r elastic_net, eval = FALSE}
## DO NOT RUN THIS CHUNK. ITS IMPLEMENTATION IS VERY INTENSIVE AND SLOW. NOTE EVAL = FALSE FLAG
# KEPT HERE MAINLY FOR LEGACY PURPOSES

#Elasticnet WRT MSE

# Can't seem to find anything online that tunes ELN for you with specified validation sets, instead of straight-forward cross validation
# nfold and foldid require that ELN be performed with at least 3 fold validation - not possible in our context

# Elasticnet only has alpha and lambda to tune

# ELN is rather finicky and likes to generate its own sequence of lambda values during the actual fitting process. Really doesn't like specifying a singular lambda value
# Solution: follow what the documentation says and let it find its own lambda sequence
# Then, when supply custom grid values of lambda when actually predicting values. This can be either through exact = TRUE (refits the model with the additional lambda gridpoint), or exact = FALSE, which just uses linear interpolation
# According to documentation, linear interpolation is very fast and is generally "good enough" anyway

# Just keep the grid small for now to make sure everything is running
ELN_grid <- expand.grid(
  alpha = seq(0, 1, 0.01),
  lambda = 10^seq(-2, 2, length.out = 50)
)

######################################
## Functions to tune an ELN model
######################################

# Fit the model over the defined grid of hyperparameters, and return it as a list
# Takes a defined grid of hyperparameters, a training set, and a validation set, fits the model over the grid, and returns the models + loss statistics as a list
# Works with both loss functions of MSE and MAE, need to specify these as "mse" or "mae" respectively

# Ideally, we would store all the model fitted over the grid. This takes up way too much memory, and because ELN is very efficiently fit anyway it's better not to store them. Relevant lines are commented out

ELN_model_grid <- function(ELN_grid, train_x, train_y, validation_x, validation_y, loss_function, nlambda) {
  ELN_model_grid <- rep(list(0), nrow(ELN_grid))
  
  for (i in 1:nrow(ELN_grid)) {
    
    ELN_model_grid[[i]] <- list(ELN_grid = 0, model = 0)
    
    # MSE Case
    if (loss_function == "mse") {
      ELN <- hqreg_raw(train_x, train_y, method = "ls", alpha = ELN_grid$alpha[i], nlambda = nlambda)
      
      #ELN_model_grid[[i]]$model <- ELN
      
      ELN_model_grid[[i]]$ELN_grid <- cbind(alpha = ELN_grid$alpha[i], lambda = ELN_grid$lambda[i],
                                            train_loss = mse(train_y, predict(ELN, train_x, lambda = ELN_grid$lambda[i])),
                                            validation_loss = mse(validation_y, predict(ELN, validation_x, lambda = ELN_grid$lambda[i]))
                                            )
    } else {
      # MAE Case
      ELN <- hqreg_raw(train_x, train_y, method = "quantile", tau = 0.5, alpha = ELN_grid$alpha[i], nlambda = nlambda)
      #ELN_model_grid[[i]]$model <- ELN

      ELN_model_grid[[i]]$ELN_grid <- cbind(alpha = ELN_grid$alpha[i], lambda = ELN_grid$lambda[i],
                                            train_loss = mae(train_y, predict(ELN, train_x, lambda = ELN_grid$lambda[i])),
                                            validation_loss = mae(validation_y, predict(ELN, validation_x, lambda = ELN_grid$lambda[i]))
                                            )
    }
  }
  return(ELN_model_grid)
}

get_ELN_best_tune <- function(model_grid) {
  ELN_tune_dataframe <- cbind(model_grid[[1]]$ELN_grid, list_index = 1)
  for (i in 2:length(model_grid)) {
    ELN_tune_dataframe <- rbind(ELN_tune_dataframe, cbind(model_grid[[i]]$ELN_grid, list_index = i))
  }
  ELN_tune_dataframe <- data.frame(ELN_tune_dataframe)
  return(ELN_tune_dataframe[which.min(ELN_tune_dataframe$validation_loss), ])
}

# Final big wrapper function that gives an ELN_stats list with everything desired

ELN_fit_stats <- function(ELN_grid, nlambda, timeSlices, pooled_panel, loss_function) {
  ELN_stats <- rep(list(0), 3)
  
  for (set in 1:3) {
    #Load Training, validation and test sets
    train <- pooled_panel %>%
      filter(time %in% timeSlices[[set]]$train)
    validation <- pooled_panel %>%
      filter(time %in% timeSlices[[set]]$validation)
    test <- pooled_panel %>%
      filter(time %in% timeSlices[[set]]$test)
    
    train_x <- as.matrix(train[4:ncol(train)])
    train_y <- as.matrix(train$rt)
    validation_x <- as.matrix(validation[4:ncol(validation)])
    validation_y <- as.matrix(validation$rt)
    test_x <- as.matrix(test[4:ncol(test)])
    test_y <- as.matrix(test$rt)
      
    #Get models fit over the grid of hyperparameters
    ELN_model_grid <- ELN_model_grid(ELN_grid, train_x, train_y, validation_x, validation_y, loss_function, nlambda = nlambda)
        
    #Get the best tuning parameters
    best_model_params <- get_ELN_best_tune(ELN_model_grid)
    
    #Initialize stats list
    ELN_stats[[set]] <- list(loss_stats = data.frame(train_MAE = 0, train_MSE = 0, train_RMSE = 0, train_RSquare = 0, 
                                                     validation_MAE = 0, validation_MSE = 0, validation_RMSE = 0, validation_RSquare = 0, 
                                                     test_MAE = 0, test_MSE = 0, test_RMSE = 0, test_RSquare = 0),
                             #Other useful things
                             forecasts = 0,
                             forecast_resids = 0,
                             model = 0)
      
    #Model
    if (loss_function == "mse") {
      model <- hqreg_raw(train_x, train_y, method = "ls", alpha = best_model_params$alpha, nlambda = nlambda)
    } else {
      model <- hqreg_raw(train_x, train_y, method = "quantile", tau = 0.5, alpha = best_model_params$alpha, nlambda = nlambda)
    }
    ELN_stats[[set]]$model <- model
      
    #Loss Stats Dataframe
    #Train
    train_predict <- predict(model, train_x, alpha = best_model_params$alpha, lambda = best_model_params$lambda)
    ELN_stats[[set]]$loss_stats$train_MAE <- mae(train_y, train_predict)
    ELN_stats[[set]]$loss_stats$train_MSE <- mse(train_y, train_predict)
    ELN_stats[[set]]$loss_stats$train_RMSE <- rmse(train_y, train_predict)
    ELN_stats[[set]]$loss_stats$train_RSquare <- (1 - sse(train_y, train_predict) / sum((train_y - mean(train_y))^2))
        
    #Validation
    valid_predict <- predict(model, validation_x, alpha = best_model_params$alpha, lambda = best_model_params$lambda)
    ELN_stats[[set]]$loss_stats$validation_MAE <- mae(validation_y, valid_predict)
    ELN_stats[[set]]$loss_stats$validation_MSE <- mse(validation_y, valid_predict)
    ELN_stats[[set]]$loss_stats$validation_RMSE <- rmse(validation_y, valid_predict)
    ELN_stats[[set]]$loss_stats$validation_RSquare <- (1 - sse(validation_y, valid_predict) / sum((validation_y - mean(validation_y))^2))
    
    #Test
    test_predict <- predict(model, test_x, alpha = best_model_params$alpha, lambda = best_model_params$lambda)
    ELN_stats[[set]]$loss_stats$test_MAE <- mae(test_y, test_predict)
    ELN_stats[[set]]$loss_stats$test_MSE <- mse(test_y, test_predict)
    ELN_stats[[set]]$loss_stats$test_RMSE <- rmse(test_y, test_predict)
    ELN_stats[[set]]$loss_stats$test_RSquare <- (1 - sse(test_y, test_predict) / sum((test_y - mean(test_y))^2))
      
    #Forecasts
    ELN_stats[[set]]$forecasts <- test_predict
    
    #Forecast residuals
    ELN_stats[[set]]$forecast_resids <- test_y - test_predict
  }
  return(ELN_stats)
}

#Testing if function works

ELN_stats_mae <- ELN_fit_stats(ELN_grid, nlambda = 100, timeSlices, pooled_panel, loss_function = "mae")

ELN_stats_mse <- ELN_fit_stats(ELN_grid, nlambda = 100, timeSlices, pooled_panel, loss_function = "mse")

ELN_stats_mae[[1]]$model$alpha
ELN_stats_mae[[2]]$model$alpha
ELN_stats_mae[[3]]$model$alpha

ELN_stats_mse[[1]]$model$alpha
ELN_stats_mse[[2]]$model$alpha
ELN_stats_mse[[3]]$model$alpha

# Optimal alpha vlaues seem to be very unstable across folds
```

```{r elastic_net}
# Alternative implementation for ELN
# This is much, much faster
# DO NOT USE THE PREVIOUS IMPLEMENTATION
# Do not specify a grid for lambda. Instead, let the algorithm determine its own sequence of lambda
# Method: specify only a grid for alpha. For each grid value of alpha, fit an ELN model and let LARS determine a suitable path of lambdas. Produce validation error statistics for each alpha and lambda value. Use the best combination for the final model
# 

alpha_grid <- seq(0, 1, 0.01)

ELN_model_grid <- function(alpha_grid, train_x, train_y, validation_x, validation_y, loss_function, nlamb) {
  #Initialize List
  ELN_model_grid <- rep(list(0), length(alpha_grid))
  
  for (i in 1:(length(alpha_grid))) {
    
    ELN_model_grid[[i]] <- list(ELN_grid = 0, model = 0)
    
    #MSE Case
    if (loss_function == "mse") {
      ELN <- hqreg_raw(train_x, train_y, method = "ls", alpha = alpha_grid[i], nlambda = nlamb)
      ELN_model_grid[[i]]$model <- ELN
    
      ELN_model_grid[[i]]$ELN_grid <- cbind(alpha = alpha_grid[i], lambda = ELN$lambda,
                                            train_loss = apply(predict(ELN, train_x), 2, mse, actual = train_y), 
                                            validation_loss = apply(predict(ELN, validation_x), 2, mse, actual = validation_y))
    } else {
      #MAE Case
      ELN <- hqreg_raw(train_x, train_y, method = "quantile", tau = 0.5, alpha = alpha_grid[i], nlambda = nlamb)
      ELN_model_grid[[i]]$model <- ELN
    
      ELN_model_grid[[i]]$ELN_grid <- cbind(alpha = alpha_grid[i], lambda = ELN$lambda,
                                            train_loss = apply(predict(ELN, train_x), 2, mse, actual = train_y), 
                                            validation_loss = apply(predict(ELN, validation_x), 2, mae, actual = validation_y))
    }
  }
  return(ELN_model_grid)
  
}

# Foreach implementation of above function
# This is much much faster

ELN_model_grid <- function(alpha_grid, train_x, train_y, validation_x, validation_y, loss_function, nlamb) {
  
  ELN_model_grid_list <- foreach(i = (1:length(alpha_grid))) %dopar% {
    ELN_model_grid <- list(ELN_grid = 0, model = 0)
    
    #MSE Case
    if (loss_function == "mse") {
      ELN <- hqreg_raw(train_x, train_y, method = "ls", alpha = alpha_grid[i], nlambda = nlamb)
      ELN_model_grid$model <- ELN
    
      ELN_model_grid$ELN_grid <- cbind(alpha = alpha_grid[i], lambda = ELN$lambda,
                                       train_loss = apply(predict(ELN, train_x), 2, mse, actual = train_y), 
                                       validation_loss = apply(predict(ELN, validation_x), 2, mse, actual = validation_y))
    } else {
      #MAE Case
      ELN <- hqreg_raw(train_x, train_y, method = "quantile", tau = 0.5, alpha = alpha_grid[i], nlambda = nlamb)
      ELN_model_grid$model <- ELN
    
      ELN_model_grid$ELN_grid <- cbind(alpha = alpha_grid[i], lambda = ELN$lambda,
                                       train_loss = apply(predict(ELN, train_x), 2, mse, actual = train_y), 
                                       validation_loss = apply(predict(ELN, validation_x), 2, mae, actual = validation_y))
    }
    ELN_model_grid
  }
  ELN_model_grid_list
}

# Given a list containing the model grid, return the "best" model according to validation loss
# Use in conjunction with previous function

get_ELN_best_tune <- function(model_grid) {
  ELN_tune_dataframe <- cbind(model_grid[[1]]$ELN_grid, list_index = 1)
  for (i in 2:length(model_grid)) {
    ELN_tune_dataframe <- rbind(ELN_tune_dataframe, cbind(model_grid[[i]]$ELN_grid, list_index = i))
  }
  ELN_tune_dataframe <- data.frame(ELN_tune_dataframe)
  return(ELN_tune_dataframe[which.min(ELN_tune_dataframe$validation_loss), ])
}

# Final big wrapper function that gives an ELN_stats list with everything desired

ELN_fit_stats <- function(alpha_grid, nlamb, timeSlices, pooled_panel, loss_function) {
  ELN_stats <- rep(list(0), 3)
  
  for (set in 1:3) {
    #Load Training, validation and test sets
    train <- pooled_panel %>%
      filter(time %in% timeSlices[[set]]$train)
    validation <- pooled_panel %>%
      filter(time %in% timeSlices[[set]]$validation)
    test <- pooled_panel %>%
      filter(time %in% timeSlices[[set]]$test)
    
    train_x <- as.matrix(train[4:ncol(train)])
    train_y <- as.matrix(train$rt)
    validation_x <- as.matrix(validation[4:ncol(validation)])
    validation_y <- as.matrix(validation$rt)
    test_x <- as.matrix(test[4:ncol(test)])
    test_y <- as.matrix(test$rt)
      
    #Get models fit over the grid of hyperparameters
    ELN_model_grid <- ELN_model_grid(alpha_grid, train_x, train_y, validation_x, validation_y, loss_function, nlamb = nlamb)
        
    #Get the best tuning parameters
    best_model_params <- get_ELN_best_tune(ELN_model_grid)
    
    #Initialize stats list
    ELN_stats[[set]] <- list(loss_stats = data.frame(train_MAE = 0, train_MSE = 0, train_RMSE = 0, train_RSquare = 0, 
                                                     validation_MAE = 0, validation_MSE = 0, validation_RMSE = 0, validation_RSquare = 0, 
                                                     test_MAE = 0, test_MSE = 0, test_RMSE = 0, test_RSquare = 0),
                             #Other useful things
                             forecasts = 0,
                             forecast_resids = 0,
                             model = 0,
                             hyperparameters = 0,
                             variable_importance = 0)
      
    #Model
    model <- ELN_model_grid[[best_model_params$list_index]]$model
    ELN_stats[[set]]$model <- model
    
    #Hyperparameters
    ELN_stats[[set]]$hyperparameters <- best_model_params
      
    #Loss Stats Dataframe
    #Train
    train_predict <- predict(model, train_x, alpha = best_model_params$alpha, lambda = best_model_params$lambda)
    ELN_stats[[set]]$loss_stats$train_MAE <- mae(train_y, train_predict)
    ELN_stats[[set]]$loss_stats$train_MSE <- mse(train_y, train_predict)
    ELN_stats[[set]]$loss_stats$train_RMSE <- rmse(train_y, train_predict)
    ELN_stats[[set]]$loss_stats$train_RSquare <- (1 - sse(train_y, train_predict) / sum((train_y - mean(train_y))^2))
        
    #Validation
    valid_predict <- predict(model, validation_x, alpha = best_model_params$alpha, lambda = best_model_params$lambda)
    ELN_stats[[set]]$loss_stats$validation_MAE <- mae(validation_y, valid_predict)
    ELN_stats[[set]]$loss_stats$validation_MSE <- mse(validation_y, valid_predict)
    ELN_stats[[set]]$loss_stats$validation_RMSE <- rmse(validation_y, valid_predict)
    ELN_stats[[set]]$loss_stats$validation_RSquare <- (1 - sse(validation_y, valid_predict) / sum((validation_y - mean(validation_y))^2))
    
    #Test
    test_predict <- predict(model, test_x, alpha = best_model_params$alpha, lambda = best_model_params$lambda)
    ELN_stats[[set]]$loss_stats$test_MAE <- mae(test_y, test_predict)
    ELN_stats[[set]]$loss_stats$test_MSE <- mse(test_y, test_predict)
    ELN_stats[[set]]$loss_stats$test_RMSE <- rmse(test_y, test_predict)
    ELN_stats[[set]]$loss_stats$test_RSquare <- (1 - sse(test_y, test_predict) / sum((test_y - mean(test_y))^2))
      
    #Forecasts
    ELN_stats[[set]]$forecasts <- predict(model, test_x, alpha = best_model_params$alpha, lambda = best_model_params$lambda)
    
    #Forecast residuals
    ELN_stats[[set]]$forecast_resids <- test_y - predict(model, test_x, alpha = best_model_params$alpha, lambda = best_model_params$lambda)
    
    #Variable Importance
    ELN_stats[[set]]$variable_importance <- ELN_variable_importance(test, timeSlices, model, alpha = best_model_params$alpha, lambda = best_model_params$lambda)
  }
  return(ELN_stats)
}

# Foreach implementation of above function

ELN_fit_stats <- function(alpha_grid, nlamb, timeSlices, pooled_panel, loss_function) {
  
    ELN_stats_list <- foreach(set = (1:3)) %dopar% {
    #Load Training, validation and test sets
    train <- pooled_panel %>%
      filter(time %in% timeSlices[[set]]$train)
    validation <- pooled_panel %>%
      filter(time %in% timeSlices[[set]]$validation)
    test <- pooled_panel %>%
      filter(time %in% timeSlices[[set]]$test)
    
    train_x <- as.matrix(train[4:ncol(train)])
    train_y <- as.matrix(train$rt)
    validation_x <- as.matrix(validation[4:ncol(validation)])
    validation_y <- as.matrix(validation$rt)
    test_x <- as.matrix(test[4:ncol(test)])
    test_y <- as.matrix(test$rt)
      
    #Get models fit over the grid of hyperparameters
    ELN_model_grid <- ELN_model_grid(alpha_grid, train_x, train_y, validation_x, validation_y, loss_function, nlamb = nlamb)
        
    #Get the best tuning parameters
    best_model_params <- get_ELN_best_tune(ELN_model_grid)
    
    #Initialize stats list
    ELN_stats <- list(loss_stats = data.frame(train_MAE = 0, train_MSE = 0, train_RMSE = 0, train_RSquare = 0, 
                                              validation_MAE = 0, validation_MSE = 0, validation_RMSE = 0, validation_RSquare = 0, 
                                              test_MAE = 0, test_MSE = 0, test_RMSE = 0, test_RSquare = 0),
                      #Other useful things
                      forecasts = 0,
                      forecast_resids = 0,
                      model = 0,
                      hyperparameters = 0,
                      variable_importance = 0)
      
    #Model
    model <- ELN_model_grid[[best_model_params$list_index]]$model
    ELN_stats$model <- model
    
    #Hyperparameters
    ELN_stats$hyperparameters <- best_model_params
      
    #Loss Stats Dataframe
    #Train
    train_predict <- predict(model, train_x, alpha = best_model_params$alpha, lambda = best_model_params$lambda)
    ELN_stats$loss_stats$train_MAE <- mae(train_y, train_predict)
    ELN_stats$loss_stats$train_MSE <- mse(train_y, train_predict)
    ELN_stats$loss_stats$train_RMSE <- rmse(train_y, train_predict)
    ELN_stats$loss_stats$train_RSquare <- (1 - sse(train_y, train_predict) / sum((train_y - mean(train_y))^2))
        
    #Validation
    valid_predict <- predict(model, validation_x, alpha = best_model_params$alpha, lambda = best_model_params$lambda)
    ELN_stats$loss_stats$validation_MAE <- mae(validation_y, valid_predict)
    ELN_stats$loss_stats$validation_MSE <- mse(validation_y, valid_predict)
    ELN_stats$loss_stats$validation_RMSE <- rmse(validation_y, valid_predict)
    ELN_stats$loss_stats$validation_RSquare <- (1 - sse(validation_y, valid_predict) / sum((validation_y - mean(validation_y))^2))
    
    #Test
    test_predict <- predict(model, test_x, alpha = best_model_params$alpha, lambda = best_model_params$lambda)
    ELN_stats$loss_stats$test_MAE <- mae(test_y, test_predict)
    ELN_stats$loss_stats$test_MSE <- mse(test_y, test_predict)
    ELN_stats$loss_stats$test_RMSE <- rmse(test_y, test_predict)
    ELN_stats$loss_stats$test_RSquare <- (1 - sse(test_y, test_predict) / sum((test_y - mean(test_y))^2))
      
    #Forecasts
    ELN_stats$forecasts <- predict(model, test_x, alpha = best_model_params$alpha, lambda = best_model_params$lambda)
    
    #Forecast residuals
    ELN_stats$forecast_resids <- test_y - predict(model, test_x, alpha = best_model_params$alpha, lambda = best_model_params$lambda)
    
    #Variable Importance
    ELN_stats$variable_importance <- ELN_variable_importance(test, timeSlices, model, alpha = best_model_params$alpha, lambda = best_model_params$lambda)
    
    ELN_stats
  }
  ELN_stats_list
}

#Testing if function works

ELN_stats_mae <- ELN_fit_stats(alpha_grid, nlamb = 100, timeSlices, pooled_panel, loss_function = "mae")

ELN_stats_mse <- ELN_fit_stats(alpha_grid, nlamb = 100, timeSlices, pooled_panel, loss_function = "mse")

ELN_stats_mse[[1]]$loss_stats
ELN_stats_mse[[1]]$model$alpha

ELN_stats_mse[[2]]$loss_stats
ELN_stats_mse[[2]]$model$alpha

ELN_stats_mse[[3]]$loss_stats
ELN_stats_mse[[3]]$model$alpha

ELN_stats_mse[[1]]$variable_importance %>% arrange(desc(importance))
ELN_stats_mse[[2]]$variable_importance %>% arrange(desc(importance))
ELN_stats_mse[[3]]$variable_importance %>% arrange(desc(importance))

ELN_stats_mae[[1]]$loss_stats
ELN_stats_mae[[1]]$model$alpha

ELN_stats_mae[[2]]$loss_stats
ELN_stats_mae[[2]]$model$alpha

ELN_stats_mae[[3]]$loss_stats
ELN_stats_mae[[3]]$model$alpha

ELN_stats_mae[[1]]$variable_importance %>% arrange(desc(importance))
ELN_stats_mae[[2]]$variable_importance %>% arrange(desc(importance))
ELN_stats_mae[[3]]$variable_importance %>% arrange(desc(importance))

# Notes
# Mostly consistent results with the previous implementation, very minor differences in optimal alpha values chosen
# Similarly, optimal alpha values are highly unstable
# Does not seem to particularly different between MAE and MSE versions

# Moving from A1 to A2
# LM struggles greatly with more complex A matrix specifications
# However, ELN actually does quite well in spite of this
# It actuallyhas higher R squared values than the true r squared, presumbaly indicating that it is somehow capturing the SV error structure
# In general, it seems that ELN is choosing very parsimonious representations (look at variable importance metrics that are at 0)
```

```{r random_forest}
#Random Forest

#Random Forest should also be fairly straightforward, only two tuning parameters mtry and ntree (usually)

#The vanilla randomForest package is very basic and too slow
#We will use the ranger package, which is apparently a C++ implementation of the random forest algorithm and should be faster
#Another alternative is to use XGBoost (which is very very fast) and set boosting rounds = 1, because in this case it should be a standard random forest

#Compute power is a real struggle, and the aim is just to get the code basis working for the moment. Very conservative grids are therefore used for now

#Check out randomForestSRC for an implementation of randomForests that allows for quantile regression trees. Quantile regression use a check loss function, which is a superset of absolute error
#forestr package doesn't seem to want to function on honours lab computers

#Default ntree = 500
#Default mtry = count(predictors/3) = 400/3 = 133

# Looking at Gu et al's paper for random forests, it seems that they ONLY gridsearch over ntree and depth
# More contemporary sources (such as the package authors) typically do NOT recommend grid searching over ntree, and recommend a grid search over mtry, nodesize, nodedepth, etc
# Gu et al's approach does not make sense as ntree is suggested to be simply as large as computationally feasible (as increasing ntree usually increases performance, but with diminishing returns), and node depth to be as deep as possible (for highly unbiased individual trees)

RF_grid <- expand.grid(
  #ntree usually isn't tuned. Just set to max of computationally feasible
  ntree = 50,
  mtry = seq(10, round(ncol(pooled_panel[4:ncol(pooled_panel)])/3), 20)
  #nodesize = seq(2, 14, 2)
  # nodedepth recommended not to be changed
  #nodedepth = 1,
)

# Fit an RF model over the entire grid of hyperparameters
# Ideally, we would save all the models as we computed them
# This is very memory intensive, hence they will NOT be saved
# Downside is that the optimal model will have to be recomputed from the optimal hyperparameters

# Special Note: variable importance using a standard for loop (not parallelized) here is very computationally intensive
# ALWAYS make sure that the parallelized version fo RF variable importance is used

RF_fit_model_grid <- function(f, train, validation, RF_grid, loss_function) {
  #Initialize List
  RF_model_grid <- rep(list(0), nrow(RF_grid))
  
  for (i in 1:nrow(RF_grid)) {
    
    RF_model_grid[[i]] <- list(RF_grid = 0, model = 0)
    
    #MSE Case
    if (loss_function == "mse") {
      RF <- rfsrc(f, train, 
                  #Hyperparameters
                  ntree = RF_grid$ntree[i],
                  mtry = RF_grid$mtry[i],
                  nodesize = RF_grid$nodesize[i],
                  splitrule = "mse"
                  )
      
      #RF_model_grid[[i]]$model <- RF
      RF_model_grid[[i]]$RF_grid <- cbind(RF_grid[i, ],
                                          #Train Loss
                                          train_loss = mse(train$rt, predict(RF)$predicted),
                                          #Validation Loss
                                          validation_loss = mse(validation$rt, predict(RF, newdata = validation)$predicted)
                                          )
      
    } else {
      #MAE Case
      RF <- rfsrc(f, train, 
                  #Hyperparameters
                  ntree = RF_grid$ntree[i],
                  mtry = RF_grid$mtry[i],
                  nodesize = RF_grid$nodesize[i],
                  splitrule = "quantile.regr",
                  prob = 0.5
                  )
      
      #RF_model_grid[[i]]$model <- RF
      RF_model_grid[[i]]$RF_grid <- cbind(RF_grid[i, ],
                                          #Train Loss
                                          train_loss = mae(train$rt, predict(RF)$predicted),
                                          #Validation Loss
                                          validation_loss = mae(validation$rt, predict(RF, newdata = validation)$predicted)
                                          )
    }
  }
  return(RF_model_grid)
}

#Returns the dataframe row containing the "best" hyperparameters

get_RF_best_tune <- function(RF_model_grid) {
  RF_tune_grid <- RF_model_grid[[1]]$RF_grid
  for (i in 2:length(RF_model_grid)) {
    RF_tune_grid <- rbind(RF_tune_grid, RF_model_grid[[i]]$RF_grid)
  }
  return(RF_tune_grid[which.min(RF_tune_grid$validation_loss), ])
}

RF_fit_stats <- function(f, pooled_panel, RF_grid, timeSlices, loss_function) {
  #Initialize
  RF_stats <- rep(list(0), 3)
  
  #Load training, validation and test sets
  
  for (set in 1:3) {
    
    RF_stats[[set]] <- list(loss_stats = data.frame(train_MAE = 0, train_MSE = 0, train_RMSE = 0, train_RSquare = 0, 
                                                    validation_MAE = 0, validation_MSE = 0, validation_RMSE = 0, validation_RSquare = 0, 
                                                    test_MAE = 0, test_MSE = 0, test_RMSE = 0, test_RSquare = 0),
                            #Other useful things
                            forecasts = 0,
                            forecast_resids = 0,
                            model = 0,
                            hyperparameters = 0,
                            variable_importance = 0)
    
    train <- pooled_panel %>%
      filter(time %in% timeSlices[[set]]$train)
    validation <- pooled_panel %>%
      filter(time %in% timeSlices[[set]]$validation)
    test <- pooled_panel %>%
      filter(time %in% timeSlices[[set]]$test)
    
    #Fit on training Set over grid of hyperparameters
    
    model_grid <- RF_fit_model_grid(f, train, validation, RF_grid, loss_function)
    
    #Get the best hyperparameters
    
    best_model_params <- get_RF_best_tune(model_grid)
    RF_stats[[set]]$hyperparameters <- best_model_params
    
    #Compute the optimal model
    
    if (loss_function == "mse") {
      model <- rfsrc(f, train, 
                     #Hyperparameters
                     ntree = best_model_params$ntree,
                     mtry = best_model_params$mtry,
                     nodesize = best_model_params$nodesize,
                     splitrule = "mse",
                     bootstrap = "by.root", samptype = "swr"
                     )
    } else {
      model <- rfsrc(f, train, 
                     #Hyperparameters
                     ntree = best_model_params$ntree,
                     mtry = best_model_params$mtry,
                     nodesize = best_model_params$nodesize,
                     splitrule = "quantile.regr",
                     prob = 0.5,
                     bootstrap = "by.root", samptype = "swr"
                     )
    }
    
    RF_stats[[set]]$model <- model
    
    #Train
    train_predict <- predict(model, train, alpha = best_model_params$alpha, lambda = best_model_params$lambda)$predicted
    RF_stats[[set]]$loss_stats$train_MAE <- mae(train$rt, train_predict)
    RF_stats[[set]]$loss_stats$train_MSE <- mse(train$rt, train_predict)
    RF_stats[[set]]$loss_stats$train_RMSE <- rmse(train$rt, train_predict)
    RF_stats[[set]]$loss_stats$train_RSquare <- (1 - sse(train$rt, train_predict) / sum((train$rt - mean(train$rt))^2))
        
    #Validation
    valid_predict <- predict(model, newdata = validation, alpha = best_model_params$alpha, lambda = best_model_params$lambda)$predicted
    RF_stats[[set]]$loss_stats$validation_MAE <- mae(validation$rt, valid_predict)
    RF_stats[[set]]$loss_stats$validation_MSE <- mse(validation$rt, valid_predict)
    RF_stats[[set]]$loss_stats$validation_RMSE <- rmse(validation$rt, valid_predict)
    RF_stats[[set]]$loss_stats$validation_RSquare <- (1 - sse(validation$rt, valid_predict) / sum((validation$rt - mean(validation$rt))^2))
    
    #Test
    test_predict <- predict(model, newdata = test, alpha = best_model_params$alpha, lambda = best_model_params$lambda)$predicted
    RF_stats[[set]]$loss_stats$test_MAE <- mae(test$rt, test_predict)
    RF_stats[[set]]$loss_stats$test_MSE <- mse(test$rt, test_predict)
    RF_stats[[set]]$loss_stats$test_RMSE <- rmse(test$rt, test_predict)
    RF_stats[[set]]$loss_stats$test_RSquare <- (1 - sse(test$rt, test_predict) / sum((test$rt - mean(test$rt))^2))
      
    #Forecasts
    RF_stats[[set]]$forecasts <- test_predict
    
    #Forecast residuals
    RF_stats[[set]]$forecast_resids <- test$rt - test_predict
    
    #Variable Importance
    RF_stats[[set]]$variable_importance <- RF_variable_importance(test, timeSlices, model)
  }
  return(RF_stats)
}

RF_MSE <- RF_fit_stats(f, pooled_panel, RF_grid, timeSlices, "mse")

RF_MAE <- RF_fit_stats(f, pooled_panel, RF_grid, timeSlices, "mae")

RF_MSE[[1]]$loss_stats
RF_MSE[[2]]$loss_stats
RF_MSE[[3]]$loss_stats

RF_MSE[[1]]$variable_importance %>% arrange(desc(importance))

RF_MAE[[1]]$loss_stats
RF_MAE[[2]]$loss_stats
RF_MAE[[3]]$loss_stats

# Notes
# Performance seems to be worse than other methods, quite surprising
# Quantile trees (ie MAE) do better than MSE trees
# Quantile trees are much more computationally intensive
# These will probably perform much better given a better grid search, and increasing ntrees.
# Currently, even ntrees = 50 is really quite intensive (overnight)
# This is much more manageable on the honours computers (approx 30 minutes)

# Other notes
# 
```
## Neural Networks WRT MSE

```{r neural_networks}
# Neural Networks

set.seed(27935248)

#Build neural networks with neuron numbers according to geometric pyramid rule, and ReLU activation function for all layers
# IE Input layer > 32 neurons > 16 neurons > 8 neurons > 4 neurons > 2 neurons > output

##Neural Network generalized

NNet_fit_stats <- function(pooled_panel, timeSlices, hidden_layers, loss_function, batch_size, patience) {
  #Initialize
  
  NNet_stats <- rep(list(0), 3)
  
  for (set in 1:3) {
    NNet_stats[[set]] <- list(loss_stats = data.frame(train_MAE = 0, train_MSE = 0, train_RMSE = 0, train_RSquare = 0, 
                                                      validation_MAE = 0, validation_MSE = 0, validation_RMSE = 0, validation_RSquare = 0, 
                                                      test_MAE = 0, test_MSE = 0, test_RMSE = 0, test_RSquare = 0),
                              #Other useful things
                              forecasts = 0,
                              forecast_resids = 0,
                              model = 0,
                              variable_importance = 0)
      
    #Load Training, validation and test sets
    train <- pooled_panel %>%
      filter(time %in% timeSlices[[set]]$train)
    validation <- pooled_panel %>%
      filter(time %in% timeSlices[[set]]$validation)
    test<- pooled_panel %>%
      filter(time %in% timeSlices[[set]]$test)
      
    train_x <- train[4:ncol(train)]
    train_y <- train$rt
      
    validation_x <- validation[4:ncol(validation)]
    validation_y <- validation$rt
      
    test_x <- test[4:ncol(test)]
    test_y <- test$rt
      
    # Fit the model
    # The patience parameter is the amount of epochs to check for improvement.
    # Gu et al don't say what their early stopping parameter p is
    early_stop <- callback_early_stopping(monitor = "val_loss", patience = patience, restore_best_weights = TRUE)
    
    build_NN <- function(hidden_layers, loss_function) {
        
      if (hidden_layers == 1) {
        
        model <- keras_model_sequential() %>%
        
        # Layer 1
        layer_dense(units = 32, input_shape = ncol(train_x)) %>%
          layer_activation("relu") %>%
          layer_activity_regularization(l1 = 0.01) %>%
          layer_batch_normalization() %>%
        # Output Layer
          layer_dense(units = 1, activation = "linear")
        
      } else if (hidden_layers == 2) {
        
        model <- keras_model_sequential() %>%
          
        # Layer 1
        layer_dense(units = 32, input_shape = ncol(train_x)) %>%
          layer_activation("relu") %>%
          layer_activity_regularization(l1 = 0.01) %>%
          layer_batch_normalization() %>%
        # Layer 2
        layer_dense(units = 16) %>%
          layer_activation("relu") %>%
          layer_activity_regularization(l1 = 0.01) %>%
          layer_batch_normalization() %>%
        # Output Layer
        layer_dense(units = 1, activation = "linear")
          
      } else if (hidden_layers == 3) {
          
        model <- keras_model_sequential() %>%
          
        # Layer 1
        layer_dense(units = 32, input_shape = ncol(train_x)) %>%
          layer_activation("relu") %>%
          layer_activity_regularization(l1 = 0.01) %>%
          layer_batch_normalization() %>%
        # Layer 2
        layer_dense(units = 16) %>%
          layer_activation("relu") %>%
          layer_activity_regularization(l1 = 0.01) %>%
          layer_batch_normalization() %>%
        #Layer 3
        layer_dense(units = 8) %>%
          layer_activation("relu") %>%
          layer_activity_regularization(l1 = 0.01) %>%
          layer_batch_normalization() %>%
        # Output Layer
        layer_dense(units = 1, activation = "linear")
          
      } else if (hidden_layers == 4) {
          
        model <- keras_model_sequential() %>%
        
        # Layer 1
        layer_dense(units = 32, input_shape = ncol(train_x)) %>%
          layer_activation("relu") %>%
          layer_activity_regularization(l1 = 0.01) %>%
          layer_batch_normalization() %>%
        # Layer 2
        layer_dense(units = 16) %>%
          layer_activation("relu") %>%
          layer_activity_regularization(l1 = 0.01) %>%
          layer_batch_normalization() %>%
        #Layer 3
        layer_dense(units = 8) %>%
          layer_activation("relu") %>%
          layer_activity_regularization(l1 = 0.01) %>%
          layer_batch_normalization() %>%
        #Layer 4
        layer_dense(units = 4) %>%
          layer_activation("relu") %>%
          layer_activity_regularization(l1 = 0.01) %>%
          layer_batch_normalization() %>%
        # Output Layer
        layer_dense(units = 1, activation = "linear")
          
      } else {
        
        model <- keras_model_sequential() %>%
        
        # Layer 1
        layer_dense(units = 32, input_shape = ncol(train_x)) %>%
          layer_activation("relu") %>%
          layer_activity_regularization(l1 = 0.01) %>%
          layer_batch_normalization() %>%
        # Layer 2
        layer_dense(units = 16) %>%
          layer_activation("relu") %>%
          layer_activity_regularization(l1 = 0.01) %>%
          layer_batch_normalization() %>%
        #Layer 3
        layer_dense(units = 8) %>%
          layer_activation("relu") %>%
          layer_activity_regularization(l1 = 0.01) %>%
          layer_batch_normalization() %>%
        #Layer 4
        layer_dense(units = 4) %>%
          layer_activation("relu") %>%
          layer_activity_regularization(l1 = 0.01) %>%
          layer_batch_normalization() %>%
        #Layer 5
        layer_dense(units = 2) %>%
          layer_activation("relu") %>%
          layer_activity_regularization(l1 = 0.01) %>%
          layer_batch_normalization() %>%
        # Output Layer
        layer_dense(units = 1, activation = "linear")
          
      }
      model %>% compile(
        loss = loss_function,
        optimizer = "adam",
        metrics = list("mae", "mse")
      )
      model
    }
      
    neural_network <- build_NN(hidden_layers, loss_function)
    
    # Other options used throughout the neural network fitting process are specified here
    # Namely, batch size
    # In general, larger batch sizes result in faster progress in training, but don't always converge as fast. Smaller batch sizes train slower, but can converge faster.
    # Default batch size is 32
    neural_network %>% fit(as.matrix(train_x), as.matrix(train_y), 
                           batch_size = batch_size, epochs = 500, verbose = 1, 
                           validation_data = list(as.matrix(validation_x), as.matrix(validation_y)),
                           callbacks = list(early_stop))
    
    #Model
    NNet_stats[[set]]$model <- neural_network
    
    #Train
    train_predict <- neural_network %>% predict(as.matrix(train_x))
    
    NNet_stats[[set]]$loss_stats$train_MAE <- mae(train$rt, train_predict)
    NNet_stats[[set]]$loss_stats$train_MSE <- mse(train$rt, train_predict)
    NNet_stats[[set]]$loss_stats$train_RMSE <- rmse(train$rt, train_predict)
    NNet_stats[[set]]$loss_stats$train_RSquare <- R2(train_predict, train$rt, form = "traditional")
          
    #Validation
    validation_predict <- neural_network %>% predict(as.matrix(validation_x))
    
    NNet_stats[[set]]$loss_stats$validation_MAE <- mae(validation$rt, validation_predict)
    NNet_stats[[set]]$loss_stats$validation_MSE <- mse(validation$rt, validation_predict)
    NNet_stats[[set]]$loss_stats$validation_RMSE <- rmse(validation$rt, validation_predict)
    NNet_stats[[set]]$loss_stats$validation_RSquare <- R2(validation_predict, validation$rt, form = "traditional")
      
    #Test
    test_predict <- neural_network %>% predict(as.matrix(test_x))
    
    NNet_stats[[set]]$loss_stats$test_MAE <- mae(test$rt, test_predict)
    NNet_stats[[set]]$loss_stats$test_MSE <- mse(test$rt, test_predict)
    NNet_stats[[set]]$loss_stats$test_RMSE <- rmse(test$rt, test_predict)
    NNet_stats[[set]]$loss_stats$test_RSquare <- R2(test_predict, test$rt, form = "traditional")
        
    #Forecasts
    NNet_stats[[set]]$forecasts <- test_predict
      
    #Forecast residuals
    NNet_stats[[set]]$forecast_resids <- test$rt - test_predict
    
    #Variable Importance
    NNet_stats[[set]]$variable_importance <- NNet_variable_importance(test, timeSlices, neural_network)
  }
  NNet_stats
}

NNet_1_mse_stats <- NNet_fit_stats(pooled_panel, timeSlices, 1, "mse", batch_size = 32, 10)
NNet_1_mae_stats <- NNet_fit_stats(pooled_panel, timeSlices, 1, "mae", batch_size = 32, 10)

NNet_1_mse_stats[[1]]$loss_stats
NNet_1_mse_stats[[2]]$loss_stats
NNet_1_mse_stats[[3]]$loss_stats

NNet_1_mse_stats[[1]]$variable_importance %>% arrange(desc(importance))

NNet_1_mae_stats[[1]]$loss_stats
NNet_1_mae_stats[[2]]$loss_stats
NNet_1_mae_stats[[3]]$loss_stats

##

NNet_2_mse_stats <- NNet_fit_stats(pooled_panel, timeSlices, 2, "mse", batch_size = 512, 10)
NNet_2_mae_stats <- NNet_fit_stats(pooled_panel, timeSlices, 2, "mae", batch_size = 512, 10)

NNet_2_mse_stats[[1]]$loss_stats
NNet_2_mse_stats[[2]]$loss_stats
NNet_2_mse_stats[[3]]$loss_stats

NNet_2_mae_stats[[1]]$loss_stats
NNet_2_mae_stats[[2]]$loss_stats
NNet_2_mae_stats[[3]]$loss_stats

##

NNet_3_mse_stats <- NNet_fit_stats(pooled_panel, timeSlices, 3, "mse", batch_size = 512, 10)
NNet_3_mae_stats <- NNet_fit_stats(pooled_panel, timeSlices, 3, "mae", batch_size = 512, 10)

NNet_3_mse_stats[[1]]$loss_stats
NNet_3_mse_stats[[2]]$loss_stats
NNet_3_mse_stats[[3]]$loss_stats

NNet_3_mae_stats[[1]]$loss_stats
NNet_3_mae_stats[[2]]$loss_stats
NNet_3_mae_stats[[3]]$loss_stats

##

NNet_4_mse_stats <- NNet_fit_stats(pooled_panel, timeSlices, 4, "mse", batch_size = 32, 30)
NNet_4_mae_stats <- NNet_fit_stats(pooled_panel, timeSlices, 4, "mae", batch_size = 32, 10)

NNet_4_mse_stats[[1]]$loss_stats
NNet_4_mse_stats[[2]]$loss_stats
NNet_4_mse_stats[[3]]$loss_stats

NNet_4_mae_stats[[1]]$loss_stats
NNet_4_mae_stats[[2]]$loss_stats
NNet_4_mae_stats[[3]]$loss_stats

##

NNet_5_mse_stats <- NNet_fit_stats(pooled_panel, timeSlices, 5, "mse", batch_size = 32, 30)
NNet_5_mae_stats <- NNet_fit_stats(pooled_panel, timeSlices, 5, "mae", batch_size = 32, 20)

NNet_5_mse_stats[[1]]$loss_stats
NNet_5_mse_stats[[2]]$loss_stats
NNet_5_mse_stats[[3]]$loss_stats

NNet_5_mae_stats[[1]]$loss_stats
NNet_5_mae_stats[[2]]$loss_stats
NNet_5_mae_stats[[3]]$loss_stats

# Other general observations
# Increasing the number of layers is NOT good. Neural Network 5 has validation loss graphs that do not look right at all

# Neural Networks WRT MAE are not good for highly non-linear specifications
# Very unstable out of sample r squared values
```


