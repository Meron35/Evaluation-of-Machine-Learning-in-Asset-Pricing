scale_fill_brewer(palette = "Paired") +
facet_grid(rows = vars(cross_corr), cols = vars(dgp_spec))
ggsave(filename = "~/GitHub/Evaluation-of-Machine-Learning-in-Asset-Pricing/Results/simulation_test_mae.pdf",
width = 15 * a4_aspect_ratio, height = 15, units = "cm")
all_loss_df_averaged %>%
ungroup() %>%
mutate(sample = as.factor(sample)) %>%
ggplot() +
geom_bar(aes(x = model, y = test_RMSE, fill = sample), stat = "identity", position = position_dodge()) +
scale_x_discrete(label = labels, breaks = breaks) +
theme_bw() +
theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
scale_fill_brewer(palette = "Paired") +
facet_grid(rows = vars(cross_corr), cols = vars(dgp_spec))
ggsave(filename = "~/GitHub/Evaluation-of-Machine-Learning-in-Asset-Pricing/Results/simulation_test_rmse.pdf",
width = 15 * a4_aspect_ratio, height = 15, units = "cm")
all_loss_df_averaged %>%
ungroup() %>%
mutate(sample = as.factor(sample)) %>%
ggplot() +
geom_bar(aes(x = model, y = test_RSquare, fill = sample), stat = "identity", position = position_dodge()) +
scale_x_discrete(label = labels, breaks = breaks) +
theme_bw() +
theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
scale_fill_brewer(palette = "Paired") +
facet_grid(rows = vars(cross_corr), cols = vars(dgp_spec))
ggsave(filename = "~/GitHub/Evaluation-of-Machine-Learning-in-Asset-Pricing/Results/simulation_test_rsquare.pdf",
width = 15 * a4_aspect_ratio, height = 15, units = "cm")
library(tidyverse)
library(jsonlite)
library(Metrics)
library(caret)
library(lubridate)
## Multicore Setup
library(foreach)
library(doFuture)
#Registering
registerDoFuture()
plan(multisession)
tidy_to_json <- function(data, start) {
stock_id <- data$stock %>%
unique()
# Number of cross sectional units
cross_units <- length(stock_id)
## JUst setting the beginning time to something arbitrary for now, change if needed
pooled_panel_json <- data.frame(start = rep(start, cross_units),
target = c(1:cross_units),
dynamic_feat = c(1:cross_units))
pooled_panel_json <- foreach(i = 1:cross_units, .combine = "rbind") %dopar% {
pooled_panel_filter <- data %>%
filter(stock == stock_id[i])
pooled_panel_filter_feature <- pooled_panel_filter %>%
select(-time, -rt, -stock) %>%
unname() %>%
as.matrix() %>%
# Transpose it to get the right format of one feature series per row
t()
data.frame(start = start, target = list(pooled_panep_filter$rt), dynamic_feat = list(pooled_panel_filter_feature))
}
pooled_panel_json
}
pooled_panel %>% tidy_to_json(start = "2000-01-01")
tidy_to_json <- function(data, start) {
stock_id <- data$stock %>%
unique()
# Number of cross sectional units
cross_units <- length(stock_id)
## JUst setting the beginning time to something arbitrary for now, change if needed
pooled_panel_json <- data.frame(start = rep(start, cross_units),
target = c(1:cross_units),
dynamic_feat = c(1:cross_units))
pooled_panel_json <- foreach(i = 1:cross_units, .combine = "rbind") %dopar% {
pooled_panel_filter <- data %>%
filter(stock == stock_id[i])
pooled_panel_filter_feature <- pooled_panel_filter %>%
select(-time, -rt, -stock) %>%
unname() %>%
as.matrix() %>%
# Transpose it to get the right format of one feature series per row
t()
data.frame(start = start, target = list(pooled_panel_filter$rt), dynamic_feat = list(pooled_panel_filter_feature))
}
pooled_panel_json
}
pooled_panel %>% tidy_to_json(start = "2000-01-01")
tidy_to_json <- function(data, start) {
stock_id <- data$stock %>%
unique()
# Number of cross sectional units
cross_units <- length(stock_id)
## JUst setting the beginning time to something arbitrary for now, change if needed
pooled_panel_json <- data.frame(start = rep(start, cross_units),
target = c(1:cross_units),
dynamic_feat = c(1:cross_units))
pooled_panel_json <- foreach(i = 1:cross_units, .combine = "rbind") %dopar% {
pooled_panel_filter <- data %>%
filter(stock == stock_id[i])
pooled_panel_filer_rt <- pooled_panel_filter$rt %>% list()
pooled_panel_filter_feature <- pooled_panel_filter %>%
select(-time, -rt, -stock) %>%
unname() %>%
as.matrix() %>%
# Transpose it to get the right format of one feature series per row
t() %>%
list()
data.frame(start = start, target = pooled_panel_filter_rt, dynamic_feat = pooled_filter_feature)
}
pooled_panel_json
}
pooled_panel_train <- pooled_panel %>%
filter(time %in% timeSlices[[1]]$train) %>%
tidy_to_json(start = "2000-01-01")
tidy_to_json <- function(data, start) {
stock_id <- data$stock %>%
unique()
# Number of cross sectional units
cross_units <- length(stock_id)
## JUst setting the beginning time to something arbitrary for now, change if needed
pooled_panel_json <- data.frame(start = rep(start, cross_units),
target = c(1:cross_units),
dynamic_feat = c(1:cross_units))
pooled_panel_json <- foreach(i = 1:cross_units, .combine = "rbind") %dopar% {
pooled_panel_filter <- data %>%
filter(stock == stock_id[i])
pooled_panel_filter_rt <- pooled_panel_filter$rt %>% list()
pooled_panel_filter_feature <- pooled_panel_filter %>%
select(-time, -rt, -stock) %>%
unname() %>%
as.matrix() %>%
# Transpose it to get the right format of one feature series per row
t() %>%
list()
data.frame(start = start, target = pooled_panel_filter_rt, dynamic_feat = pooled_filter_feature)
}
pooled_panel_json
}
pooled_panel_train <- pooled_panel %>%
filter(time %in% timeSlices[[1]]$train) %>%
tidy_to_json(start = "2000-01-01")
tidy_to_json <- function(data, start) {
stock_id <- data$stock %>%
unique()
# Number of cross sectional units
cross_units <- length(stock_id)
## JUst setting the beginning time to something arbitrary for now, change if needed
pooled_panel_json <- data.frame(start = rep(start, cross_units),
target = c(1:cross_units),
dynamic_feat = c(1:cross_units))
pooled_panel_json <- foreach(i = 1:cross_units, .combine = "rbind") %dopar% {
pooled_panel_filter <- data %>%
filter(stock == stock_id[i])
pooled_panel_filter_rt <- pooled_panel_filter$rt %>% list()
pooled_panel_filter_feature <- pooled_panel_filter %>%
select(-time, -rt, -stock) %>%
unname() %>%
as.matrix() %>%
# Transpose it to get the right format of one feature series per row
t() %>%
list()
data.frame(start = start, target = pooled_panel_filter_rt, dynamic_feat = pooled_panel_filter_feature)
}
pooled_panel_json
}
pooled_panel_train <- pooled_panel %>%
filter(time %in% timeSlices[[1]]$train) %>%
tidy_to_json(start = "2000-01-01")
tidy_to_json <- function(data, start) {
stock_id <- data$stock %>%
unique()
# Number of cross sectional units
cross_units <- length(stock_id)
## JUst setting the beginning time to something arbitrary for now, change if needed
pooled_panel_json <- data.frame(start = rep(start, cross_units),
target = c(1:cross_units),
dynamic_feat = c(1:cross_units))
pooled_panel_json <- foreach(i = 1:cross_units, .combine = "rbind") %dopar% {
df <- data.frame(start = 0, target = 0, dynamic_feat = 0)
pooled_panel_filter <- data %>%
filter(stock == stock_id[i])
pooled_panel_filter_rt <- pooled_panel_filter$rt %>% list()
pooled_panel_filter_feature <- pooled_panel_filter %>%
select(-time, -rt, -stock) %>%
unname() %>%
as.matrix() %>%
# Transpose it to get the right format of one feature series per row
t() %>%
list()
df$start <- start
df$target <- pooled_panel_filter_rt
df$dynamic_feat <- pooled_panel_filter_feature
df
}
pooled_panel_json
}
pooled_panel_train <- pooled_panel %>%
filter(time %in% timeSlices[[1]]$train) %>%
tidy_to_json(start = "2000-01-01")
View(pooled_panel_train)
?enframe
test_list <- rep(list(0), 10)
for (i in 1:10) {
test_list[[i]] <- matrix(rnorm(100, 0, 1), 10, 10)
}
View(test_list)
enframe(test_list)
enframe(test_list)$value
rbind(test_list[[1]], test_list[[2]])
do.call(test_list, rbind())
?do.call
do.call(rbind, test_list)
?map
test_list[[1]]
test_list[1]
a <- test_list[1]
a[1]
a[[1]]
foreach(i = 1:length(test_list), .combine = "rbind") %do% {
test_list[[i]]
}
library(foreach)
foreach(i = 1:length(test_list), .combine = "cbind") %do% {
test_list[[i]]
}
foreach(i = 1:length(test_list), .combine = "rbind") %do% {
test_list[[i]] <- test_list[[i]]^2
test_list[[i]]
}
################
##Load Libraries
################
library(speedglm)
library(tidyverse)
library(keras)
library(ggplot2)
library(forecast)
library(rlist)
library(Metrics)
library(tensorflow)
library(quantreg)
library(randomForestSRC)
library(hqreg)
library(caret)
library(tsfeatures)
## Vector arima/garch packages
library(xts)
library(uwot)
library(rugarch)
## SPecial xgboost instructions
# CLone my forked version of xgboost with multiclass fix
# Set working directoty the r pakcage folder of that repo
# setwd('C:/Users/Zeyu Zhong/Documents/GitHub/xgboost/R-package')
# install.packages('.', repos = NULL, type="source")
# Other alternative
library(xgboost)
#Parallel Computing
library(foreach)
library(doFuture)
#Registering
registerDoFuture()
plan(multisession)
#Create Training + Test Sets
# Gu et al set a training, validation and test sample equal in length for their simulations. Not the most sensible idea
#$T = 180$ monthly periods corresponds to 15 years. The training sample was set to start from $T = 108$ or 9 years, a validation set 1 year in length. The last 3 years were reserved as a test set never to be used for validation or training.
#This means that there are only 3 sample periods to train on, yay
#Caret and other packages are not so good at dealing with custom training/validation/test sets. Have to specify resampling, tuning and testing manually (ugh)
# Change this to 9 + 3 + 3, maybe more stable procedure
customTimeSlices <- function(start, initialWindow, horizon, validation_size, test_size, set_no) {
time_slice <- list(train = 0, validation = 0, test = 0)
time_slices <- rep(list(time_slice), set_no)
for (t in 1:set_no) {
time_slice$train <- c(start:(initialWindow + (t-1) * horizon + 1))
time_slice$validation <- c((initialWindow + (t-1) * horizon + 2):((initialWindow + (t-1) * horizon) + validation_size + 1))
time_slice$test <- c((initialWindow + (t-1) * horizon) + validation_size + 2):((initialWindow + (t-1) * horizon) + validation_size + test_size + 1)
time_slices[[t]] <- time_slice
}
time_slices
}
#Create custom time slices
timeSlices <- customTimeSlices(start = 2, initialWindow = 108, horizon = 12, validation_size = 36, test_size = 12, set_no = 3)
#Formula Function, makes it easier for those packages with a formula interface
panel_formula <- function(panel){
#Remove the first 3 colNames, as these correspond to the return, time and stock id
panel_colnames <- colnames(panel)[-c(1:3)]
f <- as.formula(c("rt ~", paste(panel_colnames, collapse = "+")))
return(f)
}
g1_A1_sv_0.1 <- readRDS("~/GitHub/Evaluation-of-Machine-Learning-in-Asset-Pricing/R/Simulated_dataset/g1_A1_sv_0.1.RDS")
pooled_panel <- g1_A1_sv_0.1[[1]]$panel
f <- panel_formula(pooled_panel)
# Foreach parallel implementation of previous section
# There are a LOT of predictors to inerate over and often this is the bottleneck for the model fitting procedure
# Therefore, use these parallel implementations if possible
# Also, the code is noticeably neater
# Variable Importance Metric Functions
# Given a single model and test set, return a variable importance dataframe (don't bother sorting them now)
# Note that a training set is NOT needed because this is to be called AFTER the model is fit
# These functions were originally written after the actul fitting functions were
# These are used by the fitting functions, so they need to be declared before them
# Similarly, different functions are needed for all model types because all the model objects have slightly different predict methods
################################################
# Linear Model
LM_variable_importance <- function(test, lm_model) {
test_x <- test[4:ncol(test)]
# Specify .packages = "quantreg" here as it seems it isn't supported and therefore is missed by doFuture
variable_importance_df <- foreach(i = (1:ncol(test_x)), .combine = "rbind", .packages = c("speedglm", "quantreg")) %dopar% {
test_x_zero <- test_x
test_x_zero[, i] <- 0
original_R2 <- R2(predict(lm_model, newdata = test_x), test$rt, form = "traditional")
new_R2 <- R2(predict(lm_model, newdata = test_x_zero), test$rt, form = "traditional")
variable_importance <- data.frame(variable = colnames(test_x)[i], importance = (original_R2 - new_R2))
variable_importance
}
variable_importance_df
}
# Penalized Linear Model
ELN_variable_importance <- function(test, eln_model, alpha, lambda) {
test_x <- as.matrix(test[4:ncol(test)])
variable_importance_df <- foreach(i = (1:ncol(test_x)), .combine = "rbind") %dopar% {
test_x_zero <- test_x
test_x_zero[, i] <- 0
original_R2 <- R2(predict(eln_model, test_x, alpha = alpha, lambda = lambda), test$rt, form = "traditional")
new_R2 <- R2(predict(eln_model, test_x_zero, alpha = alpha, lambda = lambda), test$rt, form = "traditional")
variable_importance <- data.frame(variable = colnames(test_x)[i], importance = (original_R2 - new_R2))
variable_importance
}
variable_importance_df
}
# Random Forest
RF_variable_importance <- function(test, rf_model) {
test_x <- test[4:ncol(test)]
variable_importance_df <- foreach(i = (1:ncol(test_x)), .combine = "rbind", .packages = "randomForestSRC") %do% {
test_x_zero <- test_x
test_x_zero[, i] <- 0
original_R2 <- R2(predict(rf_model, newdata = test_x)$predicted, test$rt, form = "traditional")
new_R2 <- R2(predict(rf_model, newdata = test_x_zero)$predicted, test$rt, form = "traditional")
variable_importance <- data.frame(variable = colnames(test_x)[i], importance = (original_R2 - new_R2))
variable_importance
}
variable_importance_df
}
# Neural Network
# Unfortunately it seems that the tensorflow/keras backend does not work with foreach, so a standard for loop is needed instead
NNet_variable_importance <- function(test, nnet_model) {
test_x <- test[4:ncol(test)]
variable_importance_df <- foreach(i = (1:ncol(test_x)), .combine = "rbind", .packages = c("keras", "tensorflow", "reticulate")) %do% {
test_x_zero <- test_x
test_x_zero[, i] <- 0
original_R2 <- R2(predict(nnet_model, as.matrix(test_x)), test$rt, form = "traditional")
new_R2 <- R2(predict(nnet_model, as.matrix(test_x_zero)), test$rt, form = "traditional")
variable_importance <- data.frame(variable = colnames(test_x)[i], importance = (original_R2 - new_R2))
variable_importance
}
variable_importance_df
}
## Function to calculate cross sectional average residuals, used for Diebold Mariano Tests later
## Don't really believe in this, mostly done because its popular in literature
# Similarly, different functions are needed for each type of model object as they all have slightly different predict methods
## Linear Model
# Given a an lm object and a test dataset, return the cross sectional average forecast errors
lm_ave_forecast_resids <- function(lm_model, test) {
time_periods <- unique(test$time)
# Set .combine to "c" to return a vector of results, which is what we want
ave_forecast_resids_vector <- foreach(t = 1:length(time_periods), .combine = "c", .packages = c("speedglm", "quantreg")) %do% {
test_cross_section <- test %>%
filter(time == time_periods[t])
residuals <- test_cross_section$rt - predict(lm_model, newdata = test_cross_section)
mean(residuals)
}
ave_forecast_resids_vector
}
## Elastic Net
eln_ave_forecast_resids <- function(eln_model, test, alpha, lambda) {
time_periods <- unique(test$time)
# Set .combine to "c" to return a vector of results, which is what we want
ave_forecast_resids_vector <- foreach(t = 1:length(time_periods), .combine = "c") %do% {
test_cross_section <- test %>%
filter(time == time_periods[t])
test_cross_section_x <- as.matrix(test_cross_section[4:ncol(test_cross_section)])
residuals <- test_cross_section$rt - predict(eln_model, test_cross_section_x,
alpha = alpha, lambda = lambda)
mean(residuals)
}
ave_forecast_resids_vector
}
## Random Forest
rf_ave_forecast_resids <- function(rf_model, test) {
time_periods <- unique(test$time)
# Set .combine to "c" to return a vector of results, which is what we want
ave_forecast_resids_vector <- foreach(t = 1:length(time_periods), .combine = "c") %do% {
test_cross_section <- test %>%
filter(time == time_periods[t])
residuals <- test_cross_section$rt - predict(rf_model, newdata = test_cross_section)$predicted
mean(residuals)
}
ave_forecast_resids_vector
}
## Neural Networks
nnet_ave_forecast_resids <- function(nnet_model, test) {
time_periods <- unique(test$time)
# Set .combine to "c" to return a vector of results, which is what we want
ave_forecast_resids_vector <- foreach(t = 1:length(time_periods), .combine = "c") %do% {
test_cross_section <- test %>%
filter(time == time_periods[t])
test_cross_section_x <- test_cross_section[4:ncol(test_cross_section)]
residuals <- test_cross_section$rt - (nnet_model %>% predict(as.matrix(test_cross_section_x)))
mean(residuals)
}
ave_forecast_resids_vector
}
loss_function <- "mae"
set <- 1
knitr::opts_chunk$set(echo = TRUE)
################
##Load Libraries
################
library(tidyverse)
library(keras)
library(quantreg)
library(ggplot2)
library(forecast)
library(rlist)
library(Metrics)
library(ranger)
library(caret)
library(forcats)
library(xtable)
library(randomForestSRC)
library(xts)
library(uwot)
library(rugarch)
#Parallel Computing
library(foreach)
library(doFuture)
#Registering
registerDoFuture()
plan(multisession)
set.seed(27935248)
# Random Forests
RF_grid <- expand.grid(
#ntree usually isn't tuned. Just set to max of computationally feasible
ntree = 50,
mtry = seq(10, ncol(pooled_panel[4:ncol(pooled_panel)])/4, 10),
nodesize = seq(9, 15, 3)
# nodedepth recommended not to be changed
#nodedepth = 1
)
RF_fit_model_grid <- function(f, train, validation, RF_grid, loss_function) {
#Initialize List
RF_model_grid <- rep(list(0), nrow(RF_grid))
for (i in 1:nrow(RF_grid)) {
RF_model_grid[[i]] <- list(RF_grid = 0, model = 0)
#MSE Case
if (loss_function == "mse") {
RF <- rfsrc(f, train,
#Hyperparameters
ntree = RF_grid$ntree[i],
mtry = RF_grid$mtry[i],
nodesize = RF_grid$nodesize[i],
splitrule = "mse"
)
#RF_model_grid[[i]]$model <- RF
RF_model_grid[[i]]$RF_grid <- cbind(RF_grid[i, ],
#Train Loss
train_loss = mse(train$rt, predict(RF)$predicted),
#Validation Loss
validation_loss = mse(validation$rt, predict(RF, newdata = validation)$predicted)
)
} else {
#MAE Case
RF <- rfsrc(f, train,
#Hyperparameters
ntree = RF_grid$ntree[i],
mtry = RF_grid$mtry[i],
nodesize = RF_grid$nodesize[i],
splitrule = "quantile.regr",
prob = 0.5
)
#RF_model_grid[[i]]$model <- RF
RF_model_grid[[i]]$RF_grid <- cbind(RF_grid[i, ],
#Train Loss
train_loss = mae(train$rt, predict(RF)$predicted),
#Validation Loss
validation_loss = mae(validation$rt, predict(RF, newdata = validation)$predicted)
)
}
}
return(RF_model_grid)
}
get_RF_best_tune <- function(RF_model_grid) {
RF_tune_grid <- RF_model_grid[[1]]$RF_grid
for (i in 2:length(RF_model_grid)) {
RF_tune_grid <- rbind(RF_tune_grid, RF_model_grid[[i]]$RF_grid)
}
return(RF_tune_grid[which.min(RF_tune_grid$validation_loss), ])
}
train <- pooled_panel %>%
filter(time %in% timeSlices[[set]]$train)
validation <- pooled_panel %>%
filter(time %in% timeSlices[[set]]$validation)
test <- pooled_panel %>%
filter(time %in% timeSlices[[set]]$test)
model_grid <- RF_fit_model_grid(f, train, validation, RF_grid, loss_function)
best_model_params <- get_RF_best_tune(model_grid)
model <- rfsrc(f, train,
#Hyperparameters
ntree = best_model_params$ntree,
mtry = best_model_params$mtry,
nodesize = best_model_params$nodesize,
splitrule = "quantile.regr",
prob = 0.5,
bootstrap = "by.root", samptype = "swr"
)
