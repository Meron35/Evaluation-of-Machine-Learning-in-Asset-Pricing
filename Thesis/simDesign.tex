\documentclass[12pt]{article}
\usepackage[colorlinks,hyperfootnotes=false,pageanchor=false]{hyperref}
\hypersetup{colorlinks=true, linkcolor=black, citecolor=black}
\usepackage{amsmath, amsthm}
\usepackage{algorithm}
\usepackage[noend]{algpseudocode}
\usepackage[nameinlink,noabbrev]{cleveref}
\usepackage[doublespacing]{setspace}
\usepackage{tabularx}
\usepackage{tabulary}
\usepackage{placeins}
\usepackage{graphicx}
\usepackage{pgfplots}
\usepackage{tikz}
\usepackage{ctable}
\usetikzlibrary{shapes,backgrounds}
\pgfplotsset{compat=newest}
\pgfplotsset{plot coordinates/math parser=false}
\newlength\figureheight
\newlength\figurewidth
\usepackage{relsize}
\usepackage{pdflscape}
\usepackage{amssymb}
\usepackage[round]{natbib}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{bm}
\usepackage{latexsym}
\usepackage{epsfig}
\usepackage{rotating}
\usepackage{ctable}
\usepackage{array}
\usepackage{tabularx}
\usepackage{tabulary}
\usepackage{footnote}
\usepackage{threeparttable}
\usepackage{float}
\newcommand{\blind}{0}
\def\1{1\!{\rm l}}

\addtolength{\oddsidemargin}{-.5in}%
\addtolength{\evensidemargin}{-.25in}%
\addtolength{\textwidth}{1in}%
\addtolength{\textheight}{1.3in}%
\addtolength{\topmargin}{-.8in}%

\setlength{\textwidth}{17.4cm}
\newtheorem{theorem}{Theorem}
\newtheorem{axiom}{Axiom}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}{Proposition}
\newtheorem{assumption}{Assumption}
\newtheorem*{Ass}{Assumption}
\newtheorem{corollary}{Corollary}
\newtheorem{fact}[theorem]{Fact}
\newtheorem{definition}{Definition}
\newtheorem{remark}{Remark}
\newtheorem{comment}{Comment}
\setcounter{definition}{0}
\setcounter{assumption}{0}

\newtheorem{Result}[theorem]{Result}
\newtheorem{conjecture}[theorem]{Conjecture}
\newtheorem{question}[theorem]{Question}
\newtheorem{answer}[theorem]{Answer}
\newtheorem{exercise}[theorem]{Exercise}
\newtheorem{example}[theorem]{Example}

\newcommand{\ignore}[1]{}
\newcommand{\expec}{\mathbb{E}}
\newcommand*{\V}[1]{\mathbf{#1}}
\newcommand{\prob}{\mathbb{P}}
\newcommand{\Keywords}[1]{\par\noindent{{\em \large{Keywords}\/}: #1}}
\newcommand{\JEL}[1]{\par\noindent{{\em \large{JEL Classification}\/}: #1}}
\newcommand{\Var}{{\bf Var}}
\newcommand{\Cov}{{\bf Cov}}
\newcommand{\Inf}{\mathrm{Inf}}
\newcommand{\I}{\mathrm{I}}
\newcommand{\J}{\mathrm{J}}
\newcommand{\y}{\mathbf{y}}
\newcommand{\z}{\mathbf{z}}

\DeclareMathOperator*{\plim}{plim}
\DeclareMathOperator*{\argmax}{arg\,max}
\newcommand{\Plim}[1]{\raisebox{0.5ex}{\scalebox{0.8}{$\displaystyle \plim_{#1}\;$}}}
\def\firstcircle{(0,0) circle (1.5cm)}
\def\secondcircle{(45:2cm) circle (1.5cm)}
\def\thirdcircle{(0:2cm) circle (1.5cm)}

	\title{Simulation Design}
\date{} 
\begin{document}
	\maketitle
	\def\spacingset#1{\renewcommand{\baselinestretch}%
		{#1}\small\normalsize} \spacingset{1}
	



\section*{Monte Carlo Design}

Simulate a latent factor model with stochastic volatility for excess return, $r_{t+1}$, for $t=1,\dots,T$:
\begin{flalign*}
r_{i, t+1}&=gâ•£\left(z_{i, t}\right)+\beta_{i,t+1}v_{t+1}+e_{i, t+1}, \quad z_{i, t}=\left(1, x_{t}\right)^{\prime} \otimes c_{i, t}, \quad \beta_{i, t}=\left(c_{i 1, t}, c_{i 2, t}, c_{i 3, t}\right)\\ e_{i, t+1}&=\exp(\sigma_{i, t+1}/2)\varepsilon_{i, t+1},\\\sigma^2_{i,t+1}&=\omega+\alpha_{i}e_{i,t+1}^{2}+\gamma_i\sigma^2_{t,i}+w_{i,t+1}.
\end{flalign*}Let $v_{t+1}$ be a $3\times 1$ vector of errors, and $w_{i,t+1},\varepsilon_{i,t+1}$ scalar error terms. The matrix $C_t$ is an $N\times P_c$ vector of latent factors, where the first three columns correspond to $\beta_{i,t}$, across the $1\leq i\leq N$ dimensions, while the remaining $P_c-3$ factors do not enter the return equation. The $P_x\times1$ vector $x_t$ is a multivariate time series, and $\varepsilon_{t+1}$ is a $N\times 1$ vector of idiosyncratic errors. 



One of my key concerns with the Gu et al. (2019) design is that the factors are uncorrelated across $i$, and, in particular, that the factors which do not matter in the return equation are uncorrelated with those that matter. This is not what is observed in practice. 

Instead, we will choose a simulation mechanism for $C_t$ that gives some correlation across the factors and across time. To that end, first consider drawing normal random numbers for each $1\leq i\leq N$ and $1\leq j\leq P_{c}$, according to 
$$\overline{c}_{i j, t}=\rho_{j} \overline{c}_{i j, t-1}+\epsilon_{i j, t}, \;\rho_{j}\mathcal{U}[1/2,1].$$Then, define the matrix $$
B:=\Lambda\Lambda'+\frac{1}{10}\mathbb{I}_{n},\;\Lambda_i=(\lambda_{i1},\dots,\lambda_{i4})',\;\lambda_{ik}\sim N(0,1),\; k=1,\dots,4, $$ which we transform into a correlation matrix $W$ via $$W=\text{diag}^{-1/2}(W)W\text{diag}^{-1/2}(W).$$
To build in cross-sectional correlation, from the $N\times P_{c}$ matrix $\bar{C}_t$, we simulate characteristics according to $$\widehat{C}_{t}=W\overline{C}_{t}.$$
Finally, we can construct the ``observed'' characteristics for each $1\leq i\leq N$ and for $j=1,\dots,P_{c}$ according to  $$c_{i j, t}=\frac{2}{n+1} \operatorname{rank}\left(\overline{c}_{i j, t}\right)-1.$$

For simulation of $x_{t}$ we consider a VAR model
\begin{flalign*}
x_{t}=Ax_{t-1}+u_t,
\end{flalign*}where we have three separate specifications for the matrix $A$:
\begin{flalign*}
(1)&\; A=\begin{pmatrix}.95&0&0\\0&.95&0\\0&0&.95\end{pmatrix}\;\;
(2)\; A=\begin{pmatrix}1&0&.25\\0&.95&0\\.25&0&.95\end{pmatrix}\;\;
(3)\; A=\begin{pmatrix}.99&.2&.1\\.2&.90&-.3\\.1&-.3&-.99\end{pmatrix}\end{flalign*}


We will consider four different functions $g(\cdot)$
\begin{flalign*}(1)\; & g\left(z_{i, t}\right)=\left(c_{i 1, t}, c_{i 2, t}, c_{i 3, t} \times x_{t}'\right) \theta_{0}, \;\text { where } \theta_{0}=(0.02,0.02,0.02)^{\prime}\\(2)\;&g\left(z_{i, t}\right)=\left(c_{i 1, t}^{2}, c_{i 1, t} \times c_{i 2, t}, \operatorname{sgn}\left(c_{i 3, t} \times  x_{t}'\right)\right) \theta_{0}, \; \text { where } \; \theta_{0}=(0.04,0.035,0.01)^{\prime} \\(3)\; & g\left(z_{i, t}\right)=\left(1[c_{i3,t}>0],c_{i 2, t}^{3}, c_{i 1, t} \times c_{i 2, t}\times 1[c_{i3,t}>0], \text{logit}\left({c}_{i 3, t} \right)\right) \theta_{0}, \;\text { where } \; \theta_{0}=(0.04,0.035,0.01)^{\prime}  \\(4)\; &g\left(z_{i, t}\right)=\left(\hat{c}_{i 1, t}, \hat{c}_{i 2, t}, \hat{c}_{i 3, t} \times x_{t}'\right) \theta_{0}, \;\text { where } \theta_{0}=(0.02,0.02,0.02)^{\prime}
\end{flalign*}

Need to work out the corresponding cr0ss-sectional $R^2$ in this case. We can then tune $\theta^0$ to be this close to Gu et al. (2019), as well as the predictive $R^2$. This will require some work. 

Follow Gu et al. (2019) in regards to the choice of $N,T,P_{c}$






\end{document}