---
title: "Simulation Models"
author: "Ze Yu Zhong"
date: "19 June 2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

################
##Load Libraries
################

library(glmnet)
library(tidyverse)
library(keras)
library(ggplot2)
library(caret)
library(forecast)
library(rlist)
library(plm)
library(Metrics)
library(ranger)
library(glmnetUtils)

#forestr
# This apparently needs admin privileges to install, do this at home maybe
# devtools::install_github("andeek/forestr")
# devtools::install_github("andeek/rpart")

set.seed(27935248)
```

```{r train_valid_test}
#Create Training + Test Sets

#$T = 180$ monthly periods corresponds to 15 years. The training sample was set to start from $T = 108$ or 9 years, a validation set 1 year in length. The last 3 years were reserved as a test set never to be used for validation or training.

#This means that there are only 3 sample periods to train on, yay

#Caret and other packages are not so good at dealing with custom training/validation/test sets. Have to specify resampling, tuning and testing manually (ugh)

customTimeSlices <- function(start, end, initialWindow, horizon, test_size) {
  total <- c(start:end)
  offset <- start - 1
  set_num <- (end - start + offset - test_size - initialWindow) / horizon
  
  time_slice <- list(train = 0, validation = 0, test = 0)
  time_slices <- rep(list(time_slice), 3)

  for (t in 1:set_num) {
    time_slice$train <- c(start:(initialWindow + (t-1) * horizon + offset))
    time_slice$validation <- c((initialWindow + (t-1) * horizon + offset + 1):(initialWindow + (t) * horizon + offset))
    time_slice$test <- c((initialWindow + (t) * horizon + offset + 1):end)
    
    time_slices[[t]] <- time_slice
  }
  return(time_slices)
}

#Create custom time slices
timeSlices <- customTimeSlices(start = 2, end = 181, initialWindow = 108, horizon = 12, test_size = 36)

```

```{r pooled_ols}
#POLS

#Use panel_g1_A1 generated from Simulation.rmd for now. Generalize it later

pooled_panel <- panel_g1_A1

#Expand formula manually this way because PLM is dumb and gets confused by y ~ .
plm_formula <- function(panel){
  #Remove the first 3 colNames, as these correspond to the return, time and stock id
  panel_colnames <- colnames(panel)[-c(1:3)]
  f <- as.formula(c("rt ~", paste(panel_colnames, collapse = "+")))
  return(f)
}

f <- plm_formula(pooled_panel)

#Train/fit a POLS model wrt MSE

POLS_fit_mse <- function(pooled_panel, timeSlices) {
  #Initialize Loss Function Statistics
  POLS_stats <- rep(list(0), 3)
  
  for (set in 1:3) {
    POLS_stats[[set]] <- list(loss_stats = data.frame(train_MAE = 0, train_MSE = 0, train_RMSE = 0, train_RSquare = 0, 
                                                validation_MAE = 0, validation_MSE = 0, validation_RMSE = 0, validation_RSquare = 0, 
                                                test_MAE = 0, test_MSE = 0, test_RMSE = 0, test_RSquare = 0),
                        #Other useful things
                        forecasts = 0,
                        forecast_resids = 0,
                        model = 0)
    
    #Load Training, validation and test sets
    train <- pooled_panel %>%
      filter(time %in% timeSlices[[set]]$train)
    validation <- pooled_panel %>%
      filter(time %in% timeSlices[[set]]$validation)
    test<- pooled_panel %>%
      filter(time %in% timeSlices[[set]]$test)
    
    #Train Model on training set
    pols <- plm(f, data = train, model = "pooling", index = c("time", "stock"))
    
    POLS_stats[[set]]$model <- pols
    
    #No Tuning Needed
    
    #Statistics
    #Training Set
    POLS_stats[[set]]$loss_stats$train_MAE <- mae(train$rt, predict(pols))
    POLS_stats[[set]]$loss_stats$train_MSE <- mse(train$rt, predict(pols))
    POLS_stats[[set]]$loss_stats$train_RMSE <- rmse(train$rt, predict(pols))
    POLS_stats[[set]]$loss_stats$train_RSquare <- (1 - sse(train$rt, predict(pols, newdata = train)) / sum((train$rt - mean(train$rt))^2))
    
    #Validation Set Statistics
    POLS_stats[[set]]$loss_stats$validation_MAE <- mae(validation$rt, predict(pols, newdata = validation))
    POLS_stats[[set]]$loss_stats$validation_MSE <- mse(validation$rt, predict(pols, newdata = validation))
    POLS_stats[[set]]$loss_stats$validation_RMSE <- rmse(validation$rt, predict(pols, newdata = validation))
    POLS_stats[[set]]$loss_stats$validation_RSquare <- (1 - sse(validation$rt, predict(pols, newdata = validation)) / sum((validation$rt - mean(validation$rt))^2))
      
    #Test Set Statistics
    POLS_stats[[set]]$loss_stats$test_MAE <- mae(test$rt, predict(pols, newdata = test))
    POLS_stats[[set]]$loss_stats$test_MSE <- mse(test$rt, predict(pols, newdata = test))
    POLS_stats[[set]]$loss_stats$test_RMSE <- rmse(test$rt, predict(pols, newdata = test))
    POLS_stats[[set]]$loss_stats$test_RSquare <- (1 - sse(test$rt, predict(pols, newdata = test)) / sum((test$rt - mean(test$rt))^2))
        
    #Forecasts
    POLS_stats[[set]]$forecasts <- predict(pols, newdata = test)
    #Forecast Residuals
    POLS_stats[[set]]$forecast_resids <- test$rt - predict(pols, newdata = test)
  }
  return(POLS_stats)
}

POLS_stats <- POLS_fit_mse(pooled_panel, timeSlices)

POLS_stats[[1]]$loss_stats
POLS_stats[[2]]$loss_stats
POLS_stats[[3]]$loss_stats
```

```{r elastic_net}
#Elasticnet

#Can't seem to find anything online that tunes ELN for you with specified validation sets, instead of straight-forward cross validation
#nfold and foldid require that ELN be performed with at least 3 fold validation - not possible in our context
#DOuble check with David maybe, on hold for now

#actually same function from POLS case
eln_formula <- function(panel){
  #Remove the first 3 colNames, as these correspond to the return, time and stock id
  panel_colnames <- colnames(panel)[-c(1:3)]
  f <- as.formula(c("rt ~", paste(panel_colnames, collapse = "+")))
  return(f)
}

f <- eln_formula(pooled_panel)

#Elasticnet only has alpha and lambda to tune

#Just keep the grid small for now to make sure everything is running
ELN_grid <- expand.grid(
  alpha = seq(0, 1, 0.01),
  lambda = 10^seq(-3, 5, length.out = 50)
)

######################################
## Functions to tune an ELN model
######################################

# Fit the model over the defined grid of hyperparameters, and return it as a list
# Takes a defined grid of hyperparameters, a training set, and a validation set, fits the model over the grid, and returns the models + loss statistics as a list
# Works with both loss functions of MSE and MAE, need to specify these as "mse" or "mae" respectively

ELN_model_grid <- function(ELN_grid, train, validation, loss_function) {
  #Initialize List
  ELN_model_grid <- rep(list(0), nrow(ELN_grid))
  
  for (i in 1:nrow(ELN_grid)) {
    ELN_model_grid[[i]] <- list(ELN_grid = ELN_grid[i,], model = 0, train_loss = 0, validation_loss = 0)
    #Remember to load glmnetUtils package for a formula interface to glmnet
    #Fit the model using grid value
    ELN <- glmnet(formula = f, data = train, alpha = ELN_grid$alpha[i], lambda = ELN_grid$lambda[i])
    ELN_model_grid[[i]]$model <- ELN
    
    #MSE case
    if (loss_function == "mse") {
      #Training Error
      ELN_model_grid[[i]]$train_loss <- mse(train$rt, predict(ELN, newdata = train))
      #Validation Error
      ELN_model_grid[[i]]$validation_loss <- mse(validation$rt, predict(ELN, newdata = validation))
    }
    #MAE case
    else {
      #Training Error
      ELN_model_grid[[i]]$train_loss <- mae(train$rt, predict(ELN, newdata = train))
      #Validation Error
      ELN_model_grid[[i]]$validation_loss <- mae(validation$rt, predict(ELN, newdata = validation))
    }
  }
  return(ELN_model_grid)
}

# Given a list containing the model grid, return the "best" model according to validation loss
# Use in conjunction with previous function

get_ELN_best_tune <- function(ELN_grid, model_grid) {
  ELN_tune_dataframe <- cbind(ELN_grid, train_loss = rep(0, nrow(ELN_grid)), validation_loss = rep(0, nrow(ELN_grid)))
  for (i in 1:nrow(ELN_grid)) {
    #Training Error
    ELN_tune_dataframe$train_loss[i] <- model_grid[[i]]$train_loss
    #Validation Error
    ELN_tune_dataframe$validation_loss[i] <- model_grid[[i]]$validation_loss
  }
  #Return the row index with the lowest validation loss
  return(which.min(ELN_tune_dataframe$validation_loss))
}

# Final big wrapper function that gives an ELN_stats list with everything desired

ELN_fit_stats <- function(ELN_grid, timeSlices, pooled_panel, loss_function) {
  ELN_stats <- rep(list(0), 3)
  
  for (set in 1:3) {
    #Load Training, validation and test sets
    train <- pooled_panel %>%
      filter(time %in% timeSlices[[set]]$train)
    validation <- pooled_panel %>%
      filter(time %in% timeSlices[[set]]$validation)
    test<- pooled_panel %>%
      filter(time %in% timeSlices[[set]]$test)
      
    #Get models fit over the grid of hyperparameters
    ELN_model_grid <- ELN_model_grid(ELN_grid, train, validation, loss_function)
        
    #Get the best tuning parameters
    best_model_index <- get_ELN_best_tune(ELN_grid, ELN_model_grid)
    
    #Initialize stats list
    ELN_stats[[set]] <- list(loss_stats = data.frame(train_MAE = 0, train_MSE = 0, train_RMSE = 0, train_RSquare = 0, 
                         validation_MAE = 0, validation_MSE = 0, validation_RMSE = 0, validation_RSquare = 0, 
                         test_MAE = 0, test_MSE = 0, test_RMSE = 0, test_RSquare = 0),
                         #Other useful things
                         forecasts = 0,
                         forecast_resids = 0,
                         model = 0)
      
    #Model
    model <- ELN_model_grid[[best_model_index]]$model
    ELN_stats[[set]]$model <- ELN_model_grid[[best_model_index]]$model
      
    #Loss Stats Dataframe
    #Train
    ELN_stats[[set]]$loss_stats$train_MAE <- mae(train$rt, predict(model, newdata = train))
    ELN_stats[[set]]$loss_stats$train_MSE <- mse(train$rt, predict(model, newdata = train))
    ELN_stats[[set]]$loss_stats$train_RMSE <- rmse(train$rt, predict(model, newdata = train))
    ELN_stats[[set]]$loss_stats$train_RSquare <- (1 - sse(train$rt, predict(model, newdata = train)) / sum((train$rt - mean(train$rt))^2))
        
    #Validation
    ELN_stats[[set]]$loss_stats$validation_MAE <- mae(validation$rt, predict(model, newdata = validation))
    ELN_stats[[set]]$loss_stats$validation_MSE <- mse(validation$rt, predict(model, newdata = validation))
    ELN_stats[[set]]$loss_stats$validation_RMSE <- rmse(validation$rt, predict(model, newdata = validation))
    ELN_stats[[set]]$loss_stats$validation_RSquare <- (1 - sse(validation$rt, predict(model, newdata = validation)) / sum((validation$rt - mean(validation$rt))^2))
    
    #Test
    ELN_stats[[set]]$loss_stats$test_MAE <- mae(test$rt, predict(model, newdata = test))
    ELN_stats[[set]]$loss_stats$test_MSE <- mse(test$rt, predict(model, newdata = test))
    ELN_stats[[set]]$loss_stats$test_RMSE <- rmse(test$rt, predict(model, newdata = test))
    ELN_stats[[set]]$loss_stats$test_RSquare <- (1 - sse(test$rt, predict(model, newdata = test)) / sum((test$rt - mean(test$rt))^2))
      
    #Forecasts
    ELN_stats[[set]]$forecasts <- predict(model, newdata = test)
    
    #Forecast residuals
    ELN_stats[[set]]$forecast_resids <- test$rt - predict(model, newdata = test)
  }
  return(ELN_stats)
}

#Testing if function works
ELN_stats_mse <- ELN_fit_stats(ELN_grid, timeSlices, pooled_panel, loss_function = "mse")

ELN_stats_mae <- ELN_fit_stats(ELN_grid, timeSlices, pooled_panel, loss_function = "mae")

# Seeing if different loss functions actuallly do anything (they should)
# Note: make sure the grid is sufficiently large enough, otherwise the two different loss functions will often agree on the exact same hyperparameters

ELN_stats_mse[[1]]$loss_stats$test_MSE
ELN_stats_mse[[1]]$model

ELN_stats_mae[[1]]$loss_stats$test_MSE
ELN_stats_mae[[1]]$model
```

```{r random_forest}
#Random Forest

#Random Forest should also be fairly straightforward, only two tuning parameters mtry and ntree (usually)

#The vanilla randomForest package is very basic and too slow
#We will use the ranger package, which is apparently a C++ implementation of the random forest algorithm and should be faster
#Another alternative is to use XGBoost (which is very very fast) and set boosting rounds = 1, because in this case it should be a standard random forest

#Compute power is a real struggle, and the aim is just to get the code basis working for the moment. Very conservative grids are therefore used for now

#Check out randomForestSRC for an implementation of randomForests that allows for quantile regression trees. Quantile regression use a check loss function, which is a superset of absolute error
#forestr package doesn't seem to want to function on honours lab computers

#Default ntree = 500
#Default mtry = count(predictors/3) = 400/3 = 133

rf_grid <- expand.grid(
  ntree = seq(500, 1000, 1000),
  mtry = seq(10, 100, 10)
)

#Generate formula, actually same code from PLM earlier

rf_formula <- function(panel){
  #Remove the first 3 colNames, as these correspond to the return, time and stock id
  panel_colnames <- colnames(panel)[-c(1:3)]
  f <- as.formula(c("rt ~", paste(panel_colnames, collapse = "+")))
  return(f)
}

f <- rf_formula(pooled_panel)

#Initialize Loss Function Statistics

RF_stats <- data.frame(
  set = c(1:3),
  #Training
  train_MAE = c(1:3),
  train_MSE = c(1:3),
  train_RMSE = c(1:3),
  train_R2 = c(1:3),
  #Validation
  valid_MAE = c(1:3),
  valid_MSE = c(1:3),
  valid_RMSE = c(1:3),
  valid_R2 = c(1:3),
  #Test
  test_MAE = c(1:3),
  test_MSE = c(1:3),
  test_RMSE = c(1:3),
  test_R2 = c(1:3)
)

#Initialize Forecasts

RF_forecasts <- rep(list(0), 3)

#Initialize Models

RF_models <- rep(list(0), 3)

#Load training, validation and test sets

train <- pooled_panel %>%
  filter(time %in% timeSlices[[set]]$train)
validation <- pooled_panel %>%
  filter(time %in% timeSlices[[set]]$validation)
test<- pooled_panel %>%
  filter(time %in% timeSlices[[set]]$test)

#Train Model on Training Set

rf <- ranger(f, data = train, num.trees = rf_grid$ntree[1], mtry = rf_grid$mtry[1],
             write.forest = TRUE, min.node.size = 5, splitrule = "variance")

RF_models[[1]] <- 


for (set in 1:3) {
  
  #Training Set Statistics
    RF_stats$train_MAE[set] <- mae(train$return, predict(rf))
    RF_stats$train_MSE[set] <- mse(train$return, predict(rf))
    RF_stats$train_RMSE[set] <- rmse(train$return, predict(rf))
    #R squared still pending, haven't figured out how to do it yet fully
    
  #Validation Set Statistics
    RF_stats$valid_MAE[set] <- mae(validation$return, predict(rf, newdata = validation))
    RF_stats$valid_MSE[set] <- mse(validation$return, predict(rf, newdata = validation))
    RF_stats$valid_RMSE[set] <- rmse(validation$return, predict(rf, newdata = validation))
    #R squared still pending, haven't figured out how to do it yet fully
  
  #Test Set Statistics
    RF_stats$test_MAE[set] <- mae(test$return, predict(rf, newdata = test))
    RF_stats$test_MSE[set] <- mse(test$return, predict(rf, newdata = test))
    RF_stats$test_RMSE[set] <- rmse(test$return, predict(rf, newdata = test))
    #R squared still pending, haven't figured out how to do it yet fully
    
  #Forecasts
    RF_forecasts[[set]]$forecast <- predict(rf, newdata = test)
    RF_forecasts[[set]]$forecast_error <- test$return - predict(rf, newdata = test)
    
}

```

```{r}
#Train RF Model + tune
rf <- ranger(f, data = train, 
             #Hyperparameters
             
             #Tuning parameters
              num.trees = 10,
              mtry = 10,
             
             #No Tuning
              #Default Min node size
              min.node.size = 5,
             
             #Other
             write.forest = TRUE,
             importance = "impurity"
             )

importance(rf)

```


```{r neural_networks}
#Neural Network

```

