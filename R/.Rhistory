layer_batch_normalization() %>%
#Layer 3
layer_dense(units = 8) %>%
layer_activation("relu") %>%
layer_activity_regularization(l1 = 0.01) %>%
layer_batch_normalization() %>%
# Output Layer
layer_dense(units = 1, activation = "linear")
} else if (hidden_layers == 4) {
model <- keras_model_sequential() %>%
# Layer 1
layer_dense(units = 32, input_shape = ncol(train_x)) %>%
layer_activation("relu") %>%
layer_activity_regularization(l1 = 0.01) %>%
layer_batch_normalization() %>%
# Layer 2
layer_dense(units = 16) %>%
layer_activation("relu") %>%
layer_activity_regularization(l1 = 0.01) %>%
layer_batch_normalization() %>%
#Layer 3
layer_dense(units = 8) %>%
layer_activation("relu") %>%
layer_activity_regularization(l1 = 0.01) %>%
layer_batch_normalization() %>%
#Layer 4
layer_dense(units = 4) %>%
layer_activation("relu") %>%
layer_activity_regularization(l1 = 0.01) %>%
layer_batch_normalization() %>%
# Output Layer
layer_dense(units = 1, activation = "linear")
} else {
model <- keras_model_sequential() %>%
# Layer 1
layer_dense(units = 32, input_shape = ncol(train_x)) %>%
layer_activation("relu") %>%
layer_activity_regularization(l1 = 0.01) %>%
layer_batch_normalization() %>%
# Layer 2
layer_dense(units = 16) %>%
layer_activation("relu") %>%
layer_activity_regularization(l1 = 0.01) %>%
layer_batch_normalization() %>%
#Layer 3
layer_dense(units = 8) %>%
layer_activation("relu") %>%
layer_activity_regularization(l1 = 0.01) %>%
layer_batch_normalization() %>%
#Layer 4
layer_dense(units = 4) %>%
layer_activation("relu") %>%
layer_activity_regularization(l1 = 0.01) %>%
layer_batch_normalization() %>%
#Layer 5
layer_dense(units = 2) %>%
layer_activation("relu") %>%
layer_activity_regularization(l1 = 0.01) %>%
layer_batch_normalization() %>%
# Output Layer
layer_dense(units = 1, activation = "linear")
}
model %>% compile(
loss = loss_function,
optimizer = "adam",
metrics = list("mae", "mse")
)
model
}
neural_network <- build_NN(hidden_layers, loss_function)
# Other options used throughout the neural network fitting process are specified here
# Namely, batch size
# In general, larger batch sizes result in faster progress in training, but don't always converge as fast. Smaller batch sizes train slower, but can converge faster.
# Default batch size is 32
neural_network %>% fit(as.matrix(train_x), as.matrix(train_y),
batch_size = batch_size, epochs = 100, verbose = 1,
validation_data = list(as.matrix(validation_x), as.matrix(validation_y)),
callbacks = list(early_stop))
#Model
NNet_stats[[set]]$model <- neural_network
#Train
train_predict <- neural_network %>% predict(as.matrix(train_x))
NNet_stats[[set]]$loss_stats$train_MAE <- mae(train$rt, train_predict)
NNet_stats[[set]]$loss_stats$train_MSE <- mse(train$rt, train_predict)
NNet_stats[[set]]$loss_stats$train_RMSE <- rmse(train$rt, train_predict)
NNet_stats[[set]]$loss_stats$train_RSquare <- R2(train_predict, train$rt, form = "traditional")
#Validation
validation_predict <- neural_network %>% predict(as.matrix(validation_x))
NNet_stats[[set]]$loss_stats$validation_MAE <- mae(validation$rt, validation_predict)
NNet_stats[[set]]$loss_stats$validation_MSE <- mse(validation$rt, validation_predict)
NNet_stats[[set]]$loss_stats$validation_RMSE <- rmse(validation$rt, validation_predict)
NNet_stats[[set]]$loss_stats$validation_RSquare <- R2(validation_predict, validation$rt, form = "traditional")
#Test
test_predict <- neural_network %>% predict(as.matrix(test_x))
NNet_stats[[set]]$loss_stats$test_MAE <- mae(test$rt, test_predict)
NNet_stats[[set]]$loss_stats$test_MSE <- mse(test$rt, test_predict)
NNet_stats[[set]]$loss_stats$test_RMSE <- rmse(test$rt, test_predict)
NNet_stats[[set]]$loss_stats$test_RSquare <- R2(test_predict, test$rt, form = "traditional")
#Forecasts
NNet_stats[[set]]$forecasts <- test_predict
#Forecast residuals
NNet_stats[[set]]$forecast_resids <- test$rt - test_predict
}
NNet_stats
}
NNet_1_mse_stats <- NNet_fit_stats(pooled_panel, timeSlices, 1, "mse", batch_size = 64, 10)
NNet_1_mse_stats[[1]]$loss_stats
NNet_1_mse_stats[[2]]$loss_stats
NNet_1_mse_stats[[3]]$loss_stats
NNet_1_mae_stats[[1]]$loss_stats
NNet_1_mae_stats <- NNet_fit_stats(pooled_panel, timeSlices, 1, "mae", batch_size = 64, 10)
NNet_1_mae_stats[[1]]$loss_stats
NNet_1_mae_stats[[2]]$loss_stats
NNet_1_mae_stats[[3]]$loss_stats
NNet_1_mse_stats <- NNet_fit_stats(pooled_panel, timeSlices, 1, "mse", batch_size = 4096, 10)
NNet_1_mse_stats[[1]]$loss_stats
NNet_1_mse_stats[[2]]$loss_stats
NNet_1_mse_stats[[3]]$loss_stats
NNet_fit_stats <- function(pooled_panel, timeSlices, hidden_layers, loss_function, batch_size, patience) {
#Initialize
NNet_stats <- rep(list(0), 3)
for (set in 1:3) {
NNet_stats[[set]] <- list(loss_stats = data.frame(train_MAE = 0, train_MSE = 0, train_RMSE = 0, train_RSquare = 0,
validation_MAE = 0, validation_MSE = 0, validation_RMSE = 0, validation_RSquare = 0,
test_MAE = 0, test_MSE = 0, test_RMSE = 0, test_RSquare = 0),
#Other useful things
forecasts = 0,
forecast_resids = 0,
model = 0)
#Load Training, validation and test sets
train <- pooled_panel %>%
filter(time %in% timeSlices[[set]]$train)
validation <- pooled_panel %>%
filter(time %in% timeSlices[[set]]$validation)
test<- pooled_panel %>%
filter(time %in% timeSlices[[set]]$test)
train_x <- train[4:ncol(train)]
train_y <- train$rt
validation_x <- validation[4:ncol(validation)]
validation_y <- validation$rt
test_x <- test[4:ncol(test)]
test_y <- test$rt
# Fit the model
# The patience parameter is the amount of epochs to check for improvement.
# Gu et al don't say what their early stopping parameter p is
early_stop <- callback_early_stopping(monitor = "val_loss", patience = patience, restore_best_weights = TRUE)
build_NN <- function(hidden_layers, loss_function) {
if (hidden_layers == 1) {
model <- keras_model_sequential() %>%
# Layer 1
layer_dense(units = 32, input_shape = ncol(train_x)) %>%
layer_activation("relu") %>%
layer_activity_regularization(l1 = 0.01) %>%
layer_batch_normalization() %>%
# Output Layer
layer_dense(units = 1, activation = "linear")
} else if (hidden_layers == 2) {
model <- keras_model_sequential() %>%
# Layer 1
layer_dense(units = 32, input_shape = ncol(train_x)) %>%
layer_activation("relu") %>%
layer_activity_regularization(l1 = 0.01) %>%
layer_batch_normalization() %>%
# Layer 2
layer_dense(units = 16) %>%
layer_activation("relu") %>%
layer_activity_regularization(l1 = 0.01) %>%
layer_batch_normalization() %>%
# Output Layer
layer_dense(units = 1, activation = "linear")
} else if (hidden_layers == 3) {
model <- keras_model_sequential() %>%
# Layer 1
layer_dense(units = 32, input_shape = ncol(train_x)) %>%
layer_activation("relu") %>%
layer_activity_regularization(l1 = 0.01) %>%
layer_batch_normalization() %>%
# Layer 2
layer_dense(units = 16) %>%
layer_activation("relu") %>%
layer_activity_regularization(l1 = 0.01) %>%
layer_batch_normalization() %>%
#Layer 3
layer_dense(units = 8) %>%
layer_activation("relu") %>%
layer_activity_regularization(l1 = 0.01) %>%
layer_batch_normalization() %>%
# Output Layer
layer_dense(units = 1, activation = "linear")
} else if (hidden_layers == 4) {
model <- keras_model_sequential() %>%
# Layer 1
layer_dense(units = 32, input_shape = ncol(train_x)) %>%
layer_activation("relu") %>%
layer_activity_regularization(l1 = 0.01) %>%
layer_batch_normalization() %>%
# Layer 2
layer_dense(units = 16) %>%
layer_activation("relu") %>%
layer_activity_regularization(l1 = 0.01) %>%
layer_batch_normalization() %>%
#Layer 3
layer_dense(units = 8) %>%
layer_activation("relu") %>%
layer_activity_regularization(l1 = 0.01) %>%
layer_batch_normalization() %>%
#Layer 4
layer_dense(units = 4) %>%
layer_activation("relu") %>%
layer_activity_regularization(l1 = 0.01) %>%
layer_batch_normalization() %>%
# Output Layer
layer_dense(units = 1, activation = "linear")
} else {
model <- keras_model_sequential() %>%
# Layer 1
layer_dense(units = 32, input_shape = ncol(train_x)) %>%
layer_activation("relu") %>%
layer_activity_regularization(l1 = 0.01) %>%
layer_batch_normalization() %>%
# Layer 2
layer_dense(units = 16) %>%
layer_activation("relu") %>%
layer_activity_regularization(l1 = 0.01) %>%
layer_batch_normalization() %>%
#Layer 3
layer_dense(units = 8) %>%
layer_activation("relu") %>%
layer_activity_regularization(l1 = 0.01) %>%
layer_batch_normalization() %>%
#Layer 4
layer_dense(units = 4) %>%
layer_activation("relu") %>%
layer_activity_regularization(l1 = 0.01) %>%
layer_batch_normalization() %>%
#Layer 5
layer_dense(units = 2) %>%
layer_activation("relu") %>%
layer_activity_regularization(l1 = 0.01) %>%
layer_batch_normalization() %>%
# Output Layer
layer_dense(units = 1, activation = "linear")
}
model %>% compile(
loss = loss_function,
optimizer = "adam",
metrics = list("mae", "mse")
)
model
}
neural_network <- build_NN(hidden_layers, loss_function)
# Other options used throughout the neural network fitting process are specified here
# Namely, batch size
# In general, larger batch sizes result in faster progress in training, but don't always converge as fast. Smaller batch sizes train slower, but can converge faster.
# Default batch size is 32
neural_network %>% fit(as.matrix(train_x), as.matrix(train_y),
batch_size = batch_size, epochs = 500, verbose = 1,
validation_data = list(as.matrix(validation_x), as.matrix(validation_y)),
callbacks = list(early_stop))
#Model
NNet_stats[[set]]$model <- neural_network
#Train
train_predict <- neural_network %>% predict(as.matrix(train_x))
NNet_stats[[set]]$loss_stats$train_MAE <- mae(train$rt, train_predict)
NNet_stats[[set]]$loss_stats$train_MSE <- mse(train$rt, train_predict)
NNet_stats[[set]]$loss_stats$train_RMSE <- rmse(train$rt, train_predict)
NNet_stats[[set]]$loss_stats$train_RSquare <- R2(train_predict, train$rt, form = "traditional")
#Validation
validation_predict <- neural_network %>% predict(as.matrix(validation_x))
NNet_stats[[set]]$loss_stats$validation_MAE <- mae(validation$rt, validation_predict)
NNet_stats[[set]]$loss_stats$validation_MSE <- mse(validation$rt, validation_predict)
NNet_stats[[set]]$loss_stats$validation_RMSE <- rmse(validation$rt, validation_predict)
NNet_stats[[set]]$loss_stats$validation_RSquare <- R2(validation_predict, validation$rt, form = "traditional")
#Test
test_predict <- neural_network %>% predict(as.matrix(test_x))
NNet_stats[[set]]$loss_stats$test_MAE <- mae(test$rt, test_predict)
NNet_stats[[set]]$loss_stats$test_MSE <- mse(test$rt, test_predict)
NNet_stats[[set]]$loss_stats$test_RMSE <- rmse(test$rt, test_predict)
NNet_stats[[set]]$loss_stats$test_RSquare <- R2(test_predict, test$rt, form = "traditional")
#Forecasts
NNet_stats[[set]]$forecasts <- test_predict
#Forecast residuals
NNet_stats[[set]]$forecast_resids <- test$rt - test_predict
}
NNet_stats
}
NNet_1_mse_stats <- NNet_fit_stats(pooled_panel, timeSlices, 1, "mse", batch_size = 8192, 10)
NNet_1_mse_stats[[1]]$loss_stats
NNet_1_mse_stats[[2]]$loss_stats
NNet_1_mse_stats[[3]]$loss_stats
NNet_1_mse_stats <- NNet_fit_stats(pooled_panel, timeSlices, 1, "mse", batch_size = 16384, 20)
NNet_1_mse_stats[[1]]$loss_stats
NNet_1_mse_stats[[2]]$loss_stats
NNet_1_mse_stats[[3]]$loss_stats
NNet_1_mse_stats <- NNet_fit_stats(pooled_panel, timeSlices, 1, "mse", batch_size = 32, 20)
NNet_1_mse_stats[[1]]$loss_stats
NNet_1_mse_stats[[2]]$loss_stats
NNet_1_mse_stats[[3]]$loss_stats
LM_stats_mse <- LM_fit(pooled_panel, timeSlices, "mse")
LM_stats_mse[[1]]$loss_stats
LM_stats_mse[[2]]$loss_stats
LM_stats_mse[[3]]$loss_stats
NNet_1_mse_stats[[1]]$loss_stats
NNet_1_mse_stats[[2]]$loss_stats
NNet_1_mse_stats[[3]]$loss_stats
NNet_1_mse_stats <- NNet_fit_stats(pooled_panel, timeSlices, 1, "mse", batch_size = 16, 20)
NNet_1_mse_stats[[1]]$loss_stats
NNet_1_mse_stats[[2]]$loss_stats
NNet_1_mse_stats[[3]]$loss_stats
knitr::opts_chunk$set(echo = TRUE)
################
##Load Libraries
################
library(tidyverse)
library(keras)
library(ggplot2)
library(forecast)
library(rlist)
library(plm)
library(Metrics)
library(ranger)
library(caret)
#Parallel Computing
library(foreach)
library(doFuture)
#Registering
registerDoFuture()
plan(multisession)
set.seed(27935248)
gen_W <- function(){
#Generate Lambda Matrix first
Lambda <- matrix(
data = rnorm(N*4, 0, 1),
nrow = N, ncol = 4
)
#Use Lambda to create B matrix
B <- (Lambda) %*% t(Lambda)
B <- B + 1/10*diag(nrow = nrow(B))
#Turn B into a correlation matrix
W <- diag(B)^(-1/2)*B*diag(B)^(-1/2)
return(W)
}
gen_W()
sqrt(gen_W())
summary(gen_W())
gen_W <- function(){
#Generate Lambda Matrix first
Lambda <- matrix(
data = rnorm(N*4, 0, 1),
nrow = N, ncol = 4
)
#Use Lambda to create B matrix
B <- (Lambda) %*% t(Lambda)
#B <- B + 1/10*diag(nrow = nrow(B))
#Turn B into a correlation matrix
W <- diag(B)^(-1/2)*B*diag(B)^(-1/2)
return(W)
}
summary(gen_W())
gen_W()
Lambda <- matrix(
data = rnorm(N*4, 0, 1),
nrow = N, ncol = 4
)
B <- (Lambda) %*% t(Lambda)
B <- B + 1/10*diag(nrow = nrow(B))
sqrt(diag(B))
sqrt(diag(B))*B*sqrt(diag(B))
?cov2cor
cor(B)
?cor
diag(B)^(-1/2)*B*diag(B)^(-1/2)
diag(B)
diag(B)^(-1/2)
sqrt(diag(B))
B
gen_W <- function(){
#Generate Lambda Matrix first
Lambda <- matrix(
data = rnorm(N, 0, 1),
nrow = N, ncol = 1
)
#Use Lambda to create B matrix
B <- (Lambda) %*% t(Lambda)
B <- B + 1/10*diag(nrow = nrow(B))
#Turn B into a correlation matrix
W <- diag(B)^(-1/2)*B*diag(B)^(-1/2)
return(W)
}
gen_W()
gen_W <- function(){
#Generate Lambda Matrix first
Lambda <- matrix(
data = rnorm(N, 0, 1),
nrow = N, ncol = 1
)
#Use Lambda to create B matrix
B <- (Lambda) %*% t(Lambda)
#B <- B + 1/10*diag(nrow = nrow(B))
#Turn B into a correlation matrix
W <- diag(B)^(-1/2)*B*diag(B)^(-1/2)
return(W)
}
gen_W()
gen_W <- function(){
#Generate Lambda Matrix first
Lambda <- matrix(
data = rnorm(N*4, 0, 1),
nrow = N, ncol = 4
)
#Use Lambda to create B matrix
B <- (Lambda) %*% t(Lambda)
B <- B + 1/10*diag(nrow = nrow(B))
#Turn B into a correlation matrix
W <- diag(B)^(-1/2)*B*diag(B)^(-1/2)
return(W)
}
Lambda <- matrix(
data = rnorm(N*4, 0, 1),
nrow = N, ncol = 4
)
B <- (Lambda) %*% t(Lambda)
B <- B + 1/10*diag(nrow = nrow(B))
gen_W <- function(){
#Generate Lambda Matrix first
Lambda <- matrix(
data = rnorm(N*4, 0, 1),
nrow = N, ncol = 4
)
#Use Lambda to create B matrix
B <- (Lambda) %*% t(Lambda)
B <- B + 1/10*diag(nrow = nrow(B))
#Turn B into a correlation matrix
W <- cor(B)
return(W)
}
gen_W()
summary(gen_W())
gen_W()
g1_A1_panel <- sim_panel_data(simN,
char_rho_a = 0.5, char_rho_b = 1,
cross_corr = 1, A1, xt_multi = 1, g_function = "g1", theta = matrix(c(0.007, 0.007, 0.007), nrow = 1),
error_sv = 1, error_ep_sd = 0.05,
error_omega = -0.736, error_gamma = 0.9, error_w = sqrt(0.363), error_v_sd = 0.05)
summary(sim_tune_statistics(g1_A1_panel))
knitr::opts_chunk$set(echo = TRUE)
################
##Load Libraries
################
library(tidyverse)
library(keras)
library(ggplot2)
library(forecast)
library(rlist)
library(plm)
library(Metrics)
library(tensorflow)
library(quantreg)
library(randomForestSRC)
library(hqreg)
library(caret)
#Parallel Computing
library(foreach)
library(doFuture)
#Registering
registerDoFuture()
plan(multisession)
#Create Training + Test Sets
#$T = 180$ monthly periods corresponds to 15 years. The training sample was set to start from $T = 108$ or 9 years, a validation set 1 year in length. The last 3 years were reserved as a test set never to be used for validation or training.
#This means that there are only 3 sample periods to train on, yay
#Caret and other packages are not so good at dealing with custom training/validation/test sets. Have to specify resampling, tuning and testing manually (ugh)
customTimeSlices <- function(start, end, initialWindow, horizon, test_size) {
total <- c(start:end)
offset <- start - 1
set_num <- (end - start + offset - test_size - initialWindow) / horizon
time_slice <- list(train = 0, validation = 0, test = 0)
time_slices <- rep(list(time_slice), 3)
for (t in 1:set_num) {
time_slice$train <- c(start:(initialWindow + (t-1) * horizon + offset))
time_slice$validation <- c((initialWindow + (t-1) * horizon + offset + 1):(initialWindow + (t) * horizon + offset))
time_slice$test <- c((initialWindow + (t) * horizon + offset + 1):end)
time_slices[[t]] <- time_slice
}
return(time_slices)
}
#Create custom time slices
timeSlices <- customTimeSlices(start = 2, end = 181, initialWindow = 108, horizon = 12, test_size = 36)
#Formula Function, makes it easier for those packages with a formula interface
panel_formula <- function(panel){
#Remove the first 3 colNames, as these correspond to the return, time and stock id
panel_colnames <- colnames(panel)[-c(1:3)]
f <- as.formula(c("rt ~", paste(panel_colnames, collapse = "+")))
return(f)
}
# Change data specifcation here
pooled_panel <- g1_A1_panel[[1]]$panel
pooled_panel <- g2_A1_panel[[1]]$panel
pooled_panel <- g3_A1_panel[[1]]$panel
pooled_panel <- gu_et_al_g1[[1]]$panel
f <- panel_formula(pooled_panel)
pooled_panel <- g1_A1_panel[[1]]$panel
f <- panel_formula(pooled_panel)
LM_stats_mse <- LM_fit(pooled_panel, timeSlices, "mse")
LM_stats_mae <- LM_fit(pooled_panel, timeSlices, "mae")
LM_stats_mse[[1]]$loss_stats
LM_stats_mse[[2]]$loss_stats
LM_stats_mse[[3]]$loss_stats
LM_stats_mae[[1]]$loss_stats
LM_stats_mae[[2]]$loss_stats
LM_stats_mae[[3]]$loss_stats
NNet_1_mse_stats <- NNet_fit_stats(pooled_panel, timeSlices, 1, "mse", batch_size = 32, 20)
NNet_1_mse_stats <- NNet_fit_stats(pooled_panel, timeSlices, 1, "mse", batch_size = 32, 20)
