# Rule of thumb: drop any column missing more than 20% of data
datashare %>% filter(Time > 2000) %>%
apply(2, function(col)sum(is.na(col))/length(col)) %>%
data.frame()
# Check Missing data stats
# Rule of thumb: drop any column missing more than 20% of data
datashare %>% filter(Time > 2000) %>%
apply(2, function(col)sum(is.na(col))/length(col)) %>%
data.frame() %>%
rename("" = factor) %>%
rename("." = percentage_missing)
# Check Missing data stats
# Rule of thumb: drop any column missing more than 20% of data
datashare %>% filter(Time > 2000) %>%
apply(2, function(col)sum(is.na(col))/length(col)) %>%
data.frame() %>%
rename(" " = factor) %>%
rename("." = percentage_missing)
# Check Missing data stats
# Rule of thumb: drop any column missing more than 20% of data
datashare %>% filter(Time > 2000) %>%
group_by(col) %>%
summarize(missing_share = mean(is.na(value)))
# Check Missing data stats
# Rule of thumb: drop any column missing more than 20% of data
datashare %>% filter(Time > 2000) %>%
gather(col, value) %>%
group_by(col) %>%
summarize(missing_share = mean(is.na(value)))
# Check Missing data stats
# Rule of thumb: drop any column missing more than 20% of data
datashare %>% filter(Time > 2000) %>%
gather(col, value) %>%
group_by(col) %>%
summarize(missing_share = mean(is.na(value))) %>%
filter(missing_share > 0.20)
# Check Missing data stats
# Missing data tends to get a bit better as time goes forward
# Therefore conduct this with
# Rule of thumb: drop any column missing more than 20% of data
datashare %>%
#filter(Time > 2000) %>%
gather(col, value) %>%
group_by(col) %>%
summarize(missing_share = mean(is.na(value))) %>%
filter(missing_share > 0.20)
# Check Missing data stats
# Missing data tends to get a bit better as time goes forward
# Therefore conduct this with
# Rule of thumb: drop any column missing more than 20% of data
datashare %>%
filter(Time > 1990) %>%
gather(col, value) %>%
group_by(col) %>%
summarize(missing_share = mean(is.na(value))) %>%
filter(missing_share > 0.20)
# Check Missing data stats
# Missing data tends to get a bit better as time goes forward
# Therefore conduct this with
# Rule of thumb: drop any column missing more than 20% of data
datashare %>%
filter(Time > 2000) %>%
gather(col, value) %>%
group_by(col) %>%
summarize(missing_share = mean(is.na(value))) %>%
filter(missing_share > 0.20)
# Check Missing data stats
# Missing data tends to get a bit better as time goes forward
# Therefore conduct this with
# Rule of thumb: drop any column missing more than 20% of data
datashare %>%
filter(Time > 1995) %>%
gather(col, value) %>%
group_by(col) %>%
summarize(missing_share = mean(is.na(value))) %>%
filter(missing_share > 0.20)
# Check Missing data stats
# Missing data tends to get a bit better as time goes forward
# Therefore conduct this with
# Rule of thumb: drop any column missing more than 20% of data
datashare %>%
filter(Time > 1996) %>%
gather(col, value) %>%
group_by(col) %>%
summarize(missing_share = mean(is.na(value))) %>%
filter(missing_share > 0.20)
# Double check against original RAW data
datashare_RAW %>%
mutate(Time = as.yearmon(as.character(DATE), "%Y%m%d"))
# Double check against original RAW data
datashare_RAW %>%
mutate(Time = as.yearmon(as.character(DATE), "%Y%m%d")) %>%
filter(Time > 1996) %>%
gather(col, value) %>%
group_by(col) %>%
summarize(missing_share = mean(is.na(value))) %>%
filter(missing_share > 0.20)
# Check Missing data stats
# Missing data tends to get a bit better as time goes forward
# Therefore conduct this with
# Rule of thumb: drop any column missing more than 20% of data
datashare %>%
filter(Time > 1996) %>%
gather(col, value) %>%
group_by(col) %>%
summarize(missing_share = mean(is.na(value))) %>%
filter(missing_share > 0.20)
object.size(datashare)
datashare_sample <- sample(datashare, 100000)
?sample
nrow(datashare)
datashare_sample <- sample(datashare, 100000)
210665 > 100000
datashare_sample <- sample_frac(datashare, 0.5)
object.size(datashare_sample)
?write.csv
write.csv(datashare_sample, file = "datashare_sample.csv")
# Check Missing data stats
# Missing data tends to get a bit better as time goes forward
# Therefore conduct this with
# Rule of thumb: drop any column missing more than 20% of data
datashare %>%
filter(Time > 1980) %>%
gather(col, value) %>%
group_by(col) %>%
summarize(missing_share = mean(is.na(value))) %>%
filter(missing_share > 0.20)
# Check Missing data stats
# Missing data tends to get a bit better as time goes forward
# Therefore conduct this with
# Rule of thumb: drop any column missing more than 20% of data
datashare %>%
filter(Time > 1980) %>%
gather(col, value) %>%
group_by(col) %>%
summarize(missing_share = mean(is.na(value))) %>%
filter(missing_share > 0.20) %>%
nrow()
# Check Missing data stats
# Missing data tends to get a bit better as time goes forward
# Therefore conduct this with
# Rule of thumb: drop any column missing more than 20% of data
datashare %>%
filter(Time > 1990) %>%
gather(col, value) %>%
group_by(col) %>%
summarize(missing_share = mean(is.na(value))) %>%
filter(missing_share > 0.20) %>%
nrow()
# Check Missing data stats
# Missing data tends to get a bit better as time goes forward
# Therefore conduct this with
# Rule of thumb: drop any column missing more than 20% of data
datashare %>%
filter(Time > 1996) %>%
gather(col, value) %>%
group_by(col) %>%
summarize(missing_share = mean(is.na(value))) %>%
filter(missing_share > 0.20) %>%
nrow()
datashare <- datashare_RAW_join %>%
mutate(Time = as.yearmon(as.character(DATE), "%Y%m%d")) %>%
select(-DATE) %>%
# Generate log prices
# Drop RET which is monthly return
select(Time, permno, PRC, -RET, everything()) %>%
# Filter so that we only have NASDAQ stocks
# Remember that the NASDAQ only opened in 1971 ish
filter(PRIMEXCH == "Q") %>%
# Negative PRC means that there was no closing price, and the dash denotes the bid ask average instead
# Not too unreasonable to just take this as the actual price
# Note that PRC has not been corrected for stock splits etc. This is more sensible for modelling purposes
mutate(PRC = abs(PRC)) %>%
# Prices of 0 mean that neither the bid ask average or price were available, ie missing
filter(PRC != 0) %>%
# Filter out PRC < 5 to get rid of penny stocks
filter(PRC >= 5) %>%
# SIC codes according to WRDS/CRSP are recommended to be used with caution, as they are not strictly enforced
# Companies can also belong in multiple SIC codes, or change SIC codes over time. This is not adequately captured by CRSP
# Also don't make much consistent sense either
# Personal decision: drop them entirely
select(-sic2) %>%
# We will construct quarterly returns using the end of each month
# Ie using the 3rd, 6th, 9th and 12th month of each year
# yearmon stores year + 0 for January. Exploit this to filter out non-quarter months
filter((as.yearmon(Time) %% 3) == 2/12 | (as.yearmon(Time) %% 3) == 5/12 | (as.yearmon(Time) %% 3) == 8/12 | (as.yearmon(Time) %% 3) == 11/12) %>%
# Generate log of prices
mutate(logPRC = log(PRC))
# Check Missing data stats
# Missing data tends to get a bit better as time goes forward
# Therefore conduct this with
# Rule of thumb: drop any column missing more than 20% of data
datashare %>%
filter(Time > 1980) %>%
gather(col, value) %>%
group_by(col) %>%
summarize(missing_share = mean(is.na(value))) %>%
filter(missing_share > 0.20) %>%
nrow()
View(datashare_RET_RAW)
View(datashare_RET_RAW)
# datashare with RET, Prices, and primary exchange
datashare_RET_RAW <- read_csv("data/datashare/datashare_PRC.csv") %>%
# Rename some columns
mutate(DATE = date) %>%
select(-date) %>%
mutate(permno = PERMNO) %>%
select(-PERMNO)
datashare_RAW_join <- full_join(datashare_RAW, datashare_RET_RAW, by = c("permno", "DATE")) %>%
# Reorder variables
select(DATE, permno, RET, PRC, everything())
summary(datashare_RAW_join$SHRCD)
datashare <- datashare_RAW_join %>%
mutate(Time = as.yearmon(as.character(DATE), "%Y%m%d")) %>%
select(-DATE) %>%
# Generate log prices
# Drop RET which is monthly return
select(Time, permno, PRC, -RET, everything()) %>%
# Filter so that we only have NASDAQ stocks
# Remember that the NASDAQ only opened in 1971 ish
filter(PRIMEXCH == "Q") %>%
# Negative PRC means that there was no closing price, and the dash denotes the bid ask average instead
# Not too unreasonable to just take this as the actual price
# Note that PRC has not been corrected for stock splits etc. This is more sensible for modelling purposes
mutate(PRC = abs(PRC)) %>%
# Prices of 0 mean that neither the bid ask average or price were available, ie missing
filter(PRC != 0) %>%
# Filter out PRC < 5 to get rid of penny stocks
filter(PRC >= 5) %>%
# Filter out so that only shares with share codes of 10 or 11 are included (other codes refer to other instruments such as REITs, etc)
# Don't have any idea why Gu et al thought this was originally sensible
filter(SHRCD == 10 | SHRCD == 11) %>%
# SIC codes according to WRDS/CRSP are recommended to be used with caution, as they are not strictly enforced
# Companies can also belong in multiple SIC codes, or change SIC codes over time. This is not adequately captured by CRSP
# Also don't make much consistent sense either
# Personal decision: drop them entirely
select(-sic2) %>%
# We will construct quarterly returns using the end of each month
# Ie using the 3rd, 6th, 9th and 12th month of each year
# yearmon stores year + 0 for January. Exploit this to filter out non-quarter months
filter((as.yearmon(Time) %% 3) == 2/12 | (as.yearmon(Time) %% 3) == 5/12 | (as.yearmon(Time) %% 3) == 8/12 | (as.yearmon(Time) %% 3) == 11/12) %>%
# Generate log of prices
mutate(logPRC = log(PRC))
object.size
object.size(datashare)
# Check Missing data stats
# Missing data tends to get a bit better as time goes forward
# Therefore conduct this with
# Rule of thumb: drop any column missing more than 20% of data
datashare %>%
filter(Time > 1980) %>%
gather(col, value) %>%
group_by(col) %>%
summarize(missing_share = mean(is.na(value))) %>%
filter(missing_share > 0.20) %>%
nrow()
# Check Missing data stats
# Missing data tends to get a bit better as time goes forward
# Therefore conduct this with
# Rule of thumb: drop any column missing more than 20% of data
datashare %>%
filter(Time > 1990) %>%
gather(col, value) %>%
group_by(col) %>%
summarize(missing_share = mean(is.na(value))) %>%
filter(missing_share > 0.20) %>%
nrow()
# Check Missing data stats
# Missing data tends to get a bit better as time goes forward
# Therefore conduct this with
# Rule of thumb: drop any column missing more than 20% of data
datashare %>%
filter(Time > 1996) %>%
gather(col, value) %>%
group_by(col) %>%
summarize(missing_share = mean(is.na(value))) %>%
filter(missing_share > 0.20) %>%
nrow()
# Check Missing data stats
# Missing data tends to get a bit better as time goes forward
# Therefore conduct this with
# Rule of thumb: drop any column missing more than 20% of data
datashare %>%
filter(Time > 1991) %>%
gather(col, value) %>%
group_by(col) %>%
summarize(missing_share = mean(is.na(value))) %>%
filter(missing_share > 0.20) %>%
nrow()
datashare_sample <- sample_frac(datashare, 0.5)
write.csv(datashare_sample, file = "datashare_sample.csv")
# Check Missing data stats
# Missing data tends to get a bit better as time goes forward
# Therefore conduct this with
# Rule of thumb: drop any column missing more than 20% of data
datashare %>%
filter(Time > 1992) %>%
gather(col, value) %>%
group_by(col) %>%
summarize(missing_share = mean(is.na(value))) %>%
filter(missing_share > 0.20) %>%
nrow()
# Check Missing data stats
# Missing data tends to get a bit better as time goes forward
# Therefore conduct this with
# Rule of thumb: drop any column missing more than 20% of data
datashare %>%
filter(Time > 1993) %>%
gather(col, value) %>%
group_by(col) %>%
summarize(missing_share = mean(is.na(value))) %>%
filter(missing_share > 0.20) %>%
nrow()
# Check Missing data stats
# Missing data tends to get a bit better as time goes forward
# Therefore conduct this with
# Rule of thumb: drop any column missing more than 20% of data
datashare %>%
filter(Time > 1994) %>%
gather(col, value) %>%
group_by(col) %>%
summarize(missing_share = mean(is.na(value))) %>%
filter(missing_share > 0.20) %>%
nrow()
# Check Missing data stats
# Missing data tends to get a bit better as time goes forward
# Therefore conduct this with
# Rule of thumb: drop any column missing more than 20% of data
datashare %>%
filter(Time > 1995) %>%
gather(col, value) %>%
group_by(col) %>%
summarize(missing_share = mean(is.na(value))) %>%
filter(missing_share > 0.20) %>%
nrow()
# Check Missing data stats
# Missing data tends to get a bit better as time goes forward
# Therefore conduct this with
# Rule of thumb: drop any column missing more than 20% of data
datashare %>%
filter(Time > 1996) %>%
gather(col, value) %>%
group_by(col) %>%
summarize(missing_share = mean(is.na(value))) %>%
filter(missing_share > 0.20) %>%
nrow()
# Check Missing data stats
# Missing data tends to get a bit better as time goes forward
# Therefore conduct this with
# Rule of thumb: drop any column missing more than 20% of data
datashare %>%
filter(Time > 1996) %>%
gather(col, value) %>%
group_by(col) %>%
summarize(missing_share = mean(is.na(value))) %>%
filter(missing_share > 0.20)
gen_error <- function(sv,
#epsilon sd. Note that this is the normal sd in sv case,
#and student t sd in simple case
ep_sd,
#sv parameters
omega, gamma, w,
#Beta_v Error parameters
C, v_sd){
#Initialize
error <- array(data = 0, dim = c(N, 1, Time))
##Beta_v component, add this on to the other error component at the end
Beta <- C[, 1:3, ]
Beta_v <- array(0, dim = c(N, 1, Time))
for (t in 1:(Time)) {
v <- matrix(data = rnorm(3, mean = 0, sd = v_sd),
nrow = 3,
ncol = 1)
for (i in 1:N) {
Beta_v[i, 1, t] <- Beta[i, , t] %*% v
}
}
###########
#SV errors
###########
if (sv == 1) {
##Generate Sigma first (only indexed by time)
logsigma2 <- rep(0, Time)
#Initial sigma2
logsigma2[1] <- omega + rnorm(1, 0, w)
for (t in 2:(Time)) {
logsigma2[t] <- omega + gamma*logsigma2[t-1] + rnorm(1, 0, w)
}
for (t in 1:(Time)) {
for (i in 1:N) {
error[i, 1, t] <- sqrt(exp(logsigma2[t])) * rnorm(1, 0, ep_sd)
}
}
return(0)
}
##################
#NON-SV Student t errors
##################
else {
for (t in 1:(Time)) {
error[, , t] <- matrix(data = rt(N, df = 5) * sqrt(ep_sd^2 * (5-2)/5), nrow = N)
}
return(error + Beta_v)
}
}
g1_A1_panel <- sim_panel_data(simN,
char_rho_a = 0.5, char_rho_b = 1,
cross_corr = 1, A1, xt_multi = 1, g_function = "g1", theta = matrix(c(0.02, 0.02, 0.02), nrow = 1),
error_sv = 1, error_ep_sd = 1,
error_omega = -0.98, error_gamma = 0.95, error_w = 0.2, error_v_sd = 0.05,
predictor_format = "kronecker")
summary(sim_tune_statistics(g1_A1_panel))
gen_error <- function(sv,
#epsilon sd. Note that this is the normal sd in sv case,
#and student t sd in simple case
ep_sd,
#sv parameters
omega, gamma, w,
#Beta_v Error parameters
C, v_sd){
#Initialize
error <- array(data = 0, dim = c(N, 1, Time))
##Beta_v component, add this on to the other error component at the end
Beta <- C[, 1:3, ]
Beta_v <- array(0, dim = c(N, 1, Time))
for (t in 1:(Time)) {
v <- matrix(data = rnorm(3, mean = 0, sd = v_sd),
nrow = 3,
ncol = 1)
for (i in 1:N) {
Beta_v[i, 1, t] <- Beta[i, , t] %*% v
}
}
###########
#SV errors
###########
if (sv == 1) {
##Generate Sigma first (only indexed by time)
logsigma2 <- rep(0, Time)
#Initial sigma2
logsigma2[1] <- omega + rnorm(1, 0, w)
for (t in 2:(Time)) {
logsigma2[t] <- omega + gamma*logsigma2[t-1] + rnorm(1, 0, w)
}
for (t in 1:(Time)) {
for (i in 1:N) {
error[i, 1, t] <- sqrt(exp(logsigma2[t])) * rnorm(1, 0, ep_sd)
}
}
return(Beta_v + error)
}
##################
#NON-SV Student t errors
##################
else {
for (t in 1:(Time)) {
error[, , t] <- matrix(data = rt(N, df = 5) * sqrt(ep_sd^2 * (5-2)/5), nrow = N)
}
return(error + Beta_v)
}
}
g1_A1_panel <- sim_panel_data(simN,
char_rho_a = 0.5, char_rho_b = 1,
cross_corr = 1, A1, xt_multi = 1, g_function = "g1", theta = matrix(c(0.02, 0.02, 0.02), nrow = 1),
error_sv = 1, error_ep_sd = 1,
error_omega = -0.98, error_gamma = 0.95, error_w = 0.2, error_v_sd = 0.05,
predictor_format = "kronecker")
summary(sim_tune_statistics(g1_A1_panel))
g1_A1_panel <- sim_panel_data(simN,
char_rho_a = 0.5, char_rho_b = 1,
cross_corr = 1, A1, xt_multi = 1, g_function = "g1", theta = matrix(c(0.02, 0.02, 0.02), nrow = 1),
error_sv = 1, error_ep_sd = 1,
error_omega = -0.1, error_gamma = 0.95, error_w = 0.2, error_v_sd = 0.05,
predictor_format = "kronecker")
summary(sim_tune_statistics(g1_A1_panel))
g1_A1_panel <- sim_panel_data(simN,
char_rho_a = 0.5, char_rho_b = 1,
cross_corr = 1, A1, xt_multi = 1, g_function = "g1", theta = matrix(c(0.02, 0.02, 0.02), nrow = 1),
error_sv = 1, error_ep_sd = 1,
error_omega = -1.5, error_gamma = 0.95, error_w = 0.2, error_v_sd = 0.05,
predictor_format = "kronecker")
summary(sim_tune_statistics(g1_A1_panel))
# Check Missing data stats
# Missing data tends to get a bit better as time goes forward
# Therefore conduct this with
# Rule of thumb: drop any column missing more than 20% of data
datashare %>%
filter(Time > 1980) %>%
gather(col, value) %>%
group_by(col) %>%
summarize(missing_share = mean(is.na(value))) %>%
filter(missing_share > 0.25)
# Check Missing data stats
# Missing data tends to get a bit better as time goes forward
# Therefore conduct this with
# Rule of thumb: drop any column missing more than 20% of data
datashare %>%
filter(Time > 1980) %>%
gather(col, value) %>%
group_by(col) %>%
summarize(missing_share = mean(is.na(value))) %>%
filter(missing_share > 0.25) %>%
nrow()
datashare %>%
# Change time filtering here
filter(Time > 1990) %>%
gather(col, value) %>%
group_by(col) %>%
summarize(missing_share = mean(is.na(value))) %>%
filter(missing_share > 0.25) %>%
# Delete or comment this last nrow() if you want to see which specific factors
nrow()
