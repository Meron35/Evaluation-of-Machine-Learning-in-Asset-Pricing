---
title: "Simulation Models"
author: "Ze Yu Zhong"
date: "19 June 2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

################
##Load Libraries
################

library(glmnet)
library(tidyverse)
library(keras)
library(ggplot2)
library(forecast)
library(rlist)
library(plm)
library(Metrics)
library(ranger)
library(glmnetUtils)
library(tensorflow)
library(quantreg)
library(randomForestSRC)
library(hqreg)
library(devtools)
library(forestr)


#forestr
# This apparently needs admin privileges to install, do this at home maybe
# devtools::install_github("andeek/forestr")
# devtools::install_github("andeek/rpart")

# unlink("C:/R/Library/00LOCK-rpart", recursive = TRUE)

set.seed(27935248)
```

```{r train_valid_test}
#Create Training + Test Sets

#$T = 180$ monthly periods corresponds to 15 years. The training sample was set to start from $T = 108$ or 9 years, a validation set 1 year in length. The last 3 years were reserved as a test set never to be used for validation or training.

#This means that there are only 3 sample periods to train on, yay

#Caret and other packages are not so good at dealing with custom training/validation/test sets. Have to specify resampling, tuning and testing manually (ugh)

customTimeSlices <- function(start, end, initialWindow, horizon, test_size) {
  total <- c(start:end)
  offset <- start - 1
  set_num <- (end - start + offset - test_size - initialWindow) / horizon
  
  time_slice <- list(train = 0, validation = 0, test = 0)
  time_slices <- rep(list(time_slice), 3)

  for (t in 1:set_num) {
    time_slice$train <- c(start:(initialWindow + (t-1) * horizon + offset))
    time_slice$validation <- c((initialWindow + (t-1) * horizon + offset + 1):(initialWindow + (t) * horizon + offset))
    time_slice$test <- c((initialWindow + (t) * horizon + offset + 1):end)
    
    time_slices[[t]] <- time_slice
  }
  return(time_slices)
}

#Create custom time slices
timeSlices <- customTimeSlices(start = 2, end = 181, initialWindow = 108, horizon = 12, test_size = 36)

#Formula Function, makes it easier for those packages with a formula interface

panel_formula <- function(panel){
  #Remove the first 3 colNames, as these correspond to the return, time and stock id
  panel_colnames <- colnames(panel)[-c(1:3)]
  f <- as.formula(c("rt ~", paste(panel_colnames, collapse = "+")))
  return(f)
}

#Use panel_g1_A1 generated from Simulation.rmd for now. Generalize it later

pooled_panel <- gu_et_al_g1[[1]]$panel

f <- panel_formula(pooled_panel)
```

```{r pooled_ols}
#Linear model wrt MSE

LM_fit <- function(pooled_panel, timeSlices, loss_function) {
  #Initialize Loss Function Statistics
  LM_stats <- rep(list(0), 3)
  
  for (set in 1:3) {
    LM_stats[[set]] <- list(loss_stats = data.frame(train_MAE = 0, train_MSE = 0, train_RMSE = 0, train_RSquare = 0, 
                                                    validation_MAE = 0, validation_MSE = 0, validation_RMSE = 0, validation_RSquare = 0, 
                                                    test_MAE = 0, test_MSE = 0, test_RMSE = 0, test_RSquare = 0),
                            #Other useful things
                            forecasts = 0,
                            forecast_resids = 0,
                            model = 0)
    
    #Load Training, validation and test sets
    train <- pooled_panel %>%
      filter(time %in% timeSlices[[set]]$train)
    validation <- pooled_panel %>%
      filter(time %in% timeSlices[[set]]$validation)
    test <- pooled_panel %>%
      filter(time %in% timeSlices[[set]]$test)
    
    #Train Model on training set
    
    #MSE case
    if (loss_function == "mse") {
      lm <- lm(f, data = train)
    } else {
      lm <- rq(f, data = train, tau = 0.5, method = "br")
    }
    
    LM_stats[[set]]$model <- lm
    
    #No Tuning Needed
    
    #Statistics
    #Training Set
    train_predict <- predict(lm)
    LM_stats[[set]]$loss_stats$train_MAE <- mae(train$rt, train_predict)
    LM_stats[[set]]$loss_stats$train_MSE <- mse(train$rt, train_predict)
    LM_stats[[set]]$loss_stats$train_RMSE <- rmse(train$rt, train_predict)
    LM_stats[[set]]$loss_stats$train_RSquare <- (1 - sse(train$rt, train_predict) / sum((train$rt - mean(train$rt))^2))
    
    #Validation Set Statistics
    validation_predict <- predict(lm, newdata = validation)
    LM_stats[[set]]$loss_stats$validation_MAE <- mae(validation$rt, validation_predict)
    LM_stats[[set]]$loss_stats$validation_MSE <- mse(validation$rt, validation_predict)
    LM_stats[[set]]$loss_stats$validation_RMSE <- rmse(validation$rt, validation_predict)
    LM_stats[[set]]$loss_stats$validation_RSquare <- (1 - sse(validation$rt, validation_predict) / sum((validation$rt - mean(validation$rt))^2))
      
    #Test Set Statistics
    test_predict <- predict(lm, newdata = test)
    LM_stats[[set]]$loss_stats$test_MAE <- mae(test$rt, test_predict)
    LM_stats[[set]]$loss_stats$test_MSE <- mse(test$rt, test_predict)
    LM_stats[[set]]$loss_stats$test_RMSE <- rmse(test$rt, test_predict)
    LM_stats[[set]]$loss_stats$test_RSquare <- (1 - sse(test$rt, test_predict) / sum((test$rt - mean(test$rt))^2))
        
    #Forecasts
    LM_stats[[set]]$forecasts <- test_predict
    #Forecast Residuals
    LM_stats[[set]]$forecast_resids <- test$rt - test_predict
  }
  return(LM_stats)
}

LM_stats_mse <- LM_fit(pooled_panel, timeSlices, "mse")

LM_stats_mae <- LM_fit(pooled_panel, timeSlices, "mae")

summary(LM_stats_mse[[1]]$model)

LM_stats_mse[[1]]$loss_stats
LM_stats_mse[[2]]$loss_stats
LM_stats_mse[[3]]$loss_stats

summary(LM_stats_mae[[1]]$model)

LM_stats_mae[[1]]$loss_stats
LM_stats_mae[[2]]$loss_stats
LM_stats_mae[[3]]$loss_stats
```

```{r elastic_net, eval = FALSE}
## DO NOT RUN THIS CHUNK. ITS IMPLEMENTATION IS VERY INTENSIVE AND SLOW. NOTE EVAL = FALSE FLAG
# KEPT HERE MAINLY FOR LEGACY PURPOSES

#Elasticnet WRT MSE

# Can't seem to find anything online that tunes ELN for you with specified validation sets, instead of straight-forward cross validation
# nfold and foldid require that ELN be performed with at least 3 fold validation - not possible in our context

# Elasticnet only has alpha and lambda to tune

# ELN is rather finicky and likes to generate its own sequence of lambda values during the actual fitting process. Really doesn't like specifying a singular lambda value
# Solution: follow what the documentation says and let it find its own lambda sequence
# Then, when supply custom grid values of lambda when actually predicting values. This can be either through exact = TRUE (refits the model with the additional lambda gridpoint), or exact = FALSE, which just uses linear interpolation
# According to documentation, linear interpolation is very fast and is generally "good enough" anyway

# Just keep the grid small for now to make sure everything is running
ELN_grid <- expand.grid(
  alpha = seq(0, 1, 0.01),
  lambda = 10^seq(-2, 2, length.out = 50)
)

######################################
## Functions to tune an ELN model
######################################

# Fit the model over the defined grid of hyperparameters, and return it as a list
# Takes a defined grid of hyperparameters, a training set, and a validation set, fits the model over the grid, and returns the models + loss statistics as a list
# Works with both loss functions of MSE and MAE, need to specify these as "mse" or "mae" respectively

# Ideally, we would store all the model fitted over the grid. This takes up way too much memory, and because ELN is very efficiently fit anyway it's better not to store them. Relevant lines are commented out

ELN_model_grid <- function(ELN_grid, train_x, train_y, validation_x, validation_y, loss_function, nlambda) {
  ELN_model_grid <- rep(list(0), nrow(ELN_grid))
  
  for (i in 1:nrow(ELN_grid)) {
    
    ELN_model_grid[[i]] <- list(ELN_grid = 0, model = 0)
    
    # MSE Case
    if (loss_function == "mse") {
      ELN <- hqreg_raw(train_x, train_y, method = "ls", alpha = ELN_grid$alpha[i], nlambda = nlambda)
      
      #ELN_model_grid[[i]]$model <- ELN
      
      ELN_model_grid[[i]]$ELN_grid <- cbind(alpha = ELN_grid$alpha[i], lambda = ELN_grid$lambda[i],
                                            train_loss = mse(train_y, predict(ELN, train_x, lambda = ELN_grid$lambda[i])),
                                            validation_loss = mse(validation_y, predict(ELN, validation_x, lambda = ELN_grid$lambda[i]))
                                            )
    } else {
      # MAE Case
      ELN <- hqreg_raw(train_x, train_y, method = "quantile", tau = 0.5, alpha = ELN_grid$alpha[i], nlambda = nlambda)
      #ELN_model_grid[[i]]$model <- ELN

      ELN_model_grid[[i]]$ELN_grid <- cbind(alpha = ELN_grid$alpha[i], lambda = ELN_grid$lambda[i],
                                            train_loss = mae(train_y, predict(ELN, train_x, lambda = ELN_grid$lambda[i])),
                                            validation_loss = mae(validation_y, predict(ELN, validation_x, lambda = ELN_grid$lambda[i]))
                                            )
    }
  }
  return(ELN_model_grid)
}

get_ELN_best_tune <- function(model_grid) {
  ELN_tune_dataframe <- cbind(model_grid[[1]]$ELN_grid, list_index = 1)
  for (i in 2:length(model_grid)) {
    ELN_tune_dataframe <- rbind(ELN_tune_dataframe, cbind(model_grid[[i]]$ELN_grid, list_index = i))
  }
  ELN_tune_dataframe <- data.frame(ELN_tune_dataframe)
  return(ELN_tune_dataframe[which.min(ELN_tune_dataframe$validation_loss), ])
}

# Final big wrapper function that gives an ELN_stats list with everything desired

ELN_fit_stats <- function(ELN_grid, nlambda, timeSlices, pooled_panel, loss_function) {
  ELN_stats <- rep(list(0), 3)
  
  for (set in 1:3) {
    #Load Training, validation and test sets
    train <- pooled_panel %>%
      filter(time %in% timeSlices[[set]]$train)
    validation <- pooled_panel %>%
      filter(time %in% timeSlices[[set]]$validation)
    test <- pooled_panel %>%
      filter(time %in% timeSlices[[set]]$test)
    
    train_x <- as.matrix(train[4:ncol(train)])
    train_y <- as.matrix(train$rt)
    validation_x <- as.matrix(validation[4:ncol(validation)])
    validation_y <- as.matrix(validation$rt)
    test_x <- as.matrix(test[4:ncol(test)])
    test_y <- as.matrix(test$rt)
      
    #Get models fit over the grid of hyperparameters
    ELN_model_grid <- ELN_model_grid(ELN_grid, train_x, train_y, validation_x, validation_y, loss_function, nlambda = nlambda)
        
    #Get the best tuning parameters
    best_model_params <- get_ELN_best_tune(ELN_model_grid)
    
    #Initialize stats list
    ELN_stats[[set]] <- list(loss_stats = data.frame(train_MAE = 0, train_MSE = 0, train_RMSE = 0, train_RSquare = 0, 
                                                     validation_MAE = 0, validation_MSE = 0, validation_RMSE = 0, validation_RSquare = 0, 
                                                     test_MAE = 0, test_MSE = 0, test_RMSE = 0, test_RSquare = 0),
                             #Other useful things
                             forecasts = 0,
                             forecast_resids = 0,
                             model = 0)
      
    #Model
    if (loss_function == "mse") {
      model <- hqreg_raw(train_x, train_y, method = "ls", alpha = best_model_params$alpha, nlambda = nlambda)
    } else {
      model <- hqreg_raw(train_x, train_y, method = "quantile", tau = 0.5, alpha = best_model_params$alpha, nlambda = nlambda)
    }
    ELN_stats[[set]]$model <- model
      
    #Loss Stats Dataframe
    #Train
    train_predict <- predict(model, train_x, alpha = best_model_params$alpha, lambda = best_model_params$lambda)
    ELN_stats[[set]]$loss_stats$train_MAE <- mae(train_y, train_predict)
    ELN_stats[[set]]$loss_stats$train_MSE <- mse(train_y, train_predict)
    ELN_stats[[set]]$loss_stats$train_RMSE <- rmse(train_y, train_predict)
    ELN_stats[[set]]$loss_stats$train_RSquare <- (1 - sse(train_y, train_predict) / sum((train_y - mean(train_y))^2))
        
    #Validation
    valid_predict <- predict(model, validation_x, alpha = best_model_params$alpha, lambda = best_model_params$lambda)
    ELN_stats[[set]]$loss_stats$validation_MAE <- mae(validation_y, valid_predict)
    ELN_stats[[set]]$loss_stats$validation_MSE <- mse(validation_y, valid_predict)
    ELN_stats[[set]]$loss_stats$validation_RMSE <- rmse(validation_y, valid_predict)
    ELN_stats[[set]]$loss_stats$validation_RSquare <- (1 - sse(validation_y, valid_predict) / sum((validation_y - mean(validation_y))^2))
    
    #Test
    test_predict <- predict(model, test_x, alpha = best_model_params$alpha, lambda = best_model_params$lambda)
    ELN_stats[[set]]$loss_stats$test_MAE <- mae(test_y, test_predict)
    ELN_stats[[set]]$loss_stats$test_MSE <- mse(test_y, test_predict)
    ELN_stats[[set]]$loss_stats$test_RMSE <- rmse(test_y, test_predict)
    ELN_stats[[set]]$loss_stats$test_RSquare <- (1 - sse(test_y, test_predict) / sum((test_y - mean(test_y))^2))
      
    #Forecasts
    ELN_stats[[set]]$forecasts <- test_predict
    
    #Forecast residuals
    ELN_stats[[set]]$forecast_resids <- test_y - test_predict
  }
  return(ELN_stats)
}

#Testing if function works

ELN_stats_mae <- ELN_fit_stats(ELN_grid, nlambda = 100, timeSlices, pooled_panel, loss_function = "mae")

ELN_stats_mse <- ELN_fit_stats(ELN_grid, nlambda = 100, timeSlices, pooled_panel, loss_function = "mse")

ELN_stats_mae[[1]]$model$alpha
ELN_stats_mae[[2]]$model$alpha
ELN_stats_mae[[3]]$model$alpha

ELN_stats_mse[[1]]$model$alpha
ELN_stats_mse[[2]]$model$alpha
ELN_stats_mse[[3]]$model$alpha

# Optimal alpha vlaues seem to be very unstable across folds
```

```{r elastic_net}
# Alternative implementation for ELN
# This is much, much faster
# DO NOT USE THE PREVIOUS IMPLEMENTATION
# Do not specify a grid for lambda. Instead, let the algorithm determine its own sequence of lambda
# Method: specify only a grid for alpha.

alpha_grid <- seq(0, 1, 0.01)

ELN_model_grid <- function(alpha_grid, train_x, train_y, validation_x, validation_y, loss_function, nlamb) {
  #Initialize List
  ELN_model_grid <- rep(list(0), length(alpha_grid))
  
  for (i in 1:(length(alpha_grid))) {
    
    ELN_model_grid[[i]] <- list(ELN_grid = 0, model = 0)
    
    #MSE Case
    if (loss_function == "mse") {
      ELN <- hqreg_raw(train_x, train_y, method = "ls", alpha = alpha_grid[i], nlambda = nlamb)
      ELN_model_grid[[i]]$model <- ELN
    
      ELN_model_grid[[i]]$ELN_grid <- cbind(alpha = alpha_grid[i], lambda = ELN$lambda,
                                            train_loss = apply(predict(ELN, train_x), 2, mse, actual = train_y), 
                                            validation_loss = apply(predict(ELN, validation_x), 2, mse, actual = validation_y))
    } else {
      #MAE Case
      ELN <- hqreg_raw(train_x, train_y, method = "quantile", tau = 0.5, alpha = alpha_grid[i], nlambda = nlamb)
      ELN_model_grid[[i]]$model <- ELN
    
      ELN_model_grid[[i]]$ELN_grid <- cbind(alpha = alpha_grid[i], lambda = ELN$lambda,
                                            train_loss = apply(predict(ELN, train_x), 2, mse, actual = train_y), 
                                            validation_loss = apply(predict(ELN, validation_x), 2, mae, actual = validation_y))
    }
  }
  return(ELN_model_grid)
}

# Given a list containing the model grid, return the "best" model according to validation loss
# Use in conjunction with previous function

get_ELN_best_tune <- function(model_grid) {
  ELN_tune_dataframe <- cbind(model_grid[[1]]$ELN_grid, list_index = 1)
  for (i in 2:length(model_grid)) {
    ELN_tune_dataframe <- rbind(ELN_tune_dataframe, cbind(model_grid[[i]]$ELN_grid, list_index = i))
  }
  ELN_tune_dataframe <- data.frame(ELN_tune_dataframe)
  return(ELN_tune_dataframe[which.min(ELN_tune_dataframe$validation_loss), ])
}

# Final big wrapper function that gives an ELN_stats list with everything desired

ELN_fit_stats <- function(alpha_grid, nlamb, timeSlices, pooled_panel, loss_function) {
  ELN_stats <- rep(list(0), 3)
  
  for (set in 1:3) {
    #Load Training, validation and test sets
    train <- pooled_panel %>%
      filter(time %in% timeSlices[[set]]$train)
    validation <- pooled_panel %>%
      filter(time %in% timeSlices[[set]]$validation)
    test <- pooled_panel %>%
      filter(time %in% timeSlices[[set]]$test)
    
    train_x <- as.matrix(train[4:ncol(train)])
    train_y <- as.matrix(train$rt)
    validation_x <- as.matrix(validation[4:ncol(validation)])
    validation_y <- as.matrix(validation$rt)
    test_x <- as.matrix(test[4:ncol(test)])
    test_y <- as.matrix(test$rt)
      
    #Get models fit over the grid of hyperparameters
    ELN_model_grid <- ELN_model_grid(alpha_grid, train_x, train_y, validation_x, validation_y, loss_function, nlamb = nlamb)
        
    #Get the best tuning parameters
    best_model_params <- get_ELN_best_tune(ELN_model_grid)
    
    #Initialize stats list
    ELN_stats[[set]] <- list(loss_stats = data.frame(train_MAE = 0, train_MSE = 0, train_RMSE = 0, train_RSquare = 0, 
                                                     validation_MAE = 0, validation_MSE = 0, validation_RMSE = 0, validation_RSquare = 0, 
                                                     test_MAE = 0, test_MSE = 0, test_RMSE = 0, test_RSquare = 0),
                             #Other useful things
                             forecasts = 0,
                             forecast_resids = 0,
                             model = 0)
      
    #Model
    model <- ELN_model_grid[[best_model_params$list_index]]$model
    ELN_stats[[set]]$model <- model
      
    #Loss Stats Dataframe
    #Train
    train_predict <- predict(model, train_x, alpha = best_model_params$alpha, lambda = best_model_params$lambda)
    ELN_stats[[set]]$loss_stats$train_MAE <- mae(train_y, train_predict)
    ELN_stats[[set]]$loss_stats$train_MSE <- mse(train_y, train_predict)
    ELN_stats[[set]]$loss_stats$train_RMSE <- rmse(train_y, train_predict)
    ELN_stats[[set]]$loss_stats$train_RSquare <- (1 - sse(train_y, train_predict) / sum((train_y - mean(train_y))^2))
        
    #Validation
    valid_predict <- predict(model, validation_x, alpha = best_model_params$alpha, lambda = best_model_params$lambda)
    ELN_stats[[set]]$loss_stats$validation_MAE <- mae(validation_y, valid_predict)
    ELN_stats[[set]]$loss_stats$validation_MSE <- mse(validation_y, valid_predict)
    ELN_stats[[set]]$loss_stats$validation_RMSE <- rmse(validation_y, valid_predict)
    ELN_stats[[set]]$loss_stats$validation_RSquare <- (1 - sse(validation_y, valid_predict) / sum((validation_y - mean(validation_y))^2))
    
    #Test
    test_predict <- predict(model, test_x, alpha = best_model_params$alpha, lambda = best_model_params$lambda)
    ELN_stats[[set]]$loss_stats$test_MAE <- mae(test_y, test_predict)
    ELN_stats[[set]]$loss_stats$test_MSE <- mse(test_y, test_predict)
    ELN_stats[[set]]$loss_stats$test_RMSE <- rmse(test_y, test_predict)
    ELN_stats[[set]]$loss_stats$test_RSquare <- (1 - sse(test_y, test_predict) / sum((test_y - mean(test_y))^2))
      
    #Forecasts
    ELN_stats[[set]]$forecasts <- predict(model, test_x, alpha = best_model_params$alpha, lambda = best_model_params$lambda)
    
    #Forecast residuals
    ELN_stats[[set]]$forecast_resids <- test_y - predict(model, test_x, alpha = best_model_params$alpha, lambda = best_model_params$lambda)
  }
  return(ELN_stats)
}

#Testing if function works

ELN_stats_mae <- ELN_fit_stats(alpha_grid, nlamb = 100, timeSlices, pooled_panel, loss_function = "mae")

ELN_stats_mse <- ELN_fit_stats(alpha_grid, nlamb = 100, timeSlices, pooled_panel, loss_function = "mse")

# Seeing if different loss functions actuallly do anything (they should)
# Note: make sure the grid is sufficiently large enough, otherwise the two different loss functions will often agree on the exact same hyperparameters

ELN_stats_mse[[1]]$loss_stats$test_MSE
ELN_stats_mse[[1]]$model$alpha

ELN_stats_mse[[2]]$loss_stats$test_MSE
ELN_stats_mse[[2]]$model$alpha

ELN_stats_mse[[3]]$loss_stats$test_MSE
ELN_stats_mse[[3]]$model$alpha

ELN_stats_mae[[1]]$loss_stats$test_MAE
ELN_stats_mae[[1]]$model$alpha

ELN_stats_mae[[2]]$loss_stats$test_MAE
ELN_stats_mae[[2]]$model$alpha

ELN_stats_mae[[3]]$loss_stats$test_MAE
ELN_stats_mae[[3]]$model$alpha

# Notes
# Mostly consistent results with the previous implementation, very minor differences in optimal alpha values chosen
# Similarly, optimal alpha values are highly unstable (maybe did the simulation aspect wrong?)

```

```{r random_forest}
#Random Forest

#Random Forest should also be fairly straightforward, only two tuning parameters mtry and ntree (usually)

#The vanilla randomForest package is very basic and too slow
#We will use the ranger package, which is apparently a C++ implementation of the random forest algorithm and should be faster
#Another alternative is to use XGBoost (which is very very fast) and set boosting rounds = 1, because in this case it should be a standard random forest

#Compute power is a real struggle, and the aim is just to get the code basis working for the moment. Very conservative grids are therefore used for now

#Check out randomForestSRC for an implementation of randomForests that allows for quantile regression trees. Quantile regression use a check loss function, which is a superset of absolute error
#forestr package doesn't seem to want to function on honours lab computers

#Default ntree = 500
#Default mtry = count(predictors/3) = 400/3 = 133

RF_grid <- expand.grid(
  #ntree usually isn't tuned. Just set to max of computationally feasible
  ntree = 100,
  mtry = seq(10, round(ncol(pooled_panel[4:ncol(pooled_panel)])/3), 10),
  nodesize = seq(2, 14, 2)
  # nodedepth recommended not to be changed
  #nodedepth = 1,
)

# Fit an RF model over the entire grid of hyperparameters
# Ideally, we would save all the models as we computed them
# This is very memory intensive, hence they will NOT be saved
# Downside is that the optimal model will have to be recomputed from the optimal hyperparameters
# Relevant lines have been commented out for now

RF_fit_model_grid <- function(f, train, validation, RF_grid, loss_function) {
  #Initialize List
  RF_model_grid <- rep(list(0), nrow(RF_grid))
  
  for (i in 1:nrow(RF_grid)) {
    
    RF_model_grid[[i]] <- list(RF_grid = 0, model = 0)
    
    #MSE Case
    if (loss_function == "mse") {
      RF <- rfsrc(f, train, 
                  #Hyperparameters
                  ntree = RF_grid$ntree[i],
                  mtry = RF_grid$mtry[i],
                  nodesize = RF_grid$nodesize[i],
                  splitrule = "mse"
                  )
      
      #RF_model_grid[[i]]$model <- RF
      RF_model_grid[[i]]$RF_grid <- cbind(RF_grid[i, ],
                                          #Train Loss
                                          train_loss = mse(train$rt, predict(RF)$predicted),
                                          #Validation Loss
                                          validation_loss = mse(validation$rt, predict(RF, newdata = validation)$predicted)
                                          )
      
    } else {
      #MAE Case
      RF <- rfsrc(f, train, 
                  #Hyperparameters
                  ntree = RF_grid$ntree[i],
                  mtry = RF_grid$mtry[i],
                  nodesize = RF_grid$nodesize[i],
                  splitrule = "quantile.regr",
                  prob = 0.5
                  )
      
      #RF_model_grid[[i]]$model <- RF
      RF_model_grid[[i]]$RF_grid <- cbind(RF_grid[i, ],
                                          #Train Loss
                                          train_loss = mae(train$rt, predict(RF)$predicted),
                                          #Validation Loss
                                          validation_loss = mae(validation$rt, predict(RF, newdata = validation)$predicted)
                                          )
    }
  }
  return(RF_model_grid)
}

#Returns the dataframe row containing the "best" hyperparameters

get_RF_best_tune <- function(RF_model_grid) {
  RF_tune_grid <- RF_model_grid[[1]]$RF_grid
  for (i in 2:length(RF_model_grid)) {
    RF_tune_grid <- rbind(RF_tune_grid, RF_model_grid[[i]]$RF_grid)
  }
  return(RF_tune_grid[which.min(RF_tune_grid$validation_loss), ])
}

RF_fit_stats <- function(f, pooled_panel, RF_grid, timeSlices, loss_function) {
  #Initialize
  RF_stats <- rep(list(0), 3)
  
  #Load training, validation and test sets
  
  for (set in 1:3) {
    
    RF_stats[[set]] <- list(loss_stats = data.frame(train_MAE = 0, train_MSE = 0, train_RMSE = 0, train_RSquare = 0, 
                                                    validation_MAE = 0, validation_MSE = 0, validation_RMSE = 0, validation_RSquare = 0, 
                                                    test_MAE = 0, test_MSE = 0, test_RMSE = 0, test_RSquare = 0),
                            #Other useful things
                            forecasts = 0,
                            forecast_resids = 0,
                            model = 0)
    
    train <- pooled_panel %>%
      filter(time %in% timeSlices[[set]]$train)
    validation <- pooled_panel %>%
      filter(time %in% timeSlices[[set]]$validation)
    test <- pooled_panel %>%
      filter(time %in% timeSlices[[set]]$test)
    
    #Fit on training Set over grid of hyperparameters
    
    model_grid <- RF_fit_model_grid(f, train, validation, RF_grid, loss_function)
    
    #Get the best hyperparameters
    
    best_model_params <- get_RF_best_tune(model_grid)
    
    #Compute the optimal model
    
    if (loss_function == "mse") {
      model <- rfsrc(f, train, 
                     #Hyperparameters
                     ntree = best_model_params$ntree,
                     mtry = best_model_params$mtry,
                     nodesize = best_model_params$nodesize,
                     splitrule = "mse"
                     )
    } else {
      model <- rfsrc(f, train, 
                     #Hyperparameters
                     ntree = best_model_params$ntree,
                     mtry = best_model_params$mtry,
                     nodesize = best_model_params$nodesize,
                     splitrule = "quantile.regr",
                     prob = 0.5
                     )
    }
    
    #Train
    train_predict <- predict(model, train, alpha = best_model_params$alpha, lambda = best_model_params$lambda)$predicted
    RF_stats[[set]]$loss_stats$train_MAE <- mae(train$rt, train_predict)
    RF_stats[[set]]$loss_stats$train_MSE <- mse(train$rt, train_predict)
    RF_stats[[set]]$loss_stats$train_RMSE <- rmse(train$rt, train_predict)
    RF_stats[[set]]$loss_stats$train_RSquare <- (1 - sse(train$rt, train_predict) / sum((train$rt - mean(train$rt))^2))
        
    #Validation
    valid_predict <- predict(model, newdata = validation, alpha = best_model_params$alpha, lambda = best_model_params$lambda)$predicted
    RF_stats[[set]]$loss_stats$validation_MAE <- mae(validation$rt, valid_predict)
    RF_stats[[set]]$loss_stats$validation_MSE <- mse(validation$rt, valid_predict)
    RF_stats[[set]]$loss_stats$validation_RMSE <- rmse(validation$rt, valid_predict)
    RF_stats[[set]]$loss_stats$validation_RSquare <- (1 - sse(validation$rt, valid_predict) / sum((validation$rt - mean(validation$rt))^2))
    
    #Test
    test_predict <- predict(model, newdata = test, alpha = best_model_params$alpha, lambda = best_model_params$lambda)$predicted
    RF_stats[[set]]$loss_stats$test_MAE <- mae(test$rt, test_predict)
    RF_stats[[set]]$loss_stats$test_MSE <- mse(test$rt, test_predict)
    RF_stats[[set]]$loss_stats$test_RMSE <- rmse(test$rt, test_predict)
    RF_stats[[set]]$loss_stats$test_RSquare <- (1 - sse(test$rt, test_predict) / sum((test$rt - mean(test$rt))^2))
      
    #Forecasts
    RF_stats[[set]]$forecasts <- test_predict
    
    #Forecast residuals
    RF_stats[[set]]$forecast_resids <- test$rt - test_predict
  }
  return(RF_stats)
}

RF_MSE <- RF_fit_stats(f, pooled_panel, RF_grid, timeSlices, "mse")

RF_MAE <- RF_fit_stats(f, pooled_panel, RF_grid, timeSlices, "mae")

RF_MSE[[1]]$loss_stats
RF_MSE[[2]]$loss_stats
RF_MSE[[3]]$loss_stats

RF_MAE[[1]]$loss_stats
RF_MAE[[2]]$loss_stats
RF_MAE[[3]]$loss_stats

# Notes
# Performance seems to be worse than other methods, quite surprising
# Quantile trees (ie MAE) do better than MSE trees
# Quantile trees are much more computationally intensive
```
## Neural Networks WRT MSE

```{r neural_networks}
# Neural Networks

set.seed(27935248)

#Build neural networks with neuron numbers according to geometric pyramid rule, and ReLU activation function for all layers
# IE Input layer > 32 neurons > 16 neurons > 8 neurons > 4 neurons > 2 neurons > output

##Neural Network 1

#Initialize

nnet_stats_list <- rep(list(0), 3)

for (set in 1:3) {
  nnet_stats_list[[set]] <- list(loss_stats = data.frame(train_MAE = 0, train_MSE = 0, train_RMSE = 0, train_RSquare = 0, 
                                                validation_MAE = 0, validation_MSE = 0, validation_RMSE = 0, validation_RSquare = 0, 
                                                test_MAE = 0, test_MSE = 0, test_RMSE = 0, test_RSquare = 0),
                                 #Other useful things
                                 forecasts = 0,
                                 forecast_resids = 0,
                                 model = 0)
    
    #Load Training, validation and test sets
    train <- pooled_panel %>%
      filter(time %in% timeSlices[[set]]$train)
    validation <- pooled_panel %>%
      filter(time %in% timeSlices[[set]]$validation)
    test<- pooled_panel %>%
      filter(time %in% timeSlices[[set]]$test)
    
    #Fit the model
    
}



```

```{r}
train_x <- train[4:ncol(train)]
train_y <- train$rt

validation_x <- validation[4:ncol(validation)]
validation_y <- validation$rt

# The patience parameter is the amount of epochs to check for improvement.
early_stop <- callback_early_stopping(monitor = "val_loss", patience = 20)

#It's clearer to separate these "layers" out as much as possible
build_NN1 <- function(loss_function) {
  
  model <- keras_model_sequential() %>%
    # Layer 1
    layer_dense(units = 32, input_shape = ncol(train_x)) %>%
    layer_activation("relu") %>%
    layer_activity_regularization(l1 = 0.001) %>%
    layer_batch_normalization() %>%
    # Output Layer
    layer_dense(units = 1, activation = "linear")
  
  model %>% compile(
    #MSE loss function
    loss = loss_function,
    #We're using ADAM
    optimizer = "adam",
    metrics = list("mean_absolute_error", "mean_squared_error")
  )
  model
}

## WRT MSE

neural_network_1_mse <- build_NN1("mse")
neural_network_1_mse %>% summary()

#Just do something straightforward for now

neural_network_1_mse %>% fit(as.matrix(train_x), as.matrix(train_y), 
                             epochs = 100, verbose = 1, 
                             validation_data = list(as.matrix(validation_x), as.matrix(validation_y)),
                             callbacks = list(early_stop))

#Make predictions
test_x <- test[4:ncol(test)]

NN1_mse_predictions <- neural_network_1_mse %>% predict(as.matrix(test_x))

mae(test$rt, NN1_mse_predictions)
mse(test$rt, NN1_mse_predictions)

###########################################
## WRT MAE

neural_network_1_mae <- build_NN1("mae")
neural_network_1_mae %>% summary()

#Just do something straightforward for now

neural_network_1_mae %>% fit(as.matrix(train_x), as.matrix(train_y), 
                             epochs = 100, verbose = 1, 
                             validation_data = list(as.matrix(validation_x), as.matrix(validation_y)),
                             callbacks = list(early_stop))

#Make predictions
test_x <- test[4:ncol(test)]

NN1_mae_predictions <- neural_network_1_mae %>% predict(as.matrix(test_x))

mae(test$rt, NN1_mae_predictions)
mse(test$rt, NN1_mae_predictions)
```

```{r}
# Neural Network 2

train_x <- train[4:ncol(train)]
train_y <- train$rt

validation_x <- validation[4:ncol(validation)]
validation_y <- validation$rt

# The patience parameter is the amount of epochs to check for improvement.
early_stop <- callback_early_stopping(monitor = "val_loss", patience = 20)

#It's clearer to separate these "layers" out as much as possible
build_NN2 <- function(loss_function) {
  
  model <- keras_model_sequential() %>%
    # Layer 1
    layer_dense(units = 32, input_shape = ncol(train_x)) %>%
    layer_activation("relu") %>%
    layer_batch_normalization() %>%
    # Layer 2
    layer_dense(units = 16) %>%
    layer_activation("relu") %>%
    layer_batch_normalization() %>%
    # Output Layer
    layer_dense(units = 1, activation = "linear")
  
  model %>% compile(
    #MSE loss function
    loss = loss_function,
    #We're using ADAM
    optimizer = "adam",
    metrics = list("mean_absolute_error", "mean_squared_error")
  )
  model
}

## WRT MSE

neural_network_2_mse <- build_NN2("mse")
neural_network_2_mse %>% summary()

#Just do something straightforward for now

neural_network_2_mse %>% fit(as.matrix(train_x), as.matrix(train_y), 
                             epochs = 100, verbose = 1, 
                             validation_data = list(as.matrix(validation_x), as.matrix(validation_y)),
                             callbacks = list(early_stop))

#Make predictions
test_x <- test[4:ncol(test)]

NN2_mse_predictions <- neural_network_2_mse %>% predict(as.matrix(test_x))

mae(test$rt, NN1_mse_predictions)
mse(test$rt, NN1_mse_predictions)

###########################################
## WRT MAE

neural_network_2_mae <- build_NN2("mae")
neural_network_2_mae %>% summary()

#Just do something straightforward for now

neural_network_2_mae %>% fit(as.matrix(train_x), as.matrix(train_y), 
                             epochs = 100, verbose = 1, 
                             validation_data = list(as.matrix(validation_x), as.matrix(validation_y)),
                             callbacks = list(early_stop))

#Make predictions
test_x <- test[4:ncol(test)]

NN2_mae_predictions <- neural_network_2_mae %>% predict(as.matrix(test_x))

mae(test$rt, NN2_mae_predictions)
mse(test$rt, NN2_mae_predictions)
```

```{r}
# Neural Network 3

train_x <- train[4:ncol(train)]
train_y <- train$rt

validation_x <- validation[4:ncol(validation)]
validation_y <- validation$rt

#It's clearer to separate these "layers" out as much as possible
build_NN3 <- function(loss_function) {
  
  model <- keras_model_sequential() %>%
    # Layer 1
    layer_dense(units = 32, input_shape = ncol(train_x)) %>%
    layer_activation("relu") %>%
    layer_batch_normalization() %>%
    # Layer 2
    layer_dense(units = 16) %>%
    layer_activation("relu") %>%
    layer_batch_normalization() %>%
    #Layer 3
    layer_dense(units = 8) %>%
    layer_activation("relu") %>%
    layer_batch_normalization() %>%
    # Output Layer
    layer_dense(units = 1, activation = "linear")
  
  model %>% compile(
    #MSE loss function
    loss = loss_function,
    #We're using ADAM
    optimizer = "adam",
    metrics = list("mean_absolute_error", "mean_squared_error")
  )
  model
}

## WRT MSE

neural_network_3_mse <- build_NN3("mse")
neural_network_3_mse %>% summary()

#Just do something straightforward for now

neural_network_3_mse %>% fit(as.matrix(train_x), as.matrix(train_y), 
                             epochs = 100, verbose = 1, 
                             validation_data = list(as.matrix(validation_x), as.matrix(validation_y)),
                             callbacks = list(early_stop))

#Make predictions
test_x <- test[4:ncol(test)]

NN3_mse_predictions <- neural_network_3_mse %>% predict(as.matrix(test_x))

mae(test$rt, NN3_mse_predictions)
mse(test$rt, NN3_mse_predictions)

###########################################
## WRT MAE

neural_network_3_mae <- build_NN2("mae")
neural_network_3_mae %>% summary()

#Just do something straightforward for now

neural_network_3_mae %>% fit(as.matrix(train_x), as.matrix(train_y), 
                             epochs = 100, verbose = 1, 
                             validation_data = list(as.matrix(validation_x), as.matrix(validation_y)),
                             callbacks = list(early_stop))

#Make predictions
test_x <- test[4:ncol(test)]

NN3_mae_predictions <- neural_network_3_mae %>% predict(as.matrix(test_x))

mae(test$rt, NN3_mae_predictions)
mse(test$rt, NN3_mae_predictions)
```

```{r}
# Neural Network 4

train_x <- train[4:ncol(train)]
train_y <- train$rt

validation_x <- validation[4:ncol(validation)]
validation_y <- validation$rt

# The patience parameter is the amount of epochs to check for improvement.
early_stop <- callback_early_stopping(monitor = "val_loss", patience = 20)

#It's clearer to separate these "layers" out as much as possible
build_NN4 <- function(loss_function) {
  
  model <- keras_model_sequential() %>%
    # Layer 1
    layer_dense(units = 32, input_shape = ncol(train_x)) %>%
    layer_activation("relu") %>%
    layer_batch_normalization() %>%
    # Layer 2
    layer_dense(units = 16) %>%
    layer_activation("relu") %>%
    layer_batch_normalization() %>%
    #Layer 3
    layer_dense(units = 8) %>%
    layer_activation("relu") %>%
    layer_batch_normalization() %>%
    #Layer 4
    layer_dense(units = 4) %>%
    layer_activation("relu") %>%
    layer_batch_normalization() %>%
    # Output Layer
    layer_dense(units = 1, activation = "linear")
  
  model %>% compile(
    #MSE loss function
    loss = loss_function,
    #We're using ADAM
    optimizer = "adam",
    metrics = list("mean_absolute_error", "mean_squared_error")
  )
  model
}

## WRT MSE

neural_network_4_mse <- build_NN4("mse")
neural_network_4_mse %>% summary()

#Just do something straightforward for now

neural_network_4_mse %>% fit(as.matrix(train_x), as.matrix(train_y), 
                             epochs = 100, verbose = 1, 
                             validation_data = list(as.matrix(validation_x), as.matrix(validation_y)),
                             callbacks = list(early_stop))

#Make predictions
test_x <- test[4:ncol(test)]

NN4_mse_predictions <- neural_network_4_mse %>% predict(as.matrix(test_x))

mae(test$rt, NN4_mse_predictions)
mse(test$rt, NN4_mse_predictions)

###########################################
## WRT MAE

neural_network_4_mae <- build_NN2("mae")
neural_network_4_mae %>% summary()

#Just do something straightforward for now

neural_network_4_mae %>% fit(as.matrix(train_x), as.matrix(train_y), 
                             epochs = 100, verbose = 1, 
                             validation_data = list(as.matrix(validation_x), as.matrix(validation_y)),
                             callbacks = list(early_stop))

#Make predictions
test_x <- test[4:ncol(test)]

NN4_mae_predictions <- neural_network_4_mae %>% predict(as.matrix(test_x))

mae(test$rt, NN4_mae_predictions)
mse(test$rt, NN4_mae_predictions)
```

```{r}
# Neural Network 5

train_x <- train[4:ncol(train)]
train_y <- train$rt

validation_x <- validation[4:ncol(validation)]
validation_y <- validation$rt

# The patience parameter is the amount of epochs to check for improvement.
early_stop <- callback_early_stopping(monitor = "val_loss", patience = 20)

#It's clearer to separate these "layers" out as much as possible
build_NN5 <- function(loss_function) {
  
  model <- keras_model_sequential() %>%
    # Layer 1
    layer_dense(units = 32, input_shape = ncol(train_x)) %>%
    layer_activation("relu") %>%
    layer_batch_normalization() %>%
    # Layer 2
    layer_dense(units = 16) %>%
    layer_activation("relu") %>%
    layer_batch_normalization() %>%
    #Layer 3
    layer_dense(units = 8) %>%
    layer_activation("relu") %>%
    layer_batch_normalization() %>%
    #Layer 4
    layer_dense(units = 4) %>%
    layer_activation("relu") %>%
    layer_batch_normalization() %>%
    #Layer 5
    layer_dense(units = 2) %>%
    layer_activation("relu") %>%
    layer_batch_normalization() %>%
    # Output Layer
    layer_dense(units = 1, activation = "linear")
  
  model %>% compile(
    #MSE loss function
    loss = loss_function,
    #We're using ADAM
    optimizer = "adam",
    metrics = list("mean_absolute_error", "mean_squared_error")
  )
  model
}

## WRT MSE

neural_network_5_mse <- build_NN5("mse")
neural_network_5_mse %>% summary()

#Just do something straightforward for now

neural_network_5_mse %>% fit(as.matrix(train_x), as.matrix(train_y), 
                             epochs = 100, verbose = 1, 
                             validation_data = list(as.matrix(validation_x), as.matrix(validation_y)),
                             callbacks = list(early_stop))

#Make predictions
test_x <- test[5:ncol(test)]

NN5_mse_predictions <- neural_network_5_mse %>% predict(as.matrix(test_x))

mae(test$rt, NN5_mse_predictions)
mse(test$rt, NN5_mse_predictions)

###########################################
## WRT MAE

neural_network_5_mae <- build_NN2("mae")
neural_network_5_mae %>% summary()

#Just do something straightforward for now

neural_network_5_mae %>% fit(as.matrix(train_x), as.matrix(train_y), 
                             epochs = 100, verbose = 1, 
                             validation_data = list(as.matrix(validation_x), as.matrix(validation_y)),
                             callbacks = list(early_stop))

#Make predictions
test_x <- test[4:ncol(test)]

NN5_mae_predictions <- neural_network_5_mae %>% predict(as.matrix(test_x))

mae(test$rt, NN5_mae_predictions)
mse(test$rt, NN5_mae_predictions)
```

