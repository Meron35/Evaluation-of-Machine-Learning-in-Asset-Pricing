# Generate log prices
# Drop RET which is monthly return
select(Time, permno, PRC, -RET, everything()) %>%
# Filter so that we only have NASDAQ stocks
# Remember that the NASDAQ only opened in 1971 ish
filter(PRIMEXCH == "Q") %>%
select(-PRIMEXCH) %>%
# Negative PRC means that there was no closing price, and the dash denotes the bid ask average instead
# Not too unreasonable to just take this as the actual price
# Note that PRC has not been corrected for stock splits etc. This is more sensible for modelling purposes
mutate(PRC = abs(PRC)) %>%
# Prices of 0 mean that neither the bid ask average or price were available, ie missing
filter(PRC != 0) %>%
# Filter out PRC < 5 to get rid of penny stocks
filter(PRC >= 5) %>%
# Filter out so that only shares with share codes of 10 or 11 are included (other codes refer to other instruments such as REITs, etc)
# Don't have any idea why Gu et al thought this was originally sensible
filter(SHRCD == 10 | SHRCD == 11) %>%
# SIC codes according to WRDS/CRSP are recommended to be used with caution, as they are not strictly enforced
# Companies can also belong in multiple SIC codes, or change SIC codes over time. This is not adequately captured by CRSP
# Also don't make much consistent sense either
# Personal decision: drop them entirely
select(-sic2) %>%
# We will construct quarterly returns using the end of each month
# Ie using the 3rd, 6th, 9th and 12th month of each year
# yearmon stores year + 0 for January. Exploit this to filter out non-quarter months
filter((as.yearmon(Time) %% 1) == 2/12 | (as.yearmon(Time) %% 1) == 5/12 | (as.yearmon(Time) %% 1) == 8/12 | (as.yearmon(Time) %% 1) == 11/12) %>%
select(-RET)
## Function to generate quarterly returns
## This approach keeps companies that were were unlisted and relisted
## Sensible, because companies can be bought out and floated again, etc.
## Not likely to make a huge difference, but actual quarterly returns are computed, instead of log quarterly returns
## This is because stock prices can in some cases have rather large movements over a quarter
time_df <- data.frame(Time = unique(datashare$Time)) %>%
mutate(Time = as.numeric(Time))
datashare <- foreach(i = (1:length(unique(datashare$permno))), .combine = rbind) %dopar% {
datashare_stock <- datashare %>%
filter(permno == unique(datashare$permno)[i]) %>%
full_join(time_df, by = "Time") %>%
arrange(Time) %>%
mutate(RET_M = (PRC - lag(PRC))/lag(PRC)) %>%
drop_na(RET_M)
datashare_stock
}
View(datashare)
datashare <- foreach(i = (1:length(unique(datashare$permno))), .combine = rbind) %dopar% {
datashare_stock <- datashare %>%
mutate(Time = as.numeric(Time)) %>%
filter(permno == unique(datashare$permno)[i]) %>%
full_join(time_df, by = "Time") %>%
arrange(Time) %>%
mutate(RET_M = (PRC - lag(PRC))/lag(PRC)) %>%
drop_na(RET_M)
datashare_stock
}
View(datashare)
# Reorder (again)
datashare <- datashare %>%
select(Time, permno, PRC, RET_M, everything()) %>%
mutate(Time = as.yearmon(Time))
missing_factors <- datashare %>%
# Change time filtering here
filter(Time > 1996) %>%
gather(col, value) %>%
group_by(col) %>%
summarize(missing_share = mean(is.na(value))) %>%
filter(missing_share > 0.10)
missing_factors
missing_factors <- datashare %>%
# Change time filtering here
filter(Time > 1996) %>%
gather(col, value) %>%
group_by(col) %>%
summarize(missing_share = mean(is.na(value))) %>%
filter(missing_share > 0.20)
missing_factors <- datashare %>%
# Change time filtering here
filter(Time > 1995) %>%
gather(col, value) %>%
group_by(col) %>%
summarize(missing_share = mean(is.na(value))) %>%
filter(missing_share > 0.20)
missing_factors
missing_factors <- datashare %>%
# Change time filtering here
filter(Time > 1994) %>%
gather(col, value) %>%
group_by(col) %>%
summarize(missing_share = mean(is.na(value))) %>%
filter(missing_share > 0.20)
missing_factors
missing_factors <- datashare %>%
# Change time filtering here
filter(Time > 1993) %>%
gather(col, value) %>%
group_by(col) %>%
summarize(missing_share = mean(is.na(value))) %>%
filter(missing_share > 0.20)
missing_factors
missing_factors <- datashare %>%
# Change time filtering here
filter(Time > 1992) %>%
gather(col, value) %>%
group_by(col) %>%
summarize(missing_share = mean(is.na(value))) %>%
filter(missing_share > 0.20)
missing_factors
## Remove previously identified factors
datashare_filtered <- datashare %>%
select(-c(missing_factors$col))
cross_sectional_values <- function(dataset, impute_type) {
time_periods <- unique(dataset$Time)
cross_sectional_values_df <- foreach(t = 1:length(time_periods), .combine = "rbind") %dopar% {
dataset_cross_section <- dataset %>%
filter(Time == time_periods[t])
# Mean Case
if (impute_type == "mean") {
cross_sectional_values <- dataset_cross_section %>%
summarize_all(mean, na.rm = TRUE) %>%
mutate(Time = as.yearmon(Time))
}
# Median Case
else {
cross_sectional_values <- dataset_cross_section %>%
summarize_all(median, na.rm = TRUE) %>%
mutate(Time = as.yearmon(Time))
}
cross_sectional_values
}
cross_sectional_values_df
}
test <- cross_sectional_values(datashare_filtered, impute_type = "median")
colnames(test)[colSums(is.na(test)) > 0]
(test)[colSums(is.na(test)) > 0]
cbind(test$Time, (test)[colSums(is.na(test)) > 0])
## Remove previously identified factors
datashare_filtered <- datashare %>%
filter(Time > 1992) %>%
select(-c(missing_factors$col))
test <- cross_sectional_values(datashare_filtered, impute_type = "median")
colnames(test)[colSums(is.na(test)) > 0]
cbind(test$Time, (test)[colSums(is.na(test)) > 0])
time_periods <- unique(datashare_filtered$Time)
dataset_cross_section <- datashare_filtered %>%
filter(Time == time_periods[1])
View(dataset_cross_section)
dataset_cross_section$dolvol
dataset_cross_section$dolvol[is.na(dataset_cross_section$dolvol)]
dataset_cross_section$dolvol[is.na(dataset_cross_section$dolvol)] <- median(dataset_cross_section$dolvol)
dataset_cross_section[, 1]
dataset_cross_section[, 7][is.na(dataset_cross_section[, 7])]
?select_if
install.packages("tidyimpute")
library(tidyimpute)
?impute_median
dataset_cross_section %>%
impute_median()
View(dataset_cross_section)
median(dataset_cross_section$dolvol)
median(dataset_cross_section$dolvol, na.rm = TRUE)
impute_cross_section <- function(dataset, impute_type) {
time_periods <- unique(dataset$Time)
imputed_cross_section_df <- foreach(t = 1:length(time_periods), .combine = "rbind") %dopar% {
dataset_cross_section <- dataset %>%
filter(Time == time_periods[t])
if (impute_type == "median") {
# Impute the median for ALL columns
dataset_cross_section %>%
impute_median()
} else {
dataset_cross_section %>%
impute_mean()
}
}
imputed_cross_section_df
}
impute_cross_section <- function(dataset, impute_type) {
time_periods <- unique(dataset$Time)
imputed_cross_section_df <- foreach(t = 1:length(time_periods), .combine = "rbind") %dopar% {
dataset_cross_section <- dataset %>%
filter(Time == time_periods[t])
if (impute_type == "median") {
# Impute the median for ALL columns
dataset_cross_section %>%
impute_median()
} else {
dataset_cross_section %>%
impute_mean()
}
}
imputed_cross_section_df
}
datashare_imputed <- impute_cross_section(datashare_filtered)
datashare_imputed <- impute_cross_section(datashare_filtered, impute_type = "median")
summary(datashare_imputed)
length(unique(datashare_imputed$permno))
ncol(datashare_filtered)
object.size(datashare_imputed)
?as.yearmon
as.numeric(datashare_imputed_Time)
as.numeric(datashare_imputed$Time)
View(datashare_imputed)
datashare <- datashare_RAW_join %>%
mutate(Time = as.yearmon(as.character(DATE), "%Y%m%d")) %>%
select(-DATE) %>%
# Generate log prices
# Drop RET which is monthly return
select(Time, permno, PRC, -RET, everything()) %>%
# Filter so that we only have NASDAQ stocks
# Remember that the NASDAQ only opened in 1971 ish
filter(PRIMEXCH == "Q") %>%
select(-PRIMEXCH) %>%
# Negative PRC means that there was no closing price, and the dash denotes the bid ask average instead
# Not too unreasonable to just take this as the actual price
# Note that PRC has not been corrected for stock splits etc. This is more sensible for modelling purposes
mutate(PRC = abs(PRC)) %>%
# Prices of 0 mean that neither the bid ask average or price were available, ie missing
filter(PRC != 0) %>%
# Filter out PRC < 5 to get rid of penny stocks
filter(PRC >= 5) %>%
# Filter out so that only shares with share codes of 10 or 11 are included (other codes refer to other instruments such as REITs, etc)
# Don't have any idea why Gu et al thought this was originally sensible
filter(SHRCD == 10 | SHRCD == 11) %>%
# SIC codes according to WRDS/CRSP are recommended to be used with caution, as they are not strictly enforced
# Companies can also belong in multiple SIC codes, or change SIC codes over time. This is not adequately captured by CRSP
# Also don't make much consistent sense either
# Personal decision: drop them entirely
select(-sic2) %>%
# We will construct quarterly returns using the end of each month
# Ie using the 3rd, 6th, 9th and 12th month of each year
# yearmon stores year + 0 for January. Exploit this to filter out non-quarter months
filter((as.yearmon(Time) %% 1) == 2/12 | (as.yearmon(Time) %% 1) == 5/12 | (as.yearmon(Time) %% 1) == 8/12 | (as.yearmon(Time) %% 1) == 11/12) %>%
select(-RET)
datashare <- datashare_RAW_join %>%
mutate(Time = as.yearmon(as.character(DATE), "%Y%m%d")) %>%
select(-DATE) %>%
# Generate log prices
# Drop RET which is monthly return
select(Time, permno, PRC, -RET, everything()) %>%
# Filter so that we only have NASDAQ stocks
# Remember that the NASDAQ only opened in 1971 ish
filter(PRIMEXCH == "Q") %>%
select(-PRIMEXCH) %>%
# Negative PRC means that there was no closing price, and the dash denotes the bid ask average instead
# Not too unreasonable to just take this as the actual price
# Note that PRC has not been corrected for stock splits etc. This is more sensible for modelling purposes
mutate(PRC = abs(PRC)) %>%
# Prices of 0 mean that neither the bid ask average or price were available, ie missing
filter(PRC != 0) %>%
# Filter out PRC < 5 to get rid of penny stocks
filter(PRC >= 5) %>%
# Filter out so that only shares with share codes of 10 or 11 are included (other codes refer to other instruments such as REITs, etc)
# Don't have any idea why Gu et al thought this was originally sensible
filter(SHRCD == 10 | SHRCD == 11) %>%
# SIC codes according to WRDS/CRSP are recommended to be used with caution, as they are not strictly enforced
# Companies can also belong in multiple SIC codes, or change SIC codes over time. This is not adequately captured by CRSP
# Also don't make much consistent sense either
# Personal decision: drop them entirely
select(-sic2) %>%
# We will construct quarterly returns using the end of each month
# Ie using the 3rd, 6th, 9th and 12th month of each year
# yearmon stores year + 0 for January. Exploit this to filter out non-quarter months
filter((as.yearmon(Time) %% 1) == 2/12 | (as.yearmon(Time) %% 1) == 5/12 | (as.yearmon(Time) %% 1) == 8/12 | (as.yearmon(Time) %% 1) == 11/12) %>%
# Convert Time to yearqtr format
# yearqtr stores data as year + 0/4 for q1, 1/4 for q2, etc
mutate(Time = as.yearqtr(Time - 2/12)) %>%
select(-RET)
View(datashare)
datashare_sample <- sample_frac(datashare, 0.25)
write.csv(datashare_sample, file = "datashare_sample.csv")
## Function to generate quarterly returns
## This approach keeps companies that were were unlisted and relisted
## Sensible, because companies can be bought out and floated again, etc.
## Not likely to make a huge difference, but actual quarterly returns are computed, instead of log quarterly returns
## This is because stock prices can in some cases have rather large movements over a quarter
time_df <- data.frame(Time = unique(datashare$Time)) %>%
mutate(Time = as.numeric(Time))
datashare <- foreach(i = (1:length(unique(datashare$permno))), .combine = rbind) %dopar% {
datashare_stock <- datashare %>%
mutate(Time = as.numeric(Time)) %>%
filter(permno == unique(datashare$permno)[i]) %>%
full_join(time_df, by = "Time") %>%
arrange(Time) %>%
mutate(RET_M = (PRC - lag(PRC))/lag(PRC)) %>%
drop_na(RET_M)
datashare_stock
}
# Reorder (again)
datashare <- datashare %>%
select(Time, permno, PRC, RET_M, everything()) %>%
mutate(Time = as.yearqtr(Time))
missing_factors <- datashare %>%
# Change time filtering here
filter(Time >= 1992) %>%
gather(col, value) %>%
group_by(col) %>%
summarize(missing_share = mean(is.na(value))) %>%
filter(missing_share > 0.20)
missing_factors
missing_factors <- datashare %>%
# Change time filtering here
filter(Time >= 1993) %>%
gather(col, value) %>%
group_by(col) %>%
summarize(missing_share = mean(is.na(value))) %>%
filter(missing_share > 0.20)
missing_factors
missing_factors <- datashare %>%
# Change time filtering here
filter(Time >= 1994) %>%
gather(col, value) %>%
group_by(col) %>%
summarize(missing_share = mean(is.na(value))) %>%
filter(missing_share > 0.20)
missing_factors
missing_factors <- datashare %>%
# Change time filtering here
filter(Time >= 1993.5) %>%
gather(col, value) %>%
group_by(col) %>%
summarize(missing_share = mean(is.na(value))) %>%
filter(missing_share > 0.20)
missing_factors
missing_factors <- datashare %>%
# Change time filtering here
filter(Time >= 1992.5) %>%
gather(col, value) %>%
group_by(col) %>%
summarize(missing_share = mean(is.na(value))) %>%
filter(missing_share > 0.20)
missing_factors
missing_factors <- datashare %>%
# Change time filtering here
filter(Time >= 1993.5) %>%
gather(col, value) %>%
group_by(col) %>%
summarize(missing_share = mean(is.na(value))) %>%
filter(missing_share > 0.20)
## Remove previously identified factors
datashare_filtered <- datashare %>%
filter(Time >= 1993.5) %>%
select(-c(missing_factors$col))
impute_cross_section <- function(dataset, impute_type) {
time_periods <- unique(dataset$Time)
imputed_cross_section_df <- foreach(t = 1:length(time_periods), .combine = "rbind") %dopar% {
dataset_cross_section <- dataset %>%
filter(Time == time_periods[t])
if (impute_type == "median") {
# Impute the median for ALL columns
dataset_cross_section %>%
impute_median()
} else {
dataset_cross_section %>%
impute_mean()
}
}
imputed_cross_section_df
}
datashare_imputed <- impute_cross_section(datashare_filtered, impute_type = "median")
cross_sectional_values <- function(dataset, impute_type) {
time_periods <- unique(dataset$Time)
cross_sectional_values_df <- foreach(t = 1:length(time_periods), .combine = "rbind") %dopar% {
dataset_cross_section <- dataset %>%
filter(Time == time_periods[t])
# Mean Case
if (impute_type == "mean") {
cross_sectional_values <- dataset_cross_section %>%
summarize_all(mean, na.rm = TRUE) %>%
mutate(Time = as.yearqtr(Time))
}
# Median Case
else {
cross_sectional_values <- dataset_cross_section %>%
summarize_all(median, na.rm = TRUE) %>%
mutate(Time = as.yearqtr(Time))
}
cross_sectional_values
}
cross_sectional_values_df
}
# Summary Statistics
summary(datashare_imputed)
# Summary Statistics
summary(datashare_imputed)
ncol(datashare_filtered)
####################################################
# Load RAW Welch Goyal Data
####################################################
# Note that Gu et al only used:
# Dividend price ratio dp, earnings price ratio ep, book to market ratio bm,
# net equity expansion ntis, Treasury bill rate tbl, term spread tms,
# Default spread dfy, stock variance svar
PredictorData2017_RAW <- read_excel("data/factors/PredictorData2017.xlsx",
sheet = "Quarterly",
na = "NaN")
PredictorData2017_RAW$yyyyq
as.yearqtr(PredictorData2017_RAW$yyyyq)
as.yearqtr(PredictorData2017_RAW$yyyyq, "%Y%q")
as.yearqtr(as.character(PredictorData2017_RAW$yyyyq), "%Y%q")
PredictorData2017 <- PredictorData2017_RAW %>%
mutate(Time = as.yearqtr(as.character(yyyyq), "%Y%q")) %>%
select(-yyyymm) %>%
# Dividend price ratio is the difference between the log of dividends and the log of prices
# Note that log(D12) - log(Index) is negative which doesn't make sense as a ratio
mutate(dp = abs(log(D12) - log(Index))) %>%
# earnings price ratio is the difference between the log of earnings and the log of prices
# Note that log(E12) - log(Index) is negative which doesn't make sense as a ratio
mutate(ep = abs(log(E12) - log(Index))) %>%
# Term Spread is the difference between long term yield on gov bonds and treasury bills
mutate(tms = lty - tbl) %>%
# Default spread is the difference between BAA and AAA-rated corporate bond yields
mutate(dfy = abs(BAA - AAA)) %>%
# Rename b/m to bm
rename(bm = `b/m`)
PredictorData2017 <- PredictorData2017_RAW %>%
mutate(Time = as.yearqtr(as.character(yyyyq), "%Y%q")) %>%
select(-yyyyq) %>%
# Dividend price ratio is the difference between the log of dividends and the log of prices
# Note that log(D12) - log(Index) is negative which doesn't make sense as a ratio
mutate(dp = abs(log(D12) - log(Index))) %>%
# earnings price ratio is the difference between the log of earnings and the log of prices
# Note that log(E12) - log(Index) is negative which doesn't make sense as a ratio
mutate(ep = abs(log(E12) - log(Index))) %>%
# Term Spread is the difference between long term yield on gov bonds and treasury bills
mutate(tms = lty - tbl) %>%
# Default spread is the difference between BAA and AAA-rated corporate bond yields
mutate(dfy = abs(BAA - AAA)) %>%
# Rename b/m to bm
rename(bm = `b/m`)
View(PredictorData2017)
PredictorData2017 <- PredictorData2017_RAW %>%
mutate(Time = as.yearqtr(as.character(yyyyq), "%Y%q")) %>%
select(-yyyyq) %>%
# Dividend price ratio is the difference between the log of dividends and the log of prices
# Note that log(D12) - log(Index) is negative which doesn't make sense as a ratio
mutate(dp = abs(log(D12) - log(Index))) %>%
# earnings price ratio is the difference between the log of earnings and the log of prices
# Note that log(E12) - log(Index) is negative which doesn't make sense as a ratio
mutate(ep = abs(log(E12) - log(Index))) %>%
# Term Spread is the difference between long term yield on gov bonds and treasury bills
mutate(tms = lty - tbl) %>%
# Default spread is the difference between BAA and AAA-rated corporate bond yields
mutate(dfy = abs(BAA - AAA)) %>%
# Rename b/m to bm
rename(bm = `b/m`) %>%
# Reorder
select(Time, everything())
# Subset so that we have the predictors we actually care about
# Note that some of these by default have the same names as those in the individual factor set
# Therefore, prefix them with macro_ to make it clearer
# Also filter out all the entries that are not in the datashare dataset as they can't be used (Note that this also takes care of all missing values in this dataset)
# Filter it so that we only have the years we are interested in (same filtering as datashare)
macro_predictors <- PredictorData2017 %>%
select(Time, dp, ep, bm, ntis, tbl, tms, dfy, svar) %>%
rename_at(vars(-Time), function(x) paste0("macro_", x)) %>%
filter(Time >= min(datashare$Time) & Time <= max(datashare$Time)) %>%
# Reorder
select(Time, constant, everything()) %>%
filter(Time >= 1993.5)
# Subset so that we have the predictors we actually care about
# Note that some of these by default have the same names as those in the individual factor set
# Therefore, prefix them with macro_ to make it clearer
# Also filter out all the entries that are not in the datashare dataset as they can't be used (Note that this also takes care of all missing values in this dataset)
# Filter it so that we only have the years we are interested in (same filtering as datashare)
macro_predictors <- PredictorData2017 %>%
select(Time, dp, ep, bm, ntis, tbl, tms, dfy, svar) %>%
rename_at(vars(-Time), function(x) paste0("macro_", x)) %>%
# Reorder
mutate(macro_constant = 1) %>%
select(Time, macro_constant, everything()) %>%
filter(Time >= 1993.5)
View(macro_predictors)
macro_factor_names <- colnames(select(macro_predictors, -Time))
individual_factor_names <- colnames(select(datashare_imputed, -permno, -Time,))
interaction_terms <- rep(list(0), length(macro_factor_names))
for (i in 1:length(macro_factor_names)) {
interaction_terms[[i]] <- paste(macro_factor_names[i], individual_factor_names, sep = ":", collapse = " + ")
}
f <- as.formula(c("~ ", paste(interaction_terms, collapse = "+")))
combined <- full_join(macro_predictors, datashare)
# Memory issues. This interaction matrix is way too large
interaction_matrix <- sparse.model.matrix(f, model.frame(~., combined))
dmy <- dummyVars(f, data = combined)
View(interaction_matrix)
macro_factor_names <- colnames(select(macro_predictors, -Time))
individual_factor_names <- colnames(select(datashare_imputed, -permno, -Time, -RET_M))
interaction_terms <- rep(list(0), length(macro_factor_names))
for (i in 1:length(macro_factor_names)) {
interaction_terms[[i]] <- paste(macro_factor_names[i], individual_factor_names, sep = ":", collapse = " + ")
}
f <- as.formula(c("~ ", paste(interaction_terms, collapse = "+")))
combined <- full_join(macro_predictors, datashare)
final_dataset <- predict(dmy, combined)
colnames(final_dataset)
individual_factor_names <- colnames(select(datashare_imputed, -permno, -Time, -RET_M, -PRC))
interaction_terms <- rep(list(0), length(macro_factor_names))
for (i in 1:length(macro_factor_names)) {
interaction_terms[[i]] <- paste(macro_factor_names[i], individual_factor_names, sep = ":", collapse = " + ")
}
f <- as.formula(c("~ ", paste(interaction_terms, collapse = "+")))
combined <- full_join(macro_predictors, datashare)
dmy <- dummyVars(f, data = combined)
combined <- full_join(macro_predictors, datashare, by = "Time")
dmy <- dummyVars(f, data = combined)
# This is pretty damn large at ~1.5GB. Interaction terms are brutal
final_dataset <- predict(dmy, combined)
colnames(final_dataset)
object.size(final_dataset)
ncol(final_dataset)
rm(final_dataset)
# This is pretty damn large at ~1.5GB. Interaction terms are brutal
final_dataset_x <- predict(dmy, combined)
