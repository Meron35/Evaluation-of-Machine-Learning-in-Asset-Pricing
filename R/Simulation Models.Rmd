---
title: "Simulation Models"
author: "Ze Yu Zhong"
date: "19 June 2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

################
##Load Libraries
################

library(glmnet)
library(tidyverse)
library(keras)
library(ggplot2)
library(caret)
library(forecast)
library(rlist)
library(plm)
library(Metrics)
library(ranger)
library(glmnetUtils)
library(tensorflow)
library(quantreg)
library(randomForestSRC)


#forestr
# This apparently needs admin privileges to install, do this at home maybe
# devtools::install_github("andeek/forestr")
# devtools::install_github("andeek/rpart")

set.seed(27935248)
```

```{r train_valid_test}
#Create Training + Test Sets

#$T = 180$ monthly periods corresponds to 15 years. The training sample was set to start from $T = 108$ or 9 years, a validation set 1 year in length. The last 3 years were reserved as a test set never to be used for validation or training.

#This means that there are only 3 sample periods to train on, yay

#Caret and other packages are not so good at dealing with custom training/validation/test sets. Have to specify resampling, tuning and testing manually (ugh)

customTimeSlices <- function(start, end, initialWindow, horizon, test_size) {
  total <- c(start:end)
  offset <- start - 1
  set_num <- (end - start + offset - test_size - initialWindow) / horizon
  
  time_slice <- list(train = 0, validation = 0, test = 0)
  time_slices <- rep(list(time_slice), 3)

  for (t in 1:set_num) {
    time_slice$train <- c(start:(initialWindow + (t-1) * horizon + offset))
    time_slice$validation <- c((initialWindow + (t-1) * horizon + offset + 1):(initialWindow + (t) * horizon + offset))
    time_slice$test <- c((initialWindow + (t) * horizon + offset + 1):end)
    
    time_slices[[t]] <- time_slice
  }
  return(time_slices)
}

#Create custom time slices
timeSlices <- customTimeSlices(start = 2, end = 181, initialWindow = 108, horizon = 12, test_size = 36)

#Formula Function, makes it easier for those packages with a formula interface

panel_formula <- function(panel){
  #Remove the first 3 colNames, as these correspond to the return, time and stock id
  panel_colnames <- colnames(panel)[-c(1:3)]
  f <- as.formula(c("rt ~", paste(panel_colnames, collapse = "+")))
  return(f)
}

#Use panel_g1_A1 generated from Simulation.rmd for now. Generalize it later

pooled_panel <- gu_et_al_g1[[1]]$panel

f <- panel_formula(pooled_panel)
```

```{r pooled_ols}
#Linear model wrt MSE

LM_fit_mse <- function(pooled_panel, timeSlices) {
  #Initialize Loss Function Statistics
  LM_stats <- rep(list(0), 3)
  
  for (set in 1:3) {
    LM_stats[[set]] <- list(loss_stats = data.frame(train_MAE = 0, train_MSE = 0, train_RMSE = 0, train_RSquare = 0, 
                                                    validation_MAE = 0, validation_MSE = 0, validation_RMSE = 0, validation_RSquare = 0, 
                                                    test_MAE = 0, test_MSE = 0, test_RMSE = 0, test_RSquare = 0),
                            #Other useful things
                            forecasts = 0,
                            forecast_resids = 0,
                            model = 0)
    
    #Load Training, validation and test sets
    train <- pooled_panel %>%
      filter(time %in% timeSlices[[set]]$train)
    validation <- pooled_panel %>%
      filter(time %in% timeSlices[[set]]$validation)
    test<- pooled_panel %>%
      filter(time %in% timeSlices[[set]]$test)
    
    #Train Model on training set
    lm <- lm(f, data = train)
    
    LM_stats[[set]]$model <- lm
    
    #No Tuning Needed
    
    #Statistics
    #Training Set
    LM_stats[[set]]$loss_stats$train_MAE <- mae(train$rt, predict(lm))
    LM_stats[[set]]$loss_stats$train_MSE <- mse(train$rt, predict(lm))
    LM_stats[[set]]$loss_stats$train_RMSE <- rmse(train$rt, predict(lm))
    LM_stats[[set]]$loss_stats$train_RSquare <- (1 - sse(train$rt, predict(lm, newdata = train)) / sum((train$rt - mean(train$rt))^2))
    
    #Validation Set Statistics
    LM_stats[[set]]$loss_stats$validation_MAE <- mae(validation$rt, predict(lm, newdata = validation))
    LM_stats[[set]]$loss_stats$validation_MSE <- mse(validation$rt, predict(lm, newdata = validation))
    LM_stats[[set]]$loss_stats$validation_RMSE <- rmse(validation$rt, predict(lm, newdata = validation))
    LM_stats[[set]]$loss_stats$validation_RSquare <- (1 - sse(validation$rt, predict(lm, newdata = validation)) / sum((validation$rt - mean(validation$rt))^2))
      
    #Test Set Statistics
    LM_stats[[set]]$loss_stats$test_MAE <- mae(test$rt, predict(lm, newdata = test))
    LM_stats[[set]]$loss_stats$test_MSE <- mse(test$rt, predict(lm, newdata = test))
    LM_stats[[set]]$loss_stats$test_RMSE <- rmse(test$rt, predict(lm, newdata = test))
    LM_stats[[set]]$loss_stats$test_RSquare <- (1 - sse(test$rt, predict(lm, newdata = test)) / sum((test$rt - mean(test$rt))^2))
        
    #Forecasts
    LM_stats[[set]]$forecasts <- predict(lm, newdata = test)
    #Forecast Residuals
    LM_stats[[set]]$forecast_resids <- test$rt - predict(lm, newdata = test)
  }
  return(LM_stats)
}

LM_stats <- LM_fit_mse(pooled_panel, timeSlices)

summary(LM_stats[[1]]$model)

LM_stats[[1]]$loss_stats
LM_stats[[2]]$loss_stats
LM_stats[[3]]$loss_stats
```

```{r}
#Linear Model with MAE loss function (aka quantile regression with target quantile of 0.5)

LM_MAE <- rq(f, tau = 0.5, data = train, 
             #Play around with method if speed is problem
             method = "br")

summary(LM_MAE)
  
#Turning this into a function
  
LM_fit_mae <- function(pooled_panel, timeSlices) {
  #Initialize Loss Function Statistics
  LM_stats <- rep(list(0), 3)
  
  for (set in 1:3) {
    LM_stats[[set]] <- list(loss_stats = data.frame(train_MAE = 0, train_MSE = 0, train_RMSE = 0, train_RSquare = 0, 
                                                validation_MAE = 0, validation_MSE = 0, validation_RMSE = 0, validation_RSquare = 0, 
                                                test_MAE = 0, test_MSE = 0, test_RMSE = 0, test_RSquare = 0),
                        #Other useful things
                        forecasts = 0,
                        forecast_resids = 0,
                        model = 0)
    
    #Load Training, validation and test sets
    train <- pooled_panel %>%
      filter(time %in% timeSlices[[set]]$train)
    validation <- pooled_panel %>%
      filter(time %in% timeSlices[[set]]$validation)
    test<- pooled_panel %>%
      filter(time %in% timeSlices[[set]]$test)
    
    #Train Model on training set
    lm <- rq(f, data = train, tau = 0.5, method = "br")
    
    LM_stats[[set]]$model <- lm
    
    #No Tuning Needed
    
    #Statistics
    #Training Set
    LM_stats[[set]]$loss_stats$train_MAE <- mae(train$rt, predict(lm))
    LM_stats[[set]]$loss_stats$train_MSE <- mse(train$rt, predict(lm))
    LM_stats[[set]]$loss_stats$train_RMSE <- rmse(train$rt, predict(lm))
    LM_stats[[set]]$loss_stats$train_RSquare <- (1 - sse(train$rt, predict(lm, newdata = train)) / sum((train$rt - mean(train$rt))^2))
    
    #Validation Set Statistics
    LM_stats[[set]]$loss_stats$validation_MAE <- mae(validation$rt, predict(lm, newdata = validation))
    LM_stats[[set]]$loss_stats$validation_MSE <- mse(validation$rt, predict(lm, newdata = validation))
    LM_stats[[set]]$loss_stats$validation_RMSE <- rmse(validation$rt, predict(lm, newdata = validation))
    LM_stats[[set]]$loss_stats$validation_RSquare <- (1 - sse(validation$rt, predict(lm, newdata = validation)) / sum((validation$rt - mean(validation$rt))^2))
      
    #Test Set Statistics
    LM_stats[[set]]$loss_stats$test_MAE <- mae(test$rt, predict(lm, newdata = test))
    LM_stats[[set]]$loss_stats$test_MSE <- mse(test$rt, predict(lm, newdata = test))
    LM_stats[[set]]$loss_stats$test_RMSE <- rmse(test$rt, predict(lm, newdata = test))
    LM_stats[[set]]$loss_stats$test_RSquare <- (1 - sse(test$rt, predict(lm, newdata = test)) / sum((test$rt - mean(test$rt))^2))
        
    #Forecasts
    LM_stats[[set]]$forecasts <- predict(lm, newdata = test)
    #Forecast Residuals
    LM_stats[[set]]$forecast_resids <- test$rt - predict(lm, newdata = test)
  }
  return(LM_stats)
}

LM_mae_stats <- LM_fit_mae(pooled_panel, timeSlices)

LM_mae_stats[[1]]$loss_stats
LM_mae_stats[[2]]$loss_stats
LM_mae_stats[[3]]$loss_stats

```



```{r elastic_net}
#Elasticnet

#Can't seem to find anything online that tunes ELN for you with specified validation sets, instead of straight-forward cross validation
#nfold and foldid require that ELN be performed with at least 3 fold validation - not possible in our context
#Double check with David maybe, on hold for now

#Elasticnet only has alpha and lambda to tune

#Just keep the grid small for now to make sure everything is running
ELN_grid <- expand.grid(
  alpha = seq(0, 1, 0.01),
  lambda = 10^seq(-3, 5, length.out = 50)
)

######################################
## Functions to tune an ELN model
######################################

# Fit the model over the defined grid of hyperparameters, and return it as a list
# Takes a defined grid of hyperparameters, a training set, and a validation set, fits the model over the grid, and returns the models + loss statistics as a list
# Works with both loss functions of MSE and MAE, need to specify these as "mse" or "mae" respectively

ELN_model_grid <- function(ELN_grid, train, validation, loss_function) {
  #Initialize List
  ELN_model_grid <- rep(list(0), nrow(ELN_grid))
  
  for (i in 1:nrow(ELN_grid)) {
    ELN_model_grid[[i]] <- list(ELN_grid = ELN_grid[i,], model = 0, train_loss = 0, validation_loss = 0)
    #Remember to load glmnetUtils package for a formula interface to glmnet
    #Fit the model using grid value
    ELN <- glmnet(formula = f, data = train, alpha = ELN_grid$alpha[i], lambda = ELN_grid$lambda[i])
    ELN_model_grid[[i]]$model <- ELN
    
    #MSE case
    if (loss_function == "mse") {
      #Training Error
      ELN_model_grid[[i]]$train_loss <- mse(train$rt, predict(ELN, newdata = train))
      #Validation Error
      ELN_model_grid[[i]]$validation_loss <- mse(validation$rt, predict(ELN, newdata = validation))
    }
    #MAE case
    else {
      #Training Error
      ELN_model_grid[[i]]$train_loss <- mae(train$rt, predict(ELN, newdata = train))
      #Validation Error
      ELN_model_grid[[i]]$validation_loss <- mae(validation$rt, predict(ELN, newdata = validation))
    }
  }
  return(ELN_model_grid)
}

# Given a list containing the model grid, return the "best" model according to validation loss
# Use in conjunction with previous function

get_ELN_best_tune <- function(ELN_grid, model_grid) {
  ELN_tune_dataframe <- cbind(ELN_grid, train_loss = rep(0, nrow(ELN_grid)), validation_loss = rep(0, nrow(ELN_grid)))
  for (i in 1:nrow(ELN_grid)) {
    #Training Error
    ELN_tune_dataframe$train_loss[i] <- model_grid[[i]]$train_loss
    #Validation Error
    ELN_tune_dataframe$validation_loss[i] <- model_grid[[i]]$validation_loss
  }
  #Return the row index with the lowest validation loss
  return(which.min(ELN_tune_dataframe$validation_loss))
}

# Final big wrapper function that gives an ELN_stats list with everything desired

ELN_fit_stats <- function(ELN_grid, timeSlices, pooled_panel, loss_function) {
  ELN_stats <- rep(list(0), 3)
  
  for (set in 1:3) {
    #Load Training, validation and test sets
    train <- pooled_panel %>%
      filter(time %in% timeSlices[[set]]$train)
    validation <- pooled_panel %>%
      filter(time %in% timeSlices[[set]]$validation)
    test<- pooled_panel %>%
      filter(time %in% timeSlices[[set]]$test)
      
    #Get models fit over the grid of hyperparameters
    ELN_model_grid <- ELN_model_grid(ELN_grid, train, validation, loss_function)
        
    #Get the best tuning parameters
    best_model_index <- get_ELN_best_tune(ELN_grid, ELN_model_grid)
    
    #Initialize stats list
    ELN_stats[[set]] <- list(loss_stats = data.frame(train_MAE = 0, train_MSE = 0, train_RMSE = 0, train_RSquare = 0, 
                         validation_MAE = 0, validation_MSE = 0, validation_RMSE = 0, validation_RSquare = 0, 
                         test_MAE = 0, test_MSE = 0, test_RMSE = 0, test_RSquare = 0),
                         #Other useful things
                         forecasts = 0,
                         forecast_resids = 0,
                         model = 0)
      
    #Model
    model <- ELN_model_grid[[best_model_index]]$model
    ELN_stats[[set]]$model <- ELN_model_grid[[best_model_index]]$model
      
    #Loss Stats Dataframe
    #Train
    ELN_stats[[set]]$loss_stats$train_MAE <- mae(train$rt, predict(model, newdata = train))
    ELN_stats[[set]]$loss_stats$train_MSE <- mse(train$rt, predict(model, newdata = train))
    ELN_stats[[set]]$loss_stats$train_RMSE <- rmse(train$rt, predict(model, newdata = train))
    ELN_stats[[set]]$loss_stats$train_RSquare <- (1 - sse(train$rt, predict(model, newdata = train)) / sum((train$rt - mean(train$rt))^2))
        
    #Validation
    ELN_stats[[set]]$loss_stats$validation_MAE <- mae(validation$rt, predict(model, newdata = validation))
    ELN_stats[[set]]$loss_stats$validation_MSE <- mse(validation$rt, predict(model, newdata = validation))
    ELN_stats[[set]]$loss_stats$validation_RMSE <- rmse(validation$rt, predict(model, newdata = validation))
    ELN_stats[[set]]$loss_stats$validation_RSquare <- (1 - sse(validation$rt, predict(model, newdata = validation)) / sum((validation$rt - mean(validation$rt))^2))
    
    #Test
    ELN_stats[[set]]$loss_stats$test_MAE <- mae(test$rt, predict(model, newdata = test))
    ELN_stats[[set]]$loss_stats$test_MSE <- mse(test$rt, predict(model, newdata = test))
    ELN_stats[[set]]$loss_stats$test_RMSE <- rmse(test$rt, predict(model, newdata = test))
    ELN_stats[[set]]$loss_stats$test_RSquare <- (1 - sse(test$rt, predict(model, newdata = test)) / sum((test$rt - mean(test$rt))^2))
      
    #Forecasts
    ELN_stats[[set]]$forecasts <- predict(model, newdata = test)
    
    #Forecast residuals
    ELN_stats[[set]]$forecast_resids <- test$rt - predict(model, newdata = test)
  }
  return(ELN_stats)
}

#Testing if function works
ELN_stats_mse <- ELN_fit_stats(ELN_grid, timeSlices, pooled_panel, loss_function = "mse")

ELN_stats_mae <- ELN_fit_stats(ELN_grid, timeSlices, pooled_panel, loss_function = "mae")

# Seeing if different loss functions actuallly do anything (they should)
# Note: make sure the grid is sufficiently large enough, otherwise the two different loss functions will often agree on the exact same hyperparameters

ELN_stats_mse[[1]]$loss_stats$test_MSE
ELN_stats_mse[[1]]$model

ELN_stats_mae[[1]]$loss_stats$test_MSE
ELN_stats_mae[[1]]$model
```

```{r random_forest}
#Random Forest

#Random Forest should also be fairly straightforward, only two tuning parameters mtry and ntree (usually)

#The vanilla randomForest package is very basic and too slow
#We will use the ranger package, which is apparently a C++ implementation of the random forest algorithm and should be faster
#Another alternative is to use XGBoost (which is very very fast) and set boosting rounds = 1, because in this case it should be a standard random forest

#Compute power is a real struggle, and the aim is just to get the code basis working for the moment. Very conservative grids are therefore used for now

#Check out randomForestSRC for an implementation of randomForests that allows for quantile regression trees. Quantile regression use a check loss function, which is a superset of absolute error
#forestr package doesn't seem to want to function on honours lab computers

#Default ntree = 500
#Default mtry = count(predictors/3) = 400/3 = 133

rf_grid <- expand.grid(
  ntree = seq(500, 1000, 1000),
  mtry = seq(10, 100, 10)
)

#Generate formula, actually same code from PLM earlier

#Initialize Loss Function Statistics

RF_stats <- data.frame(
  set = c(1:3),
  #Training
  train_MAE = c(1:3),
  train_MSE = c(1:3),
  train_RMSE = c(1:3),
  train_R2 = c(1:3),
  #Validation
  valid_MAE = c(1:3),
  valid_MSE = c(1:3),
  valid_RMSE = c(1:3),
  valid_R2 = c(1:3),
  #Test
  test_MAE = c(1:3),
  test_MSE = c(1:3),
  test_RMSE = c(1:3),
  test_R2 = c(1:3)
)

#Initialize Forecasts

RF_forecasts <- rep(list(0), 3)

#Initialize Models

RF_models <- rep(list(0), 3)

#Load training, validation and test sets

train <- pooled_panel %>%
  filter(time %in% timeSlices[[set]]$train)
validation <- pooled_panel %>%
  filter(time %in% timeSlices[[set]]$validation)
test<- pooled_panel %>%
  filter(time %in% timeSlices[[set]]$test)

#Train Model on Training Set

rf <- ranger(f, data = train, num.trees = rf_grid$ntree[1], mtry = rf_grid$mtry[1],
             write.forest = TRUE, min.node.size = 5, splitrule = "variance")

RF_models[[1]] <- 


for (set in 1:3) {
  
  #Training Set Statistics
    RF_stats$train_MAE[set] <- mae(train$return, predict(rf))
    RF_stats$train_MSE[set] <- mse(train$return, predict(rf))
    RF_stats$train_RMSE[set] <- rmse(train$return, predict(rf))
    #R squared still pending, haven't figured out how to do it yet fully
    
  #Validation Set Statistics
    RF_stats$valid_MAE[set] <- mae(validation$return, predict(rf, newdata = validation))
    RF_stats$valid_MSE[set] <- mse(validation$return, predict(rf, newdata = validation))
    RF_stats$valid_RMSE[set] <- rmse(validation$return, predict(rf, newdata = validation))
    #R squared still pending, haven't figured out how to do it yet fully
  
  #Test Set Statistics
    RF_stats$test_MAE[set] <- mae(test$return, predict(rf, newdata = test))
    RF_stats$test_MSE[set] <- mse(test$return, predict(rf, newdata = test))
    RF_stats$test_RMSE[set] <- rmse(test$return, predict(rf, newdata = test))
    #R squared still pending, haven't figured out how to do it yet fully
    
  #Forecasts
    RF_forecasts[[set]]$forecast <- predict(rf, newdata = test)
    RF_forecasts[[set]]$forecast_error <- test$return - predict(rf, newdata = test)
    
}

rf_MSE <- rfsrc(f, train, ntree = 500, 
                #Hyperparameters
                mtry = 10,
                nodesize = 5,
                #Other
                splitrule = "mse")

rf_MAE <- quantileReg(f, train, ntree = 500,
                      #Target quantile
                      prob = 0.5)

rf_MAE$quantileReg

predict(rf_MSE, newdata = validation)

plot(rf_MAE)

predict(rf_MAE, newdata = validation)

plot.variable(rf_MSE)
```
## Neural Networks WRT MSE

```{r neural_networks}
#Neural Networks

set.seed(27935248)

#Build neural networks with neuron numbers according to geometric pyramid rule, and ReLU activation function for all layers
# IE Input layer > 32 neurons > 16 neurons > 8 neurons > 4 neurons > 2 neurons > output

##Neural Network 1

train_x <- train[4:ncol(train)]
train_y <- train$rt

validation_x <- validation[4:ncol(validation)]
validation_y <- validation$rt

# The patience parameter is the amount of epochs to check for improvement.
early_stop <- callback_early_stopping(monitor = "val_loss", patience = 20)

#It's clearer to separate these "layers" out as much as possible
build_NN1 <- function(loss_function) {
  
  model <- keras_model_sequential() %>%
    # Layer 1
    layer_dense(units = 32, input_shape = ncol(train_x)) %>%
    layer_activation("relu") %>%
    layer_activity_regularization(l1 = 0.001) %>%
    layer_batch_normalization() %>%
    # Output Layer
    layer_dense(units = 1, activation = "linear")
  
  model %>% compile(
    #MSE loss function
    loss = loss_function,
    #We're using ADAM
    optimizer = "adam",
    metrics = list("mean_absolute_error", "mean_squared_error")
  )
  model
}

## WRT MSE

neural_network_1_mse <- build_NN1("mse")
neural_network_1_mse %>% summary()

#Just do something straightforward for now

neural_network_1_mse %>% fit(as.matrix(train_x), as.matrix(train_y), 
                             epochs = 100, verbose = 1, 
                             validation_data = list(as.matrix(validation_x), as.matrix(validation_y)),
                             callbacks = list(early_stop))

#Make predictions
test_x <- test[4:ncol(test)]

NN1_mse_predictions <- neural_network_1_mse %>% predict(as.matrix(test_x))

mae(test$rt, NN1_mse_predictions)
mse(test$rt, NN1_mse_predictions)

###########################################
## WRT MAE

neural_network_1_mae <- build_NN1("mae")
neural_network_1_mae %>% summary()

#Just do something straightforward for now

neural_network_1_mae %>% fit(as.matrix(train_x), as.matrix(train_y), 
                             epochs = 100, verbose = 1, 
                             validation_data = list(as.matrix(validation_x), as.matrix(validation_y)),
                             callbacks = list(early_stop))

#Make predictions
test_x <- test[4:ncol(test)]

NN1_mae_predictions <- neural_network_1_mae %>% predict(as.matrix(test_x))

mae(test$rt, NN1_mae_predictions)
mse(test$rt, NN1_mae_predictions)
```

```{r}
# Neural Network 2

train_x <- train[4:ncol(train)]
train_y <- train$rt

validation_x <- validation[4:ncol(validation)]
validation_y <- validation$rt

# The patience parameter is the amount of epochs to check for improvement.
early_stop <- callback_early_stopping(monitor = "val_loss", patience = 20)

#It's clearer to separate these "layers" out as much as possible
build_NN2 <- function(loss_function) {
  
  model <- keras_model_sequential() %>%
    # Layer 1
    layer_dense(units = 32, input_shape = ncol(train_x)) %>%
    layer_activation("relu") %>%
    layer_batch_normalization() %>%
    # Layer 2
    layer_dense(units = 16) %>%
    layer_activation("relu") %>%
    layer_batch_normalization() %>%
    # Output Layer
    layer_dense(units = 1, activation = "linear")
  
  model %>% compile(
    #MSE loss function
    loss = loss_function,
    #We're using ADAM
    optimizer = "adam",
    metrics = list("mean_absolute_error", "mean_squared_error")
  )
  model
}

## WRT MSE

neural_network_2_mse <- build_NN2("mse")
neural_network_2_mse %>% summary()

#Just do something straightforward for now

neural_network_2_mse %>% fit(as.matrix(train_x), as.matrix(train_y), 
                             epochs = 100, verbose = 1, 
                             validation_data = list(as.matrix(validation_x), as.matrix(validation_y)),
                             callbacks = list(early_stop))

#Make predictions
test_x <- test[4:ncol(test)]

NN2_mse_predictions <- neural_network_2_mse %>% predict(as.matrix(test_x))

mae(test$rt, NN1_mse_predictions)
mse(test$rt, NN1_mse_predictions)

###########################################
## WRT MAE

neural_network_2_mae <- build_NN2("mae")
neural_network_2_mae %>% summary()

#Just do something straightforward for now

neural_network_2_mae %>% fit(as.matrix(train_x), as.matrix(train_y), 
                             epochs = 100, verbose = 1, 
                             validation_data = list(as.matrix(validation_x), as.matrix(validation_y)),
                             callbacks = list(early_stop))

#Make predictions
test_x <- test[4:ncol(test)]

NN2_mae_predictions <- neural_network_2_mae %>% predict(as.matrix(test_x))

mae(test$rt, NN2_mae_predictions)
mse(test$rt, NN2_mae_predictions)
```

```{r}
# Neural Network 3

train_x <- train[4:ncol(train)]
train_y <- train$rt

validation_x <- validation[4:ncol(validation)]
validation_y <- validation$rt

#It's clearer to separate these "layers" out as much as possible
build_NN3 <- function(loss_function) {
  
  model <- keras_model_sequential() %>%
    # Layer 1
    layer_dense(units = 32, input_shape = ncol(train_x)) %>%
    layer_activation("relu") %>%
    layer_batch_normalization() %>%
    # Layer 2
    layer_dense(units = 16) %>%
    layer_activation("relu") %>%
    layer_batch_normalization() %>%
    #Layer 3
    layer_dense(units = 8) %>%
    layer_activation("relu") %>%
    layer_batch_normalization() %>%
    # Output Layer
    layer_dense(units = 1, activation = "linear")
  
  model %>% compile(
    #MSE loss function
    loss = loss_function,
    #We're using ADAM
    optimizer = "adam",
    metrics = list("mean_absolute_error", "mean_squared_error")
  )
  model
}

## WRT MSE

neural_network_3_mse <- build_NN3("mse")
neural_network_3_mse %>% summary()

#Just do something straightforward for now

neural_network_3_mse %>% fit(as.matrix(train_x), as.matrix(train_y), 
                             epochs = 100, verbose = 1, 
                             validation_data = list(as.matrix(validation_x), as.matrix(validation_y)),
                             callbacks = list(early_stop))

#Make predictions
test_x <- test[4:ncol(test)]

NN3_mse_predictions <- neural_network_3_mse %>% predict(as.matrix(test_x))

mae(test$rt, NN3_mse_predictions)
mse(test$rt, NN3_mse_predictions)

###########################################
## WRT MAE

neural_network_3_mae <- build_NN2("mae")
neural_network_3_mae %>% summary()

#Just do something straightforward for now

neural_network_3_mae %>% fit(as.matrix(train_x), as.matrix(train_y), 
                             epochs = 100, verbose = 1, 
                             validation_data = list(as.matrix(validation_x), as.matrix(validation_y)),
                             callbacks = list(early_stop))

#Make predictions
test_x <- test[4:ncol(test)]

NN3_mae_predictions <- neural_network_3_mae %>% predict(as.matrix(test_x))

mae(test$rt, NN3_mae_predictions)
mse(test$rt, NN3_mae_predictions)
```

```{r}
# Neural Network 4

train_x <- train[4:ncol(train)]
train_y <- train$rt

validation_x <- validation[4:ncol(validation)]
validation_y <- validation$rt

# The patience parameter is the amount of epochs to check for improvement.
early_stop <- callback_early_stopping(monitor = "val_loss", patience = 20)

#It's clearer to separate these "layers" out as much as possible
build_NN4 <- function(loss_function) {
  
  model <- keras_model_sequential() %>%
    # Layer 1
    layer_dense(units = 32, input_shape = ncol(train_x)) %>%
    layer_activation("relu") %>%
    layer_batch_normalization() %>%
    # Layer 2
    layer_dense(units = 16) %>%
    layer_activation("relu") %>%
    layer_batch_normalization() %>%
    #Layer 3
    layer_dense(units = 8) %>%
    layer_activation("relu") %>%
    layer_batch_normalization() %>%
    #Layer 4
    layer_dense(units = 4) %>%
    layer_activation("relu") %>%
    layer_batch_normalization() %>%
    # Output Layer
    layer_dense(units = 1, activation = "linear")
  
  model %>% compile(
    #MSE loss function
    loss = loss_function,
    #We're using ADAM
    optimizer = "adam",
    metrics = list("mean_absolute_error", "mean_squared_error")
  )
  model
}

## WRT MSE

neural_network_4_mse <- build_NN4("mse")
neural_network_4_mse %>% summary()

#Just do something straightforward for now

neural_network_4_mse %>% fit(as.matrix(train_x), as.matrix(train_y), 
                             epochs = 100, verbose = 1, 
                             validation_data = list(as.matrix(validation_x), as.matrix(validation_y)),
                             callbacks = list(early_stop))

#Make predictions
test_x <- test[4:ncol(test)]

NN4_mse_predictions <- neural_network_4_mse %>% predict(as.matrix(test_x))

mae(test$rt, NN4_mse_predictions)
mse(test$rt, NN4_mse_predictions)

###########################################
## WRT MAE

neural_network_4_mae <- build_NN2("mae")
neural_network_4_mae %>% summary()

#Just do something straightforward for now

neural_network_4_mae %>% fit(as.matrix(train_x), as.matrix(train_y), 
                             epochs = 100, verbose = 1, 
                             validation_data = list(as.matrix(validation_x), as.matrix(validation_y)),
                             callbacks = list(early_stop))

#Make predictions
test_x <- test[4:ncol(test)]

NN4_mae_predictions <- neural_network_4_mae %>% predict(as.matrix(test_x))

mae(test$rt, NN4_mae_predictions)
mse(test$rt, NN4_mae_predictions)
```

```{r}
# Neural Network 5

train_x <- train[4:ncol(train)]
train_y <- train$rt

validation_x <- validation[4:ncol(validation)]
validation_y <- validation$rt

# The patience parameter is the amount of epochs to check for improvement.
early_stop <- callback_early_stopping(monitor = "val_loss", patience = 20)

#It's clearer to separate these "layers" out as much as possible
build_NN5 <- function(loss_function) {
  
  model <- keras_model_sequential() %>%
    # Layer 1
    layer_dense(units = 32, input_shape = ncol(train_x)) %>%
    layer_activation("relu") %>%
    layer_batch_normalization() %>%
    # Layer 2
    layer_dense(units = 16) %>%
    layer_activation("relu") %>%
    layer_batch_normalization() %>%
    #Layer 3
    layer_dense(units = 8) %>%
    layer_activation("relu") %>%
    layer_batch_normalization() %>%
    #Layer 4
    layer_dense(units = 4) %>%
    layer_activation("relu") %>%
    layer_batch_normalization() %>%
    #Layer 5
    layer_dense(units = 2) %>%
    layer_activation("relu") %>%
    layer_batch_normalization() %>%
    # Output Layer
    layer_dense(units = 1, activation = "linear")
  
  model %>% compile(
    #MSE loss function
    loss = loss_function,
    #We're using ADAM
    optimizer = "adam",
    metrics = list("mean_absolute_error", "mean_squared_error")
  )
  model
}

## WRT MSE

neural_network_5_mse <- build_NN5("mse")
neural_network_5_mse %>% summary()

#Just do something straightforward for now

neural_network_5_mse %>% fit(as.matrix(train_x), as.matrix(train_y), 
                             epochs = 100, verbose = 1, 
                             validation_data = list(as.matrix(validation_x), as.matrix(validation_y)),
                             callbacks = list(early_stop))

#Make predictions
test_x <- test[5:ncol(test)]

NN5_mse_predictions <- neural_network_5_mse %>% predict(as.matrix(test_x))

mae(test$rt, NN5_mse_predictions)
mse(test$rt, NN5_mse_predictions)

###########################################
## WRT MAE

neural_network_5_mae <- build_NN2("mae")
neural_network_5_mae %>% summary()

#Just do something straightforward for now

neural_network_5_mae %>% fit(as.matrix(train_x), as.matrix(train_y), 
                             epochs = 100, verbose = 1, 
                             validation_data = list(as.matrix(validation_x), as.matrix(validation_y)),
                             callbacks = list(early_stop))

#Make predictions
test_x <- test[4:ncol(test)]

NN5_mae_predictions <- neural_network_5_mae %>% predict(as.matrix(test_x))

mae(test$rt, NN5_mae_predictions)
mse(test$rt, NN5_mae_predictions)
```

