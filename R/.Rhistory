# Change this to 9 + 3 + 3, maybe more stable procedure
customTimeSlices <- function(start, initialWindow, horizon, validation_size, test_size, set_no) {
time_slice <- list(train = 0, validation = 0, test = 0)
time_slices <- rep(list(time_slice), set_no)
for (t in 1:set_no) {
time_slice$train <- c(start:(initialWindow + (t-1) * horizon + 1))
time_slice$validation <- c((initialWindow + (t-1) * horizon + 2):((initialWindow + (t-1) * horizon) + validation_size + 1))
time_slice$test <- c((initialWindow + (t-1) * horizon) + validation_size + 2):((initialWindow + (t-1) * horizon) + validation_size + test_size + 1)
time_slices[[t]] <- time_slice
}
time_slices
}
#Create custom time slices
# These parameters give you the following train/validation/test splits (in terms of years)
# 6 4 3
# 7 4 3
# 8 4 3
timeSlices <- customTimeSlices(start = 2, initialWindow = 84, horizon = 12, validation_size = 60, test_size = 12, set_no = 3)
#Formula Function, makes it easier for those packages with a formula interface
panel_formula <- function(panel){
#Remove the first 3 colNames, as these correspond to the return, time and stock id
panel_colnames <- colnames(panel)[-c(1:3)]
f <- as.formula(c("rt ~", paste(panel_colnames, collapse = "+")))
return(f)
}
g1_A1_sv_0.1 <- readRDS("~/GitHub/Evaluation-of-Machine-Learning-in-Asset-Pricing/R/Simulated_dataset/g1_A1_sv_0.1.RDS")
pooled_panel <- g1_A1_sv_0.1[[1]]$panel
#################################################################################################################
## FFORMA implementation, pretending that each stock return series is an individual time series
## External regressors are only avialable through dimensional reduction techniques
## Not ideal at all, but the only practical way to implement this
#################################################################################################################
## function which extracts fforma features for a single univariate time series, and returns it as a dataframe
fforma_features <- function(time_series) {
cbind(
# Series length
length = length(time_series),
# nperiods, seasonal_period, trend, seasonality, linearity, curvature, spikiness e_acr1 and e_acf10
tsfeatures(time_series, features = "stl_features"),
# Stability
tsfeatures(time_series, features = "stability"),
# Lumpiness
tsfeatures(time_series, features = "lumpiness"),
# Hurst
tsfeatures(time_series, features = "hurst"),
# Nonlinearity
tsfeatures(time_series, features = "nonlinearity"),
# Holt alpha and beta
tsfeatures(time_series, features = "holt_parameters"),
# HW alpha and beta, only applies for seasonal time series
#tsfeatures(time_series, features = "hw_parameters"),
# Unitroot statistics
tsfeatures(time_series, features = c("unitroot_kpss", "unitroot_pp")),
# ACF features
tsfeatures(time_series, features = "acf_features"),
# PACF features
tsfeatures(time_series, features = "pacf_features"),
# Crossing point
tsfeatures(time_series, features = "crossing_points"),
# flat_spots
tsfeatures(time_series, features = "flat_spots"),
# arch_lm
tsfeatures(time_series, features = "arch_stat"),
# Heterogeneity
tsfeatures(time_series, features = "heterogeneity"),
####
## Extra Features, as it seems the above are not very sueful
tsfeatures(time_series, features = "std1st_der"),
tsfeatures(time_series, features = "outlierinclude_mdrmd"),
tsfeatures(time_series, features = "localsimple_taures"),
tsfeatures(time_series, features = "walker_propcross"),
tsfeatures(time_series, features = c("max_level_shift", "max_var_shift", "max_kl_shift"))
)
}
fforma_fit_individual <- function(panel, timeSlices, set) {
## Filter Dataset first
pooled_panel_train <- pooled_panel %>%
filter(time %in% timeSlices[[set]]$train)
pooled_panel_validation <- pooled_panel %>%
filter(time %in% timeSlices[[set]]$validation)
pooled_panel_test <- pooled_panel %>%
filter(time %in% timeSlices[[set]]$test)
out.sample_length <- length(timeSlices[[set]]$validation) + length(timeSlices[[set]]$test)
valid_index <- 1:length(timeSlices[[set]]$validation)
test_index <- (1:out.sample_length)[-valid_index]
## Dimensional Reduction of dataset regressors
umap_model <- umap(pooled_panel_train %>% dplyr::select(-rt, -stock, -time), ret_model = TRUE)
pooled_panel_train_umap <- umap_model$embedding
pooled_panel_validation_umap <- umap_transform(pooled_panel_validation %>%
dplyr::select(-rt, -stock, -time), model = umap_model)
pooled_panel_test_umap <- umap_transform(pooled_panel_test %>%
dplyr::select(-rt, -stock, -time), model = umap_model)
##
dataset_list <- foreach(i = 1:length(unique(pooled_panel$stock))) %dopar% {
stock_id <- unique(pooled_panel$stock)
time_series <- pooled_panel_train %>%
filter(stock == stock_id[i]) %>%
dplyr::select(rt) %>%
ts()
time_series_umap <- pooled_panel_train_umap %>%
data.frame() %>%
mutate(stock = pooled_panel_train$stock) %>%
filter(stock == stock_id[i]) %>%
dplyr::select(-stock)
time_series_validation <- pooled_panel_validation %>%
filter(stock == stock_id[i]) %>%
dplyr::select(rt) %>%
ts()
time_series_validation_umap <- pooled_panel_validation_umap %>%
data.frame() %>%
mutate(stock = pooled_panel_validation$stock) %>%
filter(stock == stock_id[i]) %>%
dplyr::select(-stock)
time_series_test <- pooled_panel_test %>%
filter(stock == stock_id[i]) %>%
dplyr::select(rt) %>%
ts()
time_series_test_umap <- pooled_panel_test_umap %>%
data.frame() %>%
mutate(stock = pooled_panel_test$stock) %>%
filter(stock == stock_id[i]) %>%
dplyr::select(-stock)
## Combining all sets, required for rugarch to function correctly
time_series_all <- c(time_series, time_series_validation, time_series_test) %>%
ts()
time_series_umap_all <- rbind(time_series_umap, time_series_validation_umap, time_series_test_umap)
############################################
## Estimate the constituent models first
############################################
# Naive
naive_model <- naive(time_series, h = out.sample_length)
# Random walk with drift
random_walk_model <- rwf(time_series, h = out.sample_length, drift = TRUE)
# Theta Method
theta_model <- thetaf(time_series, h = out.sample_length)
## Auto arima
auto_arima_model <- auto.arima(time_series)
# Auto ETS
auto_ets_model <- ets(time_series)
# TBATS
tbats_model <- tbats(time_series)
# Neural Network time series forecasts
nnetar_model <- nnetar(time_series)
# ARMA 1, 1 with GARCH 1, 1 ged errors, no external regressors
ugarch_spec <- ugarchspec(variance.model = list(model = "sGARCH",
garchOrder = c(1, 1),
submodel = NULL,
external.regressors = NULL,
variance.targeting = FALSE),
mean.model     = list(armaOrder = c(1, 1),
include.mean = TRUE,
external.regressors = NULL),
distribution.model = "ged")
ugarch_spec_fit <- ugarchfit(ugarch_spec, data = time_series_all, out.sample = out.sample_length,
solver = "hybrid")
ugarch_forecast <- ugarchforecast(ugarch_spec_fit, n.ahead = 1, n.roll = out.sample_length - 1)
# ARMA 1, 1 with GARCH 1, 1 ged errors with umap as external regressors
ugarch_spec_umap <- ugarchspec(variance.model = list(model = "sGARCH",
garchOrder = c(1, 1),
submodel = NULL,
external.regressors = time_series_umap_all,
variance.targeting = FALSE),
mean.model     = list(armaOrder = c(1, 1),
include.mean = TRUE,
external.regressors = time_series_umap_all),
distribution.model = "norm")
ugarch_spec_fit_umap <- ugarchfit(ugarch_spec, data = time_series_all, out.sample = out.sample_length,
solver = "hybrid")
ugarch_forecast_umap <- ugarchforecast(ugarch_spec_fit, n.ahead = 1, n.roll = out.sample_length - 1,
external.forecasts = rbind(time_series_validation_umap, time_series_test_umap) %>% list())
## Use the estimated models to generate actual forecasts for validation and test
## No easy way to organise this, just following original FFORMA structure of a list for each time series
## each containing an ff matrix with F rows and h columns, with F being the number of forecast methods
list(
validation_forecasts = rbind(
predict(naive_model)$mean[valid_index],
predict(random_walk_model)$mean[valid_index],
predict(theta_model)$mean[valid_index],
forecast(auto_arima_model, h = out.sample_length)$mean[valid_index],
forecast(tbats_model, h = out.sample_length)$mean[valid_index],
forecast(nnetar_model, h = out.sample_length)$mean[valid_index],
fitted(ugarch_forecast)[valid_index],
fitted(ugarch_forecast_umap)[valid_index]
),
validation_error_mse = data.frame(
naive = mse(time_series_validation, predict(naive_model)$mean[valid_index]),
random_walk = mse(time_series_validation, predict(random_walk_model)$mean[valid_index]),
theta = mse(time_series_validation, predict(theta_model)$mean[valid_index]),
auto_arima = mse(time_series_validation, forecast(auto_arima_model, h = out.sample_length)$mean[valid_index]),
tbats = mse(time_series_validation, forecast(tbats_model, h = out.sample_length)$mean[valid_index]),
nnetar = mse(time_series_validation, forecast(nnetar_model, h = out.sample_length)$mean[valid_index]),
garch = mse(time_series_validation, fitted(ugarch_forecast)[valid_index]),
garch_umap = mse(time_series_validation, fitted(ugarch_forecast_umap)[valid_index])
),
validation_error_mae = data.frame(
naive = mae(time_series_validation, predict(naive_model)$mean[valid_index]),
random_walk = mae(time_series_validation, predict(random_walk_model)$mean[valid_index]),
theta = mae(time_series_validation, predict(theta_model)$mean[valid_index]),
auto_arima = mae(time_series_validation, forecast(auto_arima_model, h = out.sample_length)$mean[valid_index]),
tbats = mae(time_series_validation, forecast(tbats_model, h = out.sample_length)$mean[valid_index]),
nnetar = mae(time_series_validation, forecast(nnetar_model, h = out.sample_length)$mean[valid_index]),
garch = mae(time_series_validation, fitted(ugarch_forecast)[valid_index]),
garch_umap = mae(time_series_validation, fitted(ugarch_forecast_umap)[valid_index])
),
validation_error_rsquare = data.frame(
naive = R2(predict(naive_model)$mean[valid_index], time_series_validation %>% c(),
form = "traditional"),
random_walk = R2(predict(random_walk_model)$mean[valid_index], time_series_validation %>% c(),
form = "traditional"),
theta = R2(predict(theta_model)$mean[valid_index], time_series_validation %>% c(),
form = "traditional"),
auto_arima = R2(forecast(auto_arima_model, h = out.sample_length)$mean[valid_index], time_series_validation %>% c(),
form = "traditional"),
tbats = R2(forecast(tbats_model, h = out.sample_length)$mean[valid_index], time_series_validation %>% c(),
form = "traditional"),
nnetar = R2(forecast(nnetar_model, h = out.sample_length)$mean[valid_index], time_series_validation %>% c(),
form = "traditional"),
garch = R2(fitted(ugarch_forecast)[valid_index], time_series_validation %>% c(),
form = "traditional"),
garch_umap = R2(fitted(ugarch_forecast_umap)[valid_index], time_series_validation %>% c(),
form = "traditional")
),
test_forecasts = rbind(
predict(naive_model)$mean[test_index],
predict(random_walk_model)$mean[test_index],
predict(theta_model)$mean[test_index],
forecast(auto_arima_model, h = out.sample_length)$mean[test_index],
forecast(tbats_model, h = out.sample_length)$mean[test_index],
forecast(nnetar_model, h = out.sample_length)$mean[test_index],
fitted(ugarch_forecast)[test_index],
fitted(ugarch_forecast_umap)[test_index]
),
test_error_mse = data.frame(
naive = mse(time_series_test, predict(naive_model)$mean[test_index]),
random_walk = mse(time_series_test, predict(random_walk_model)$mean[test_index]),
theta = mse(time_series_test, predict(theta_model)$mean[test_index]),
auto_arima = mse(time_series_test, forecast(auto_arima_model, h = out.sample_length)$mean[test_index]),
tbats = mse(time_series_test, forecast(tbats_model, h = out.sample_length)$mean[test_index]),
nnetar = mse(time_series_test, forecast(nnetar_model, h = out.sample_length)$mean[test_index]),
garch = mse(time_series_test, fitted(ugarch_forecast)[test_index]),
garch_umap = mse(time_series_test, fitted(ugarch_forecast_umap)[test_index])
),
test_error_mae = data.frame(
naive = mse(time_series_test, predict(naive_model)$mean[test_index]),
random_walk = mse(time_series_test, predict(random_walk_model)$mean[test_index]),
theta = mse(time_series_test, predict(theta_model)$mean[test_index]),
auto_arima = mse(time_series_test, forecast(auto_arima_model, h = out.sample_length)$mean[test_index]),
tbats = mse(time_series_test, forecast(tbats_model, h = out.sample_length)$mean[test_index]),
nnetar = mse(time_series_test, forecast(nnetar_model, h = out.sample_length)$mean[test_index]),
garch = mse(time_series_test, fitted(ugarch_forecast)[test_index]),
garch_umap = mse(time_series_test, fitted(ugarch_forecast_umap)[test_index])
),
test_error_rsquare = data.frame(
naive = R2(predict(naive_model)$mean[test_index], time_series_test %>% c(),
form = "traditional"),
random_walk = R2(predict(random_walk_model)$mean[test_index], time_series_test %>% c(),
form = "traditional"),
theta = R2(predict(theta_model)$mean[test_index], time_series_test %>% c(),
form = "traditional"),
auto_arima = R2(forecast(auto_arima_model, h = out.sample_length)$mean[test_index], time_series_test %>% c(),
form = "traditional"),
tbats = R2(forecast(tbats_model, h = out.sample_length)$mean[test_index], time_series_test %>% c(),
form = "traditional"),
nnetar = R2(forecast(nnetar_model, h = out.sample_length)$mean[test_index], time_series_test %>% c(),
form = "traditional"),
garch = R2(fitted(ugarch_forecast)[test_index], time_series_test%>% c(),
form = "traditional"),
garch_umap = R2(fitted(ugarch_forecast_umap)[test_index], time_series_test %>% c(),
form = "traditional")
)
)
}
dataset_list
}
## Testing this function
## It's working!
set_1 <- fforma_fit_individual(pooled_panel, timeSlices, set = 1)
set_1[[10]]$validation_error_mae
## Function to return a data matrix with features extracted via tsfeatures, one series per row, given panel data
# Remeber that this function doesn't do train/validation/split, so you'll have to do these outside of it
fforma_data_matrix <- function(pooled_panel) {
data <- foreach(i = 1:length(unique(pooled_panel$stock)), .combine = "rbind") %do% {
stock_id <- unique(pooled_panel$stock)
univariate_time_series <- pooled_panel %>%
filter(stock == stock_id[i]) %>%
dplyr::select(rt) %>%
ts()
fforma_features(univariate_time_series)
}
data
}
fforma_errors <- function(fforma_fit, loss_function) {
foreach(i = 1:length(fforma_fit), .combine = "rbind") %do% {
# MSE
if (loss_function == "mse") {
fforma_fit[[i]]$validation_error_mse
} else if (loff_function == "mae") {
fforma_fit[[i]]$validation_error_mae
}
}
}
fforma_labels <- function(errors) {
foreach(i = 1:nrow(errors), .combine = "rbind") %dopar% {
data.frame(label = which.min(errors[i, ]) - 1)
}
}
fforma_data_list <- function(pooled_panel, loss_function) {
list(data = fforma_data_matrix(pooled_panel),
errors = fforma_errors(set_1, loss_function),
labels = fforma_labels(errors))
}
train_list <- pooled_panel %>%
filter(time %in% timeSlices[[1]]$train) %>%
fforma_data_list(loss_function = "mse")
train_list$data
validation_list <- pooled_panel %>%
filter(time %in% timeSlices[[1]]$validation) %>%
fforma_data_list(loss_function = "mse")
test_list <- pooled_panel %>%
filter(time %in% timeSlices[[1]]$test) %>%
fforma_data_list(loss_function = "mse")
validation_list$data
#Create custom time slices
timeSlices <- customTimeSlices(start = 2, initialWindow = 96, horizon = 12, validation_size = 48, test_size = 12, set_no = 3)
## Testing this function
## It's working!
set_1 <- fforma_fit_individual(pooled_panel, timeSlices, set = 1)
set_1[[10]]$validation_error_mae
train_list <- pooled_panel %>%
filter(time %in% timeSlices[[1]]$train) %>%
fforma_data_list(loss_function = "mse")
validation_list <- pooled_panel %>%
filter(time %in% timeSlices[[1]]$validation) %>%
fforma_data_list(loss_function = "mse")
test_list <- pooled_panel %>%
filter(time %in% timeSlices[[1]]$test) %>%
fforma_data_list(loss_function = "mse")
train_list$data
softmax_transform <- function(x) {
exp(x) / sum(exp(x))
}
error_softmax_obj <- function(preds, dtrain) {
labels <- xgboost::getinfo(dtrain, "label")
errors <- attr(dtrain, "errors")
preds <- exp(preds)
sp <- rowSums(preds)
preds <- preds / replicate(ncol(preds), sp)
rowsumerrors <- replicate(ncol(preds), rowSums(preds * errors))
grad <- preds*(errors - rowsumerrors)
# Upper Bound for Hessian
#hess <- errors*preds*(1.0-preds) - grad*preds
# Correct Hessian
hess <- grad*(1.0 - 2.0*preds)
hess <- pmax(hess, 1e-16)
return(list(grad = t(grad), hess = t(hess)))
}
param <- list(booster = "gbtree",
max_depth = 6, eta = 0.3, nthread = 12,
num_class = ncol(errors),
objective = error_softmax_obj,
subsample = 0.5,
colsample_bytree = 0.1)
dtrain <- xgb.DMatrix(data = as.matrix(train_list$data),
label = train_list$labels[, 1])
attr(dtrain, "errors") <- train_list$errors
bst <- xgboost::xgb.train(params = param, data = dtrain, nrounds = 400, verbose = 1)
attr(dtrain, "errors") <- train_list$errors
bst <- xgboost::xgb.train(params = param, data = dtrain, nrounds = 400, verbose = 1)
error_softmax_obj <- function(preds, dtrain) {
labels <- xgboost::getinfo(dtrain, "label")
errors <- attr(dtrain, "errors")
preds <- exp(preds)
sp <- rowSums(preds)
preds <- preds / replicate(ncol(preds), sp)
rowsumerrors <- replicate(ncol(preds), rowSums(preds * errors))
grad <- preds*(errors - rowsumerrors)
# Upper Bound for Hessian
hess <- errors*preds*(1.0-preds) - grad*preds
# Correct Hessian
#hess <- grad*(1.0 - 2.0*preds)
#hess <- pmax(hess, 1e-16)
return(list(grad = t(grad), hess = t(hess)))
}
param <- list(booster = "gbtree",
max_depth = 6, eta = 0.3, nthread = 12,
num_class = ncol(errors),
objective = error_softmax_obj,
subsample = 0.5,
colsample_bytree = 0.1)
dtrain <- xgb.DMatrix(data = as.matrix(train_list$data),
label = train_list$labels[, 1])
attr(dtrain, "errors") <- train_list$errors
bst <- xgboost::xgb.train(params = param, data = dtrain, nrounds = 400, verbose = 1)
train_list$labels
[, 1]
train_list$labels[, 1]
param <- list(booster = "gbtree",
max_depth = 6, eta = 0.3, nthread = 12,
objective = error_softmax_obj,
subsample = 0.5,
colsample_bytree = 0.1)
dtrain <- xgb.DMatrix(data = as.matrix(train_list$data),
label = train_list$labels[, 1])
attr(dtrain, "errors") <- train_list$errors
bst <- xgboost::xgb.train(params = param, data = dtrain, nrounds = 400, verbose = 1)
bst <- xgboost::xgb.train(params = param, data = dtrain, nrounds = 400, verbose = 1)
param <- list(booster = "gbtree",
max_depth = 6, eta = 0.3, nthread = 12,
num_class = ncol(errors),
objective = error_softmax_obj,
subsample = 0.5,
colsample_bytree = 0.1)
dtrain <- xgb.DMatrix(data = as.matrix(train_list$data),
label = train_list$labels[, 1])
attr(dtrain, "errors") <- train_list$errors
bst <- xgboost::xgb.train(params = param, data = dtrain, nrounds = 400, verbose = 1)
dtrain
softmax_transform <- function(x) {
exp(x) / sum(exp(x))
}
error_softmax_obj <- function(preds, dtrain) {
labels <- xgboost::getinfo(dtrain, "label")
errors <- attr(dtrain, "errors")
preds <- exp(preds)
sp <- rowSums(preds)
preds <- preds / replicate(ncol(preds), sp)
rowsumerrors <- replicate(ncol(preds), rowSums(preds * errors))
grad <- preds*(errors - rowsumerrors)
# Upper Bound for Hessian
hess <- errors*preds*(1.0-preds) - grad*preds
# Correct Hessian
#hess <- grad*(1.0 - 2.0*preds)
#hess <- pmax(hess, 1e-16)
return(list(grad = t(grad), hess = t(hess)))
}
param <- list(booster = "gbtree",
max_depth = 6, eta = 0.3, nthread = 12,
num_class = ncol(errors),
objective = error_softmax_obj,
subsample = 0.5,
colsample_bytree = 0.1)
dtrain <- xgb.DMatrix(data = as.matrix(train_list$data),
label = train_list$labels[, 1])
attr(dtrain, "errors") <- train_list$errors
bst <- xgboost::xgb.train(params = param, data = dtrain, nrounds = 400, verbose = 1)
train_list$data %>% dim
bst <- xgboost::xgb.train(params = param, data = dtrain, nrounds = 200, verbose = 1)
## Testing this function
## It's working!
set_1 <- fforma_fit_individual(pooled_panel, timeSlices, set = 1)
## Testing this function
## It's working!
set_1 <- fforma_fit_individual(pooled_panel, timeSlices, set = 1)
set_1[[10]]$validation_error_mae
## Function to return a data matrix with features extracted via tsfeatures, one series per row, given panel data
# Remeber that this function doesn't do train/validation/split, so you'll have to do these outside of it
fforma_data_matrix <- function(pooled_panel) {
data <- foreach(i = 1:length(unique(pooled_panel$stock)), .combine = "rbind") %do% {
stock_id <- unique(pooled_panel$stock)
univariate_time_series <- pooled_panel %>%
filter(stock == stock_id[i]) %>%
dplyr::select(rt) %>%
ts()
fforma_features(univariate_time_series)
}
data
}
fforma_errors <- function(fforma_fit, loss_function) {
foreach(i = 1:length(fforma_fit), .combine = "rbind") %do% {
# MSE
if (loss_function == "mse") {
fforma_fit[[i]]$validation_error_mse
} else if (loff_function == "mae") {
fforma_fit[[i]]$validation_error_mae
}
}
}
fforma_labels <- function(errors) {
foreach(i = 1:nrow(errors), .combine = "rbind") %dopar% {
data.frame(label = which.min(errors[i, ]) - 1)
}
}
fforma_data_list <- function(pooled_panel, loss_function) {
list(data = fforma_data_matrix(pooled_panel),
errors = fforma_errors(set_1, loss_function),
labels = fforma_labels(errors))
}
train_list <- pooled_panel %>%
filter(time %in% timeSlices[[1]]$train) %>%
fforma_data_list(loss_function = "mse")
train_list <- pooled_panel %>%
filter(time %in% timeSlices[[1]]$train) %>%
fforma_data_list(loss_function = "mse")
validation_list <- pooled_panel %>%
filter(time %in% timeSlices[[1]]$validation) %>%
fforma_data_list(loss_function = "mse")
test_list <- pooled_panel %>%
filter(time %in% timeSlices[[1]]$test) %>%
fforma_data_list(loss_function = "mse")
param <- list(booster = "gbtree",
max_depth = 6, eta = 0.3, nthread = 12,
num_class = ncol(errors),
objective = error_softmax_obj,
subsample = 0.5,
colsample_bytree = 0.1)
dtrain <- xgb.DMatrix(data = as.matrix(train_list$data),
label = train_list$labels[, 1])
attr(dtrain, "errors") <- train_list$errors
bst <- xgboost::xgb.train(params = param, data = dtrain, nrounds = 200, verbose = 1)
oU
0U
1L
