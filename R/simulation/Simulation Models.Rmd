---
title: "Simulation Models"
author: "Ze Yu Zhong"
date: "19 June 2019"
output: html_document
---

```{r setup, include=FALSE}
################
##Load Libraries
################

library(speedglm)
library(tidyverse)
library(keras)
library(ggplot2)
library(forecast)
library(rlist)
library(Metrics)
library(tensorflow)
library(quantreg)
library(randomForestSRC)
library(hqreg)
library(caret)
library(tsfeatures)

## Vector arima/garch packages

library(xts)
library(uwot)
library(rugarch)

## SPecial xgboost instructions

# CLone my forked version of xgboost with multiclass fix
# Set working directoty the r pakcage folder of that repo
# setwd('C:/Users/Zeyu Zhong/Documents/GitHub/xgboost/R-package')
# install.packages('.', repos = NULL, type="source")

# Other alternative


library(xgboost)

#Parallel Computing
library(foreach)
library(doFuture)
#Registering
```

```{r train_valid_test}
#Create Training + Test Sets

# Gu et al set a training, validation and test sample equal in length for their simulations. Not the most sensible idea

#$T = 180$ monthly periods corresponds to 15 years. The training sample was set to start from $T = 108$ or 9 years, a validation set 1 year in length. The last 3 years were reserved as a test set never to be used for validation or training.

#This means that there are only 3 sample periods to train on, yay

#Caret and other packages are not so good at dealing with custom training/validation/test sets. Have to specify resampling, tuning and testing manually (ugh)

# Change this to 9 + 3 + 3, maybe more stable procedure

customTimeSlices <- function(start, initialWindow, horizon, validation_size, test_size, set_no) {
  
  time_slice <- list(train = 0, validation = 0, test = 0)
  time_slices <- rep(list(time_slice), set_no)
  
  for (t in 1:set_no) {
    time_slice$train <- c(start:(initialWindow + (t-1) * horizon + 1))
    time_slice$validation <- c((initialWindow + (t-1) * horizon + 2):((initialWindow + (t-1) * horizon) + validation_size + 1))
    time_slice$test <- c((initialWindow + (t-1) * horizon) + validation_size + 2):((initialWindow + (t-1) * horizon) + validation_size + test_size + 1)
    time_slices[[t]] <- time_slice
  }
  time_slices
}

#Create custom time slices
timeSlices <- customTimeSlices(start = 2, initialWindow = 108, horizon = 12, validation_size = 36, test_size = 12, set_no = 3)

#Formula Function, makes it easier for those packages with a formula interface

panel_formula <- function(panel){
  #Remove the first 3 colNames, as these correspond to the return, time and stock id
  panel_colnames <- colnames(panel)[-c(1:3)]
  f <- as.formula(c("rt ~", paste(panel_colnames, collapse = "+")))
  return(f)
}
```

```{r, eval = FALSE}
g1_A1_nosv_0 <- readRDS("~/GitHub/Evaluation-of-Machine-Learning-in-Asset-Pricing/R/Simulated_dataset/g1_A1_nosv_0.RDS")

pooled_panel <- g1_A1_nosv_0[[1]]$panel

g1_A1_sv_0.1 <- readRDS("~/GitHub/Evaluation-of-Machine-Learning-in-Asset-Pricing/R/Simulated_dataset/g1_A1_sv_0.1.RDS")

pooled_panel <- g1_A1_sv_0.1[[1]]$panel

f <- panel_formula(pooled_panel)
```

```{r variable_importance_functions_foreach}
# Foreach parallel implementation of previous section
# There are a LOT of predictors to inerate over and often this is the bottleneck for the model fitting procedure
# Therefore, use these parallel implementations if possible
# Also, the code is noticeably neater

# Variable Importance Metric Functions
# Given a single model and test set, return a variable importance dataframe (don't bother sorting them now)
# Note that a training set is NOT needed because this is to be called AFTER the model is fit
# These functions were originally written after the actul fitting functions were
# These are used by the fitting functions, so they need to be declared before them

# Similarly, different functions are needed for all model types because all the model objects have slightly different predict methods

################################################
# Linear Model
LM_variable_importance <- function(test, lm_model) {
  test_x <- test[4:ncol(test)]
  
  # Specify .packages = "quantreg" here as it seems it isn't supported and therefore is missed by doFuture
  variable_importance_df <- foreach(i = (1:ncol(test_x)), .combine = "rbind", .packages = c("speedglm", "quantreg")) %dopar% {
    test_x_zero <- test_x
    test_x_zero[, i] <- 0
    
    original_R2 <- R2(predict(lm_model, newdata = test_x), test$rt, form = "traditional")
    
    new_R2 <- R2(predict(lm_model, newdata = test_x_zero), test$rt, form = "traditional")
    
    variable_importance <- data.frame(variable = colnames(test_x)[i], importance = (original_R2 - new_R2))
    
    variable_importance
  }
  variable_importance_df
}

# Penalized Linear Model

ELN_variable_importance <- function(test, eln_model, alpha, lambda) {
  test_x <- as.matrix(test[4:ncol(test)])
  
  variable_importance_df <- foreach(i = (1:ncol(test_x)), .combine = "rbind") %dopar% {
    test_x_zero <- test_x
    test_x_zero[, i] <- 0
    
    original_R2 <- R2(predict(eln_model, test_x, alpha = alpha, lambda = lambda), test$rt, form = "traditional")
    
    new_R2 <- R2(predict(eln_model, test_x_zero, alpha = alpha, lambda = lambda), test$rt, form = "traditional")
    
    variable_importance <- data.frame(variable = colnames(test_x)[i], importance = (original_R2 - new_R2))
    
    variable_importance
  }
  variable_importance_df
}

# Random Forest

RF_variable_importance <- function(test, rf_model) {
  test_x <- test[4:ncol(test)]
  
  variable_importance_df <- foreach(i = (1:ncol(test_x)), .combine = "rbind", .packages = "randomForestSRC") %do% {
    test_x_zero <- test_x
    test_x_zero[, i] <- 0
    
    original_R2 <- R2(predict(rf_model, newdata = test_x)$predicted, test$rt, form = "traditional")
    
    new_R2 <- R2(predict(rf_model, newdata = test_x_zero)$predicted, test$rt, form = "traditional")
    
    variable_importance <- data.frame(variable = colnames(test_x)[i], importance = (original_R2 - new_R2))
                                      
    variable_importance
  }
variable_importance_df
}

# Neural Network
# Unfortunately it seems that the tensorflow/keras backend does not work with foreach, so a standard for loop is needed instead

NNet_variable_importance <- function(test, nnet_model) {

  test_x <- test[4:ncol(test)]

  variable_importance_df <- foreach(i = (1:ncol(test_x)), .combine = "rbind", .packages = c("keras", "tensorflow", "reticulate")) %do% {
    test_x_zero <- test_x
    test_x_zero[, i] <- 0

    original_R2 <- R2(predict(nnet_model, as.matrix(test_x)), test$rt, form = "traditional")

    new_R2 <- R2(predict(nnet_model, as.matrix(test_x_zero)), test$rt, form = "traditional")

    variable_importance <- data.frame(variable = colnames(test_x)[i], importance = (original_R2 - new_R2))

    variable_importance
  }
variable_importance_df
}
```

```{r cross_sectional_average_residuals}
## Function to calculate cross sectional average residuals, used for Diebold Mariano Tests later
## Don't really believe in this, mostly done because its popular in literature
# Similarly, different functions are needed for each type of model object as they all have slightly different predict methods

## Linear Model
# Given a an lm object and a test dataset, return the cross sectional average forecast errors

lm_ave_forecast_resids <- function(lm_model, test) {
  
  time_periods <- unique(test$time)
  
  # Set .combine to "c" to return a vector of results, which is what we want
  ave_forecast_resids_vector <- foreach(t = 1:length(time_periods), .combine = "c", .packages = c("speedglm", "quantreg")) %do% {
    
    test_cross_section <- test %>%
      filter(time == time_periods[t])
    
    residuals <- test_cross_section$rt - predict(lm_model, newdata = test_cross_section)
    
    mean(residuals)
  }
  ave_forecast_resids_vector
}

## Elastic Net

eln_ave_forecast_resids <- function(eln_model, test, alpha, lambda) {
  
  time_periods <- unique(test$time)
  
  # Set .combine to "c" to return a vector of results, which is what we want
  ave_forecast_resids_vector <- foreach(t = 1:length(time_periods), .combine = "c") %do% {
    
    test_cross_section <- test %>%
      filter(time == time_periods[t])

    test_cross_section_x <- as.matrix(test_cross_section[4:ncol(test_cross_section)])
    
    residuals <- test_cross_section$rt - predict(eln_model, test_cross_section_x,
                                                 alpha = alpha, lambda = lambda)
    
    mean(residuals)
  }
  ave_forecast_resids_vector
}

## Random Forest

rf_ave_forecast_resids <- function(rf_model, test) {
  
  time_periods <- unique(test$time)
  
  # Set .combine to "c" to return a vector of results, which is what we want
  ave_forecast_resids_vector <- foreach(t = 1:length(time_periods), .combine = "c") %do% {
    
    test_cross_section <- test %>%
      filter(time == time_periods[t])
    
    residuals <- test_cross_section$rt - predict(rf_model, newdata = test_cross_section)$predicted
    
    mean(residuals)
  }
  ave_forecast_resids_vector
}

## Neural Networks

nnet_ave_forecast_resids <- function(nnet_model, test) {
  
  time_periods <- unique(test$time)
  
  # Set .combine to "c" to return a vector of results, which is what we want
  ave_forecast_resids_vector <- foreach(t = 1:length(time_periods), .combine = "c") %do% {
    
    test_cross_section <- test %>%
      filter(time == time_periods[t])
    
    test_cross_section_x <- test_cross_section[4:ncol(test_cross_section)]
    
    residuals <- test_cross_section$rt - (nnet_model %>% predict(as.matrix(test_cross_section_x)))
    
    mean(residuals)
  }
  ave_forecast_resids_vector
}

```

```{r pooled_ols}
LM_fit <- function(pooled_panel, timeSlices, loss_function, f) {
  #Initialize Loss Function Statistics
  LM_stats <- rep(list(0), 3)
  
  for (set in 1:3) {
    LM_stats[[set]] <- list(loss_stats = data.frame(train_MAE = 0, train_MSE = 0, train_RMSE = 0, train_RSquare = 0, 
                                                    validation_MAE = 0, validation_MSE = 0, validation_RMSE = 0, validation_RSquare = 0, 
                                                    test_MAE = 0, test_MSE = 0, test_RMSE = 0, test_RSquare = 0),
                            #Other useful things
                            forecast_resids = 0,
                            model = 0,
                            #Variable Importance
                            variable_importance = 0)
    
    #Load Training, validation and test sets
    train <- pooled_panel %>%
      filter(time %in% timeSlices[[set]]$train)
    validation <- pooled_panel %>%
      filter(time %in% timeSlices[[set]]$validation)
    test <- pooled_panel %>%
      filter(time %in% timeSlices[[set]]$test)
    
    #Train Model on training set
    
    # Set model = FALSE so that it won't save a copy of the training data in the model object
    # Doen for memory efficiency
    #MSE case
    if (loss_function == "mse") {
      lm <- lm(f, data = train, model = FALSE, y = FALSE)
    } else {
      # Use pfn as method here for much faster computation
      lm <- rq(f, data = train, tau = 0.5, method = "pfn")
    }
    
    #LM_stats[[set]]$model <- lm
    
    #No Tuning Needed
    
    #Statistics
    #Training Set
    train_predict <- predict(lm, newdata = train)
    LM_stats[[set]]$loss_stats$train_MAE <- mae(train$rt, train_predict)
    LM_stats[[set]]$loss_stats$train_MSE <- mse(train$rt, train_predict)
    LM_stats[[set]]$loss_stats$train_RMSE <- rmse(train$rt, train_predict)
    LM_stats[[set]]$loss_stats$train_RSquare <- R2(train_predict, train$rt, form = "traditional")
    
    #Validation Set Statistics
    validation_predict <- predict(lm, newdata = validation)
    LM_stats[[set]]$loss_stats$validation_MAE <- mae(validation$rt, validation_predict)
    LM_stats[[set]]$loss_stats$validation_MSE <- mse(validation$rt, validation_predict)
    LM_stats[[set]]$loss_stats$validation_RMSE <- rmse(validation$rt, validation_predict)
    LM_stats[[set]]$loss_stats$validation_RSquare <- R2(validation_predict, validation$rt, form = "traditional")
      
    #Test Set Statistics
    test_predict <- predict(lm, newdata = test)
    LM_stats[[set]]$loss_stats$test_MAE <- mae(test$rt, test_predict)
    LM_stats[[set]]$loss_stats$test_MSE <- mse(test$rt, test_predict)
    LM_stats[[set]]$loss_stats$test_RMSE <- rmse(test$rt, test_predict)
    LM_stats[[set]]$loss_stats$test_RSquare <- R2(test_predict, test$rt, form = "traditional")

    #Forecast Residuals
    LM_stats[[set]]$forecast_resids <- lm_ave_forecast_resids(lm_model = lm, test = test)
    #Variable Importance
    LM_stats[[set]]$variable_importance <- LM_variable_importance(test, lm)
  }
  return(LM_stats)
}
```

```{r, eval = FALSE}

LM_stats_mse <- LM_fit(pooled_panel, timeSlices, "mse", f)
# 
LM_stats_mae <- LM_fit(pooled_panel, timeSlices, "mae", f)
# 
LM_stats_mse[[1]]$loss_stats
LM_stats_mse[[2]]$loss_stats
LM_stats_mse[[3]]$loss_stats
# 
LM_stats_mse[[1]]$variable_importance %>% arrange(desc(importance))
LM_stats_mse[[2]]$variable_importance %>% arrange(desc(importance))
LM_stats_mse[[3]]$variable_importance %>% arrange(desc(importance))
# 
summary(LM_stats_mse[[1]]$model)
# 
LM_stats_mae[[1]]$loss_stats
LM_stats_mae[[2]]$loss_stats
LM_stats_mae[[3]]$loss_stats

summary(LM_stats_mae[[1]]$model)
# 
LM_stats_mae[[1]]$variable_importance %>% arrange(desc(importance))
LM_stats_mae[[2]]$variable_importance %>% arrange(desc(importance))
LM_stats_mae[[3]]$variable_importance %>% arrange(desc(importance))
# 

LM_stats_mse[[3]]$forecast_resids
LM_stats_mae[[3]]$forecast_resids

```

```{r elastic_net, eval = FALSE}
## DO NOT RUN THIS CHUNK. ITS IMPLEMENTATION IS VERY INTENSIVE AND SLOW. NOTE EVAL = FALSE FLAG
# KEPT HERE MAINLY FOR LEGACY PURPOSES

#Elasticnet WRT MSE

# Can't seem to find anything online that tunes ELN for you with specified validation sets, instead of straight-forward cross validation
# nfold and foldid require that ELN be performed with at least 3 fold validation - not possible in our context

# Elasticnet only has alpha and lambda to tune

# ELN is rather finicky and likes to generate its own sequence of lambda values during the actual fitting process. Really doesn't like specifying a singular lambda value
# Solution: follow what the documentation says and let it find its own lambda sequence
# Then, when supply custom grid values of lambda when actually predicting values. This can be either through exact = TRUE (refits the model with the additional lambda gridpoint), or exact = FALSE, which just uses linear interpolation
# According to documentation, linear interpolation is very fast and is generally "good enough" anyway

# Just keep the grid small for now to make sure everything is running
ELN_grid <- expand.grid(
  alpha = seq(0, 1, 0.01),
  lambda = 10^seq(-2, 2, length.out = 50)
)

######################################
## Functions to tune an ELN model
######################################

# Fit the model over the defined grid of hyperparameters, and return it as a list
# Takes a defined grid of hyperparameters, a training set, and a validation set, fits the model over the grid, and returns the models + loss statistics as a list
# Works with both loss functions of MSE and MAE, need to specify these as "mse" or "mae" respectively

# Ideally, we would store all the model fitted over the grid. This takes up way too much memory, and because ELN is very efficiently fit anyway it's better not to store them. Relevant lines are commented out

ELN_model_grid <- function(ELN_grid, train_x, train_y, validation_x, validation_y, loss_function, nlambda) {
  ELN_model_grid <- rep(list(0), nrow(ELN_grid))
  
  for (i in 1:nrow(ELN_grid)) {
    
    ELN_model_grid[[i]] <- list(ELN_grid = 0, model = 0)
    
    # MSE Case
    if (loss_function == "mse") {
      ELN <- hqreg(train_x, train_y, method = "ls", alpha = ELN_grid$alpha[i], nlambda = nlambda)
      
      #ELN_model_grid[[i]]$model <- ELN
      
      ELN_model_grid[[i]]$ELN_grid <- cbind(alpha = ELN_grid$alpha[i], lambda = ELN_grid$lambda[i],
                                            train_loss = mse(train_y, predict(ELN, train_x, lambda = ELN_grid$lambda[i])),
                                            validation_loss = mse(validation_y, predict(ELN, validation_x, lambda = ELN_grid$lambda[i]))
                                            )
    } else {
      # MAE Case
      ELN <- hqreg(train_x, train_y, method = "quantile", tau = 0.5, alpha = ELN_grid$alpha[i], nlambda = nlambda)
      #ELN_model_grid[[i]]$model <- ELN

      ELN_model_grid[[i]]$ELN_grid <- cbind(alpha = ELN_grid$alpha[i], lambda = ELN_grid$lambda[i],
                                            train_loss = mae(train_y, predict(ELN, train_x, lambda = ELN_grid$lambda[i])),
                                            validation_loss = mae(validation_y, predict(ELN, validation_x, lambda = ELN_grid$lambda[i]))
                                            )
    }
  }
  return(ELN_model_grid)
}

get_ELN_best_tune <- function(model_grid) {
  ELN_tune_dataframe <- cbind(model_grid[[1]]$ELN_grid, list_index = 1)
  for (i in 2:length(model_grid)) {
    ELN_tune_dataframe <- rbind(ELN_tune_dataframe, cbind(model_grid[[i]]$ELN_grid, list_index = i))
  }
  ELN_tune_dataframe <- data.frame(ELN_tune_dataframe)
  return(ELN_tune_dataframe[which.min(ELN_tune_dataframe$validation_loss), ])
}

# Final big wrapper function that gives an ELN_stats list with everything desired

ELN_fit_stats <- function(ELN_grid, nlambda, timeSlices, pooled_panel, loss_function) {
  ELN_stats <- rep(list(0), 3)
  
  for (set in 1:3) {
    #Load Training, validation and test sets
    train <- pooled_panel %>%
      filter(time %in% timeSlices[[set]]$train)
    validation <- pooled_panel %>%
      filter(time %in% timeSlices[[set]]$validation)
    test <- pooled_panel %>%
      filter(time %in% timeSlices[[set]]$test)
    
    train_x <- as.matrix(train[4:ncol(train)])
    train_y <- as.matrix(train$rt)
    validation_x <- as.matrix(validation[4:ncol(validation)])
    validation_y <- as.matrix(validation$rt)
    test_x <- as.matrix(test[4:ncol(test)])
    test_y <- as.matrix(test$rt)
      
    #Get models fit over the grid of hyperparameters
    ELN_model_grid <- ELN_model_grid(ELN_grid, train_x, train_y, validation_x, validation_y, loss_function, nlambda = nlambda)
        
    #Get the best tuning parameters
    best_model_params <- get_ELN_best_tune(ELN_model_grid)
    
    #Initialize stats list
    ELN_stats[[set]] <- list(loss_stats = data.frame(train_MAE = 0, train_MSE = 0, train_RMSE = 0, train_RSquare = 0, 
                                                     validation_MAE = 0, validation_MSE = 0, validation_RMSE = 0, validation_RSquare = 0, 
                                                     test_MAE = 0, test_MSE = 0, test_RMSE = 0, test_RSquare = 0),
                             #Other useful things
                             forecasts = 0,
                             forecast_resids = 0,
                             model = 0)
      
    #Model
    if (loss_function == "mse") {
      model <- hqreg(train_x, train_y, method = "ls", alpha = best_model_params$alpha, nlambda = nlambda)
    } else {
      model <- hqreg(train_x, train_y, method = "quantile", tau = 0.5, alpha = best_model_params$alpha, nlambda = nlambda)
    }
    
    #ELN_stats[[set]]$model <- model
      
    #Loss Stats Dataframe
    #Train
    train_predict <- predict(model, train_x, alpha = best_model_params$alpha, lambda = best_model_params$lambda)
    ELN_stats[[set]]$loss_stats$train_MAE <- mae(train_y, train_predict)
    ELN_stats[[set]]$loss_stats$train_MSE <- mse(train_y, train_predict)
    ELN_stats[[set]]$loss_stats$train_RMSE <- rmse(train_y, train_predict)
    ELN_stats[[set]]$loss_stats$train_RSquare <- (1 - sse(train_y, train_predict) / sum((train_y - mean(train_y))^2))
        
    #Validation
    valid_predict <- predict(model, validation_x, alpha = best_model_params$alpha, lambda = best_model_params$lambda)
    ELN_stats[[set]]$loss_stats$validation_MAE <- mae(validation_y, valid_predict)
    ELN_stats[[set]]$loss_stats$validation_MSE <- mse(validation_y, valid_predict)
    ELN_stats[[set]]$loss_stats$validation_RMSE <- rmse(validation_y, valid_predict)
    ELN_stats[[set]]$loss_stats$validation_RSquare <- (1 - sse(validation_y, valid_predict) / sum((validation_y - mean(validation_y))^2))
    
    #Test
    test_predict <- predict(model, test_x, alpha = best_model_params$alpha, lambda = best_model_params$lambda)
    ELN_stats[[set]]$loss_stats$test_MAE <- mae(test_y, test_predict)
    ELN_stats[[set]]$loss_stats$test_MSE <- mse(test_y, test_predict)
    ELN_stats[[set]]$loss_stats$test_RMSE <- rmse(test_y, test_predict)
    ELN_stats[[set]]$loss_stats$test_RSquare <- (1 - sse(test_y, test_predict) / sum((test_y - mean(test_y))^2))
      
    #Forecasts
    ELN_stats[[set]]$forecasts <- test_predict
    
    #Forecast residuals
    ELN_stats[[set]]$forecast_resids <- test_y - test_predict
  }
  return(ELN_stats)
}
```

```{r elastic_net}
# Alternative implementation for ELN
# This is much, much faster
# DO NOT USE THE PREVIOUS IMPLEMENTATION
# Do not specify a grid for lambda. Instead, let the algorithm determine its own sequence of lambda
# Method: specify only a grid for alpha. For each grid value of alpha, fit an ELN model and let LARS determine a suitable path of lambdas. Produce validation error statistics for each alpha and lambda value. Use the best combination for the final model
# 

alpha_grid <- seq(0, 1, 0.01)

# Foreach implementation of above function
# This is much much faster

ELN_model_grid <- function(alpha_grid, train_x, train_y, validation_x, validation_y, loss_function, nlamb) {
  
  ELN_model_grid_list <- foreach(i = (1:length(alpha_grid))) %dopar% {
    ELN_model_grid <- list(ELN_grid = 0, model = 0)
    
    #MSE Case
    if (loss_function == "mse") {
      ELN <- hqreg(train_x, train_y, method = "ls", alpha = alpha_grid[i], nlambda = nlamb)
      ELN_model_grid$model <- ELN
    
      ELN_model_grid$ELN_grid <- cbind(alpha = alpha_grid[i], lambda = ELN$lambda,
                                       train_loss = apply(predict(ELN, train_x), 2, mse, actual = train_y), 
                                       validation_loss = apply(predict(ELN, validation_x), 2, mse, actual = validation_y))
    } else {
      #MAE Case
      ELN <- hqreg(train_x, train_y, method = "quantile", tau = 0.5, alpha = alpha_grid[i], nlambda = nlamb)
      ELN_model_grid$model <- ELN
    
      ELN_model_grid$ELN_grid <- cbind(alpha = alpha_grid[i], lambda = ELN$lambda,
                                       train_loss = apply(predict(ELN, train_x), 2, mse, actual = train_y), 
                                       validation_loss = apply(predict(ELN, validation_x), 2, mae, actual = validation_y))
    }
    ELN_model_grid
  }
  ELN_model_grid_list
}

# Given a list containing the model grid, return the "best" model according to validation loss
# Use in conjunction with previous function

get_ELN_best_tune <- function(model_grid) {
  ELN_tune_dataframe <- cbind(model_grid[[1]]$ELN_grid, list_index = 1)
  for (i in 2:length(model_grid)) {
    ELN_tune_dataframe <- rbind(ELN_tune_dataframe, cbind(model_grid[[i]]$ELN_grid, list_index = i))
  }
  ELN_tune_dataframe <- data.frame(ELN_tune_dataframe)
  return(ELN_tune_dataframe[which.min(ELN_tune_dataframe$validation_loss), ])
}

# Final big wrapper function that gives an ELN_stats list with everything desired

ELN_fit_stats <- function(alpha_grid, nlamb, timeSlices, pooled_panel, loss_function) {
  ELN_stats <- rep(list(0), 3)
  
  for (set in 1:3) {
    #Load Training, validation and test sets
    train <- pooled_panel %>%
      filter(time %in% timeSlices[[set]]$train)
    validation <- pooled_panel %>%
      filter(time %in% timeSlices[[set]]$validation)
    test <- pooled_panel %>%
      filter(time %in% timeSlices[[set]]$test)
    
    train_x <- as.matrix(train[4:ncol(train)])
    train_y <- as.matrix(train$rt)
    validation_x <- as.matrix(validation[4:ncol(validation)])
    validation_y <- as.matrix(validation$rt)
    test_x <- as.matrix(test[4:ncol(test)])
    test_y <- as.matrix(test$rt)
      
    #Get models fit over the grid of hyperparameters
    ELN_model_grid <- ELN_model_grid(alpha_grid, train_x, train_y, validation_x, validation_y, loss_function, nlamb = nlamb)
        
    #Get the best tuning parameters
    best_model_params <- get_ELN_best_tune(ELN_model_grid)
    
    #Initialize stats list
    ELN_stats[[set]] <- list(loss_stats = data.frame(train_MAE = 0, train_MSE = 0, train_RMSE = 0, train_RSquare = 0, 
                                                     validation_MAE = 0, validation_MSE = 0, validation_RMSE = 0, validation_RSquare = 0, 
                                                     test_MAE = 0, test_MSE = 0, test_RMSE = 0, test_RSquare = 0),
                             #Other useful things
                             forecasts = 0,
                             forecast_resids = 0,
                             model = 0,
                             hyperparameters = 0,
                             variable_importance = 0)
      
    #Model
    model <- ELN_model_grid[[best_model_params$list_index]]$model
    
    #ELN_stats[[set]]$model <- model
    
    #Hyperparameters
    ELN_stats[[set]]$hyperparameters <- best_model_params
      
    #Loss Stats Dataframe
    #Train
    train_predict <- predict(model, train_x, alpha = best_model_params$alpha, lambda = best_model_params$lambda)
    ELN_stats[[set]]$loss_stats$train_MAE <- mae(train_y, train_predict)
    ELN_stats[[set]]$loss_stats$train_MSE <- mse(train_y, train_predict)
    ELN_stats[[set]]$loss_stats$train_RMSE <- rmse(train_y, train_predict)
    ELN_stats[[set]]$loss_stats$train_RSquare <- (1 - sse(train_y, train_predict) / sum((train_y - mean(train_y))^2))
        
    #Validation
    valid_predict <- predict(model, validation_x, alpha = best_model_params$alpha, lambda = best_model_params$lambda)
    ELN_stats[[set]]$loss_stats$validation_MAE <- mae(validation_y, valid_predict)
    ELN_stats[[set]]$loss_stats$validation_MSE <- mse(validation_y, valid_predict)
    ELN_stats[[set]]$loss_stats$validation_RMSE <- rmse(validation_y, valid_predict)
    ELN_stats[[set]]$loss_stats$validation_RSquare <- (1 - sse(validation_y, valid_predict) / sum((validation_y - mean(validation_y))^2))
    
    #Test
    test_predict <- predict(model, test_x, alpha = best_model_params$alpha, lambda = best_model_params$lambda)
    ELN_stats[[set]]$loss_stats$test_MAE <- mae(test_y, test_predict)
    ELN_stats[[set]]$loss_stats$test_MSE <- mse(test_y, test_predict)
    ELN_stats[[set]]$loss_stats$test_RMSE <- rmse(test_y, test_predict)
    ELN_stats[[set]]$loss_stats$test_RSquare <- (1 - sse(test_y, test_predict) / sum((test_y - mean(test_y))^2))
    
    #Forecasts
    ELN_stats[[set]]$forecasts <- test_predict
    
    #Forecast residuals
    ELN_stats[[set]]$forecast_resids <- eln_ave_forecast_resids(eln_model = model, test, 
                                                                # Hyperparameters
                                                                alpha = best_model_params$alpha, 
                                                                lambda = best_model_params$lambda)
    
    #Variable Importance
    ELN_stats[[set]]$variable_importance <- ELN_variable_importance(test, model, alpha = best_model_params$alpha, lambda = best_model_params$lambda)
  }
  return(ELN_stats)
}
```

```{r, eval = FALSE}

#Testing if function works

ELN_stats_mae <- ELN_fit_stats(alpha_grid, nlamb = 100, timeSlices, pooled_panel, loss_function = "mae")

ELN_stats_mse <- ELN_fit_stats(alpha_grid, nlamb = 100, timeSlices, pooled_panel, loss_function = "mse")

ELN_stats_mse[[1]]$loss_stats
ELN_stats_mse[[1]]$model$alpha

ELN_stats_mse[[2]]$loss_stats
ELN_stats_mse[[2]]$model$alpha

ELN_stats_mse[[3]]$loss_stats
ELN_stats_mse[[3]]$model$alpha

ELN_stats_mse[[1]]$variable_importance %>% arrange(desc(importance))
ELN_stats_mse[[2]]$variable_importance %>% arrange(desc(importance))
ELN_stats_mse[[3]]$variable_importance %>% arrange(desc(importance))

ELN_stats_mae[[1]]$loss_stats
ELN_stats_mae[[1]]$model$alpha

ELN_stats_mae[[2]]$loss_stats
ELN_stats_mae[[2]]$model$alpha

ELN_stats_mae[[3]]$loss_stats
ELN_stats_mae[[3]]$model$alpha

ELN_stats_mae[[1]]$variable_importance %>% arrange(desc(importance))
ELN_stats_mae[[2]]$variable_importance %>% arrange(desc(importance))
ELN_stats_mae[[3]]$variable_importance %>% arrange(desc(importance))

# Notes
# Mostly consistent results with the previous implementation, very minor differences in optimal alpha values chosen
# Similarly, optimal alpha values are highly unstable
# Does not seem to particularly different between MAE and MSE versions

# Moving from A1 to A2
# LM struggles greatly with more complex A matrix specifications
# However, ELN actually does quite well in spite of this
# It actuallyhas higher R squared values than the true r squared, presumbaly indicating that it is somehow capturing the SV error structure
# In general, it seems that ELN is choosing very parsimonious representations (look at variable importance metrics that are at 0)
```

```{r random_forest}
#Random Forest

#Random Forest should also be fairly straightforward, only two tuning parameters mtry and ntree (usually)

#Check out randomForestSRC for an implementation of randomForests that allows for quantile regression trees. Quantile regression use a check loss function, which is a superset of absolute error
#forestr package doesn't seem to want to function on honours lab computers

#Default ntree = 500
#Default mtry = count(predictors/3) = 400/3 = 133

# Fit an RF model over the entire grid of hyperparameters
# Ideally, we would save all the models as we computed them
# This is very memory intensive, hence they will NOT be saved
# Downside is that the optimal model will have to be recomputed from the optimal hyperparameters

# Special Note: variable importance using a standard for loop (not parallelized) here is very computationally intensive
# ALWAYS make sure that the parallelized version fo RF variable importance is used

## Extra feature: calculate the variable imporance using the built in vimp method for rfsrc objects
## This is primarily done just to contrast the traditional or normal way of doing VI with our new measure that is consistent across all models
## There are two vimps available: Breiman Cutler (permutation) and Ishwaran-Kogalur vimp
## Just do them both, as they don't seem to be too intensive
## A newer vimp called holdout vimp involves truly holding out certain variables from the entire forest fitting process and seeing hwat effects this has - this is WAY too computationally intensive (they recommend ntree = 1000 * vtry, and a default of ntree = 1000)

## Bug with randomForestSRC - default vimp function is IN SAMPLE vimp which is very memory intensive
## If you don't have enough memory, R will just crash and this is the major bug
## Make sure you specify a test set so that it will do out of sample vimp, which is feasible in terms of memory limits and will prevent crashing

RF_fit_model_grid <- function(f, train, validation, RF_grid, loss_function) {
  #Initialize List
  RF_model_grid <- rep(list(0), nrow(RF_grid))
  
  for (i in 1:nrow(RF_grid)) {
    
    RF_model_grid[[i]] <- list(RF_grid = 0, model = 0)
    
    #MSE Case
    if (loss_function == "mse") {
      RF <- rfsrc(f, train, 
                  #Hyperparameters
                  ntree = RF_grid$ntree[i],
                  mtry = RF_grid$mtry[i],
                  nodesize = RF_grid$nodesize[i],
                  splitrule = "mse"
                  )
      
      #RF_model_grid[[i]]$model <- RF
      RF_model_grid[[i]]$RF_grid <- cbind(RF_grid[i, ],
                                          #Train Loss
                                          train_loss = mse(train$rt, predict(RF)$predicted),
                                          #Validation Loss
                                          validation_loss = mse(validation$rt, predict(RF, newdata = validation)$predicted)
                                          )
      
    } else {
      #MAE Case
      RF <- rfsrc(f, train, 
                  #Hyperparameters
                  ntree = RF_grid$ntree[i],
                  mtry = RF_grid$mtry[i],
                  nodesize = RF_grid$nodesize[i],
                  splitrule = "quantile.regr",
                  prob = 0.5
                  )
      
      #RF_model_grid[[i]]$model <- RF
      RF_model_grid[[i]]$RF_grid <- cbind(RF_grid[i, ],
                                          #Train Loss
                                          train_loss = mae(train$rt, predict(RF)$predicted),
                                          #Validation Loss
                                          validation_loss = mae(validation$rt, predict(RF, newdata = validation)$predicted)
                                          )
    }
  }
  return(RF_model_grid)
}

#Returns the dataframe row containing the "best" hyperparameters

get_RF_best_tune <- function(RF_model_grid) {
  RF_tune_grid <- RF_model_grid[[1]]$RF_grid
  for (i in 2:length(RF_model_grid)) {
    RF_tune_grid <- rbind(RF_tune_grid, RF_model_grid[[i]]$RF_grid)
  }
  return(RF_tune_grid[which.min(RF_tune_grid$validation_loss), ])
}

RF_fit_stats <- function(pooled_panel, RF_grid, timeSlices, loss_function, f) {
  #Initialize
  RF_stats <- rep(list(0), 3)
  
  #Load training, validation and test sets
  
  for (set in 1:3) {
    
    RF_stats[[set]] <- list(loss_stats = data.frame(train_MAE = 0, train_MSE = 0, train_RMSE = 0, train_RSquare = 0, 
                                                    validation_MAE = 0, validation_MSE = 0, validation_RMSE = 0, validation_RSquare = 0, 
                                                    test_MAE = 0, test_MSE = 0, test_RMSE = 0, test_RSquare = 0),
                            #Other useful things
                            forecast_resids = 0,
                            model = 0,
                            hyperparameters = 0,
                            variable_importance = 0, vimp = 0)
    
    train <- pooled_panel %>%
      filter(time %in% timeSlices[[set]]$train)
    validation <- pooled_panel %>%
      filter(time %in% timeSlices[[set]]$validation)
    test <- pooled_panel %>%
      filter(time %in% timeSlices[[set]]$test)
    
    #Fit on training Set over grid of hyperparameters
    
    model_grid <- RF_fit_model_grid(f, train, validation, RF_grid, loss_function)
    
    #Get the best hyperparameters
    
    best_model_params <- get_RF_best_tune(model_grid)
    RF_stats[[set]]$hyperparameters <- best_model_params
    
    #Compute the optimal model
    
    if (loss_function == "mse") {
      model <- rfsrc(f, train, 
                     #Hyperparameters
                     ntree = best_model_params$ntree,
                     mtry = best_model_params$mtry,
                     nodesize = best_model_params$nodesize,
                     splitrule = "mse",
                     bootstrap = "by.root", samptype = "swr",
                     forest = TRUE
                     )
    } else {
      model <- rfsrc(f, train, 
                     #Hyperparameters
                     ntree = best_model_params$ntree,
                     mtry = best_model_params$mtry,
                     nodesize = best_model_params$nodesize,
                     splitrule = "quantile.regr",
                     prob = 0.5,
                     bootstrap = "by.root", samptype = "swr",
                     forest = TRUE)
      
    }
    
    #RF_stats[[set]]$model <- model
    
    #Train
    train_predict <- predict(model, train)$predicted
    RF_stats[[set]]$loss_stats$train_MAE <- mae(train$rt, train_predict)
    RF_stats[[set]]$loss_stats$train_MSE <- mse(train$rt, train_predict)
    RF_stats[[set]]$loss_stats$train_RMSE <- rmse(train$rt, train_predict)
    RF_stats[[set]]$loss_stats$train_RSquare <- (1 - sse(train$rt, train_predict) / sum((train$rt - mean(train$rt))^2))
        
    #Validation
    valid_predict <- predict(model, newdata = validation)$predicted
    RF_stats[[set]]$loss_stats$validation_MAE <- mae(validation$rt, valid_predict)
    RF_stats[[set]]$loss_stats$validation_MSE <- mse(validation$rt, valid_predict)
    RF_stats[[set]]$loss_stats$validation_RMSE <- rmse(validation$rt, valid_predict)
    RF_stats[[set]]$loss_stats$validation_RSquare <- (1 - sse(validation$rt, valid_predict) / sum((validation$rt - mean(validation$rt))^2))
    
    #Test
    test_predict <- predict(model, newdata = test)$predicted
    RF_stats[[set]]$loss_stats$test_MAE <- mae(test$rt, test_predict)
    RF_stats[[set]]$loss_stats$test_MSE <- mse(test$rt, test_predict)
    RF_stats[[set]]$loss_stats$test_RMSE <- rmse(test$rt, test_predict)
    RF_stats[[set]]$loss_stats$test_RSquare <- (1 - sse(test$rt, test_predict) / sum((test$rt - mean(test$rt))^2))
    
    #Forecast residuals
    RF_stats[[set]]$forecast_resids <- rf_ave_forecast_resids(rf_model = model, test = test)
    
    #Variable Importance
    RF_stats[[set]]$variable_importance <- RF_variable_importance(test, model)
    
    #RF Built in VIMPS
    RF_stats[[set]]$vimp <- list(breiman_cutler = vimp(model, newdata = test, block.size = 1)$importance,
                                 ishwaran_kogalur = vimp(model, newdata = test, block.size = best_model_params$ntree)$importance)
  }
  return(RF_stats)
}
```

```{r, eval = FALSE}

RF_grid <- expand.grid(
      #ntree usually isn't tuned. Just set to max of computationally feasible
      ntree = 100,
      mtry = seq(20, round(ncol(pooled_panel[4:ncol(pooled_panel)])/3), 20)
      # nodesize = seq(2, 14, 2)
      # nodedepth recommended not to be changed
      #nodedepth = 1
      )

RF_MSE <- RF_fit_stats(pooled_panel, RF_grid, timeSlices, "mse")

RF_MAE <- RF_fit_stats(pooled_panel, RF_grid, timeSlices, "mae")

RF_MSE[[1]]$loss_stats
RF_MSE[[2]]$loss_stats
RF_MSE[[3]]$loss_stats

RF_MSE[[1]]$variable_importance %>% arrange(desc(importance))

RF_MAE[[1]]$loss_stats
RF_MAE[[2]]$loss_stats
RF_MAE[[3]]$loss_stats

RF_MAE[[1]]$variable_importance %>% arrange(desc(importance))
RF_MAE[[1]]$forecast_resids

dm.test(RF_MSE[[1]]$forecast_resids, RF_MAE[[1]]$forecast_resids)
```

```{r neural_networks}
# Neural Networks

set.seed(27935248)

#Build neural networks with neuron numbers according to geometric pyramid rule, and ReLU activation function for all layers
# IE Input layer > 32 neurons > 16 neurons > 8 neurons > 4 neurons > 2 neurons > output

##Neural Network generalized

NNet_fit_stats <- function(pooled_panel, timeSlices, hidden_layers, loss_function, batch_size, patience) {
  #Initialize
  
  NNet_stats <- rep(list(0), 3)
  
  for (set in 1:3) {
    NNet_stats[[set]] <- list(loss_stats = data.frame(train_MAE = 0, train_MSE = 0, train_RMSE = 0, train_RSquare = 0, 
                                                      validation_MAE = 0, validation_MSE = 0, validation_RMSE = 0, validation_RSquare = 0, 
                                                      test_MAE = 0, test_MSE = 0, test_RMSE = 0, test_RSquare = 0),
                              #Other useful things
                              # Keep forecasts here in for nnet objects to make sure they aren't doing something stupid
                              forecasts = 0,
                              forecast_resids = 0,
                              model = 0,
                              variable_importance = 0)
      
    #Load Training, validation and test sets
    train <- pooled_panel %>%
      filter(time %in% timeSlices[[set]]$train)
    validation <- pooled_panel %>%
      filter(time %in% timeSlices[[set]]$validation)
    test<- pooled_panel %>%
      filter(time %in% timeSlices[[set]]$test)
      
    train_x <- train[4:ncol(train)]
    train_x <- scale(train_x)
    col_means_train <- attr(train_x, "scaled:center") 
    col_stddevs_train <- attr(train_x, "scaled:scale")
    train_y <- train$rt
    
    validation_x <- validation[4:ncol(validation)]
    validation_x <- scale(validation_x, center = col_means_train, scale = col_stddevs_train)
    validation_y <- validation$rt
      
    test_x <- test[4:ncol(test)]
    test_x <- scale(test_x, center = col_means_train, scale = col_stddevs_train)
    test_y <- test$rt
      
    # Fit the model
    # The patience parameter is the amount of epochs to check for improvement.
    # Gu et al don't say what their early stopping parameter p is
    early_stop <- callback_early_stopping(monitor = "val_loss", patience = patience, restore_best_weights = TRUE)
    
    print_dot_callback <- callback_lambda(
      on_epoch_end = function(epoch, logs) {
        if (epoch %% 50 == 0) cat("\n")
        cat(".")
      }
    ) 
    
    l1_penalty <- 0.01
    
    build_NN <- function(hidden_layers, loss_function) {
        
      if (hidden_layers == 1) {
        
        model <- keras_model_sequential() %>%
        
        # Layer 1
        layer_dense(units = 32, input_shape = ncol(train_x),
                    bias_initializer = initializer_zeros(),
                    kernel_initializer = initializer_truncated_normal(mean = 0, stddev = 0.05, seed = NULL)) %>%
          layer_activation("tanh") %>%
          layer_activity_regularization(l1 = l1_penalty) %>%
          layer_batch_normalization() %>%
        # Output Layer
          layer_dense(units = 1, activation = "linear")
        
      } else if (hidden_layers == 2) {
        
        model <- keras_model_sequential() %>%
          
        # Layer 1
        layer_dense(units = 32, input_shape = ncol(train_x),
                    bias_initializer = initializer_zeros(),
                    kernel_initializer = initializer_truncated_normal(mean = 0, stddev = 0.05, seed = NULL)) %>%
          layer_activation("tanh") %>%
          layer_activity_regularization(l1 = l1_penalty) %>%
          layer_batch_normalization() %>%
        # Layer 2
        layer_dense(units = 16,
                    bias_initializer = initializer_zeros(),
                    kernel_initializer = initializer_truncated_normal(mean = 0, stddev = 0.05, seed = NULL)) %>%
          layer_activation("tanh") %>%
          layer_activity_regularization(l1 = l1_penalty) %>%
          layer_batch_normalization() %>%
        # Output Layer
        layer_dense(units = 1, activation = "linear")
          
      } else if (hidden_layers == 3) {
          
        model <- keras_model_sequential() %>%
          
        # Layer 1
        layer_dense(units = 32, input_shape = ncol(train_x),
                    bias_initializer = initializer_zeros(),
                    kernel_initializer = initializer_truncated_normal(mean = 0, stddev = 0.05, seed = NULL)) %>%
          layer_activation("tanh") %>%
          layer_activity_regularization(l1 = l1_penalty) %>%
          layer_batch_normalization() %>%
        # Layer 2
        layer_dense(units = 16,
                    bias_initializer = initializer_zeros(),
                    kernel_initializer = initializer_truncated_normal(mean = 0, stddev = 0.05, seed = NULL)) %>%
          layer_activation("tanh") %>%
          layer_activity_regularization(l1 = l1_penalty) %>%
          layer_batch_normalization() %>%
        #Layer 3
        layer_dense(units = 8,
                    bias_initializer = initializer_zeros(),
                    kernel_initializer = initializer_truncated_normal(mean = 0, stddev = 0.05, seed = NULL)) %>%
          layer_activation("tanh") %>%
          layer_activity_regularization(l1 = l1_penalty) %>%
          layer_batch_normalization() %>%
        # Output Layer
        layer_dense(units = 1, activation = "linear")
          
      } else if (hidden_layers == 4) {
          
        model <- keras_model_sequential() %>%
        
        # Layer 1
        layer_dense(units = 32, input_shape = ncol(train_x),
                    bias_initializer = initializer_zeros(),
                    kernel_initializer = initializer_truncated_normal(mean = 0, stddev = 0.05, seed = NULL)) %>%
          layer_activation("tanh") %>%
          layer_activity_regularization(l1 = l1_penalty) %>%
          layer_batch_normalization() %>%
        # Layer 2
        layer_dense(units = 16,
                    bias_initializer = initializer_zeros(),
                    kernel_initializer = initializer_truncated_normal(mean = 0, stddev = 0.05, seed = NULL)) %>%
          layer_activation("tanh") %>%
          layer_activity_regularization(l1 = l1_penalty) %>%
          layer_batch_normalization() %>%
        #Layer 3
        layer_dense(units = 8,
                    bias_initializer = initializer_zeros(),
                    kernel_initializer = initializer_truncated_normal(mean = 0, stddev = 0.05, seed = NULL)) %>%
          layer_activation("tanh") %>%
          layer_activity_regularization(l1 = l1_penalty) %>%
          layer_batch_normalization() %>%
        #Layer 4
        layer_dense(units = 4,
                    bias_initializer = initializer_zeros(),
                    kernel_initializer = initializer_truncated_normal(mean = 0, stddev = 0.05, seed = NULL)) %>%
          layer_activation("tanh") %>%
          layer_activity_regularization(l1 = l1_penalty) %>%
          layer_batch_normalization() %>%
        # Output Layer
        layer_dense(units = 1, activation = "linear")
          
      } else {
        
        model <- keras_model_sequential() %>%
        
        # Layer 1
        layer_dense(units = 32, input_shape = ncol(train_x),
                    bias_initializer = initializer_zeros(),
                    kernel_initializer = initializer_truncated_normal(mean = 0, stddev = 0.05, seed = NULL)) %>%
          layer_activation("tanh") %>%
          layer_activity_regularization(l1 = l1_penalty) %>%
          layer_batch_normalization() %>%
        # Layer 2
        layer_dense(units = 16,
                    bias_initializer = initializer_zeros(),
                    kernel_initializer = initializer_truncated_normal(mean = 0, stddev = 0.05, seed = NULL)) %>%
          layer_activation("tanh") %>%
          layer_activity_regularization(l1 = l1_penalty) %>%
          layer_batch_normalization() %>%
        #Layer 3
        layer_dense(units = 8,
                    bias_initializer = initializer_zeros(),
                    kernel_initializer = initializer_truncated_normal(mean = 0, stddev = 0.05, seed = NULL)) %>%
          layer_activation("tanh") %>%
          layer_activity_regularization(l1 = l1_penalty) %>%
          layer_batch_normalization() %>%
        #Layer 4
        layer_dense(units = 4,
                    bias_initializer = initializer_zeros(),
                    kernel_initializer = initializer_truncated_normal(mean = 0, stddev = 0.05, seed = NULL)) %>%
          layer_activation("tanh") %>%
          layer_activity_regularization(l1 = l1_penalty) %>%
          layer_batch_normalization() %>%
        #Layer 5
        layer_dense(units = 2,
                    bias_initializer = initializer_zeros(),
                    kernel_initializer = initializer_truncated_normal(mean = 0, stddev = 0.05, seed = NULL)) %>%
          layer_activation("tanh") %>%
          layer_activity_regularization(l1 = l1_penalty) %>%
          layer_batch_normalization() %>%
        # Output Layer
        layer_dense(units = 1, activation = "linear")
          
      }
      model %>% compile(
        loss = loss_function,
        optimizer = optimizer_adam(lr = 0.001, beta_1 = 0.9, beta_2 = 0.999,
                                   epsilon = NULL, clipnorm = NULL,
                                   clipvalue = NULL),
        metrics = list("mae", "mse")
      )
      model
    }
      
    neural_network <- build_NN(hidden_layers, loss_function)
    
    # Other options used throughout the neural network fitting process are specified here
    # Namely, batch size
    # In general, larger batch sizes result in faster progress in training, but don't always converge as fast. Smaller batch sizes train slower, but can converge faster.
    # Default batch size is 32
    neural_network %>% fit(as.matrix(train_x), as.matrix(train_y), 
                           batch_size = batch_size, epochs = 500, verbose = 0, 
                           validation_data = list(as.matrix(validation_x), as.matrix(validation_y)),
                           callbacks = list(early_stop, print_dot_callback))
    
    #Model
    #NNet_stats[[set]]$model <- neural_network
    
    #Train
    train_predict <- neural_network %>% predict(as.matrix(train_x))
    
    NNet_stats[[set]]$loss_stats$train_MAE <- mae(train$rt, train_predict)
    NNet_stats[[set]]$loss_stats$train_MSE <- mse(train$rt, train_predict)
    NNet_stats[[set]]$loss_stats$train_RMSE <- rmse(train$rt, train_predict)
    NNet_stats[[set]]$loss_stats$train_RSquare <- R2(train_predict, train$rt, form = "traditional")
          
    #Validation
    validation_predict <- neural_network %>% predict(as.matrix(validation_x))
    
    NNet_stats[[set]]$loss_stats$validation_MAE <- mae(validation$rt, validation_predict)
    NNet_stats[[set]]$loss_stats$validation_MSE <- mse(validation$rt, validation_predict)
    NNet_stats[[set]]$loss_stats$validation_RMSE <- rmse(validation$rt, validation_predict)
    NNet_stats[[set]]$loss_stats$validation_RSquare <- R2(validation_predict, validation$rt, form = "traditional")
      
    #Test
    test_predict <- neural_network %>% predict(as.matrix(test_x))
    
    NNet_stats[[set]]$loss_stats$test_MAE <- mae(test$rt, test_predict)
    NNet_stats[[set]]$loss_stats$test_MSE <- mse(test$rt, test_predict)
    NNet_stats[[set]]$loss_stats$test_RMSE <- rmse(test$rt, test_predict)
    NNet_stats[[set]]$loss_stats$test_RSquare <- R2(test_predict, test$rt, form = "traditional")
        
    #Forecasts
    #NNet_stats[[set]]$forecasts <- test_predict
      
    #Forecast residuals
    NNet_stats[[set]]$forecast_resids <- nnet_ave_forecast_resids(nnet_model = neural_network, test = test)
    
    #Variable Importance
    NNet_stats[[set]]$variable_importance <- NNet_variable_importance(test, neural_network)
    
  }
  #Clear Kera Session
  k_clear_session()
  NNet_stats
}

```

```{r, eval = FALSE}

NNet_1_mse_stats <- NNet_fit_stats(pooled_panel, timeSlices, 1, "mse", batch_size = 32, 40)
NNet_1_mae_stats <- NNet_fit_stats(pooled_panel, timeSlices, 1, "mae", batch_size = 128, 40)

# # 
NNet_1_mse_stats[[1]]$loss_stats
NNet_1_mse_stats[[2]]$loss_stats
NNet_1_mse_stats[[3]]$loss_stats

NNet_1_mse_stats[[1]]$forecasts
NNet_1_mse_stats[[2]]$forecasts
NNet_1_mse_stats[[3]]$forecasts

NNet_1_mse_stats[[1]]$variable_importance %>% arrange(desc(importance))
# 
NNet_1_mae_stats[[1]]$loss_stats
NNet_1_mae_stats[[2]]$loss_stats
NNet_1_mae_stats[[3]]$loss_stats

NNet_1_mae_stats[[1]]$forecasts
NNet_1_mae_stats[[2]]$forecasts
NNet_1_mae_stats[[3]]$forecasts

NNet_1_mae_stats[[1]]$variable_importance %>% arrange(desc(importance))
# 
# ##
# 
# NNet_2_mse_stats <- NNet_fit_stats(pooled_panel, timeSlices, 2, "mse", batch_size = 512, 10)
# NNet_2_mae_stats <- NNet_fit_stats(pooled_panel, timeSlices, 2, "mae", batch_size = 512, 10)
# 
# NNet_2_mse_stats[[1]]$loss_stats
# NNet_2_mse_stats[[2]]$loss_stats
# NNet_2_mse_stats[[3]]$loss_stats
# 
# NNet_2_mae_stats[[1]]$loss_stats
# NNet_2_mae_stats[[2]]$loss_stats
# NNet_2_mae_stats[[3]]$loss_stats
# 
# ##
# 
# NNet_3_mse_stats <- NNet_fit_stats(pooled_panel, timeSlices, 3, "mse", batch_size = 512, 10)
# NNet_3_mae_stats <- NNet_fit_stats(pooled_panel, timeSlices, 3, "mae", batch_size = 512, 10)
# 
# NNet_3_mse_stats[[1]]$loss_stats
# NNet_3_mse_stats[[2]]$loss_stats
# NNet_3_mse_stats[[3]]$loss_stats
# 
# NNet_3_mae_stats[[1]]$loss_stats
# NNet_3_mae_stats[[2]]$loss_stats
# NNet_3_mae_stats[[3]]$loss_stats
# 
# ##
# 
# NNet_4_mse_stats <- NNet_fit_stats(pooled_panel, timeSlices, 4, "mse", batch_size = 32, 30)
# NNet_4_mae_stats <- NNet_fit_stats(pooled_panel, timeSlices, 4, "mae", batch_size = 32, 10)
# 
# NNet_4_mse_stats[[1]]$loss_stats
# NNet_4_mse_stats[[2]]$loss_stats
# NNet_4_mse_stats[[3]]$loss_stats
# 
# NNet_4_mae_stats[[1]]$loss_stats
# NNet_4_mae_stats[[2]]$loss_stats
# NNet_4_mae_stats[[3]]$loss_stats
# 
# ##
# 
NNet_5_mse_stats <- NNet_fit_stats(pooled_panel, timeSlices, 5, "mse", batch_size = 32, 10)
NNet_5_mae_stats <- NNet_fit_stats(pooled_panel, timeSlices, 5, "mae", batch_size = 32, 10)

NNet_5_mse_stats[[1]]$loss_stats
NNet_5_mse_stats[[2]]$loss_stats
NNet_5_mse_stats[[3]]$loss_stats

NNet_5_mse_stats[[1]]$forecasts

NNet_5_mse_stats[[1]]$variable_importance %>%
  arrange(desc(importance))

NNet_5_mae_stats[[1]]$loss_stats
NNet_5_mae_stats[[2]]$loss_stats
NNet_5_mae_stats[[3]]$loss_stats

NNet_5_mae_stats[[1]]$forecasts
NNet_5_mae_stats[[2]]$forecasts
NNet_5_mae_stats[[3]]$forecasts

NNet_5_mae_stats[[1]]$variable_importance %>%
  arrange(desc(importance))

# Other general observations
# Increasing the number of layers is NOT good. Neural Network 5 has validation loss graphs that do not look right at all

# Neural Networks WRT MAE are not good for highly non-linear specifications
# Very unstable out of sample r squared values
```

```{r}
## RNN and LSTM Neural Network Models

# Prepare the data into a format which RNNs and LSTM models from keras recognise and can work with
# We want a multipl einput (factor set), multiple output (multiple stocks) structure
# Annoyingly, this means we have to convert our dataset from tidy to array format

# Keras needs the X (input) to be an array of (sample no, time steps, no. of features) dimensions
# In our case, this corresponds to (length of training set, 1 as we are using 1 lag, 400 factors)

# Keras needs the Y output to be an array of (sample no, number of outputs per sample)
# In our case, this corresponds to (length of training set, no. of stocks/cross sectional units)

# Observations when fitting:
# LSTM parameter count is very explosive, usually having an LSTM unit with 8 units approaches GPU memory limit of 2GB already
# Stacking LSTM units doesn't seem to help too much
# Adding more vanilla layers also doesn't seem to help much
# Predictions seem to be well behaved compared to MLPs
# tanh seems to be better than ReLU, though as least ReLU works here
# Fitting wrt to MAE seems to be better
# Batch size seems to be best at around 32 or 64

## Variable Importance function for LSTM

LSTM_variable_importance <- function(X_array_test, Y_array_test, test_x, lstm_model) {

  variable_importance_df <- foreach(i = (1:400), .combine = "rbind", .packages = c("keras", "tensorflow", "reticulate")) %do% {
    index <- seq(1, 200, by = 1) + (i - 1)*200
    
    X_array_test_zero <- X_array_test
    for (t in 1:dim(X_array_test)[1]) {
      X_array_test_zero[t, 1, index] <- 0
    }

    original_R2 <- R2(predict(lstm_model, X_array_test), Y_array_test %>% as.vector(), form = "traditional")

    new_R2 <- R2(predict(lstm_model, X_array_test_zero), Y_array_test %>% as.vector(), form = "traditional")

    variable_importance <- data.frame(variable = colnames(test_x)[i], importance = (original_R2 - new_R2))

    variable_importance
  }
variable_importance_df
}

## Function to fit LSTM Model

LSTM_fit_stats <- function(pooled_panel, timeSlices, loss_function, batch_size, patience) {
  ## Initialize
  LSTM_stats <- rep(list(0), 3)
  
  ## Loop over different timeslices
  for (set in 1:3) {
    ## Important things
    
    LSTM_stats[[set]] <- list(loss_stats = data.frame(train_MAE = 0, train_MSE = 0, train_RMSE = 0, train_RSquare = 0, 
                                                      validation_MAE = 0, validation_MSE = 0, validation_RMSE = 0, validation_RSquare = 0, 
                                                      test_MAE = 0, test_MSE = 0, test_RMSE = 0, test_RSquare = 0),
                              #Other useful things
                              # Keep forecasts here in for nnet objects to make sure they aren't doing something stupid
                              forecasts = 0,
                              forecast_resids = 0,
                              model = 0,
                              variable_importance = 0)
    
    ## Train, validation, test
    
    ## Scale Data first
    
    train <- pooled_panel %>%
      dplyr::filter(time %in% timeSlices[[set]]$train)
    validation <- pooled_panel %>%
      dplyr::filter(time %in% timeSlices[[set]]$validation)
    test<- pooled_panel %>%
      dplyr::filter(time %in% timeSlices[[set]]$test)
      
    train_x <- train %>%
      dplyr::select(-rt, -stock, -time)
    train_x <- scale(train_x)
    col_means_train <- attr(train_x, "scaled:center") 
    col_stddevs_train <- attr(train_x, "scaled:scale")
    train_y <- train$rt
    
    validation_x <- validation %>%
      dplyr::select(-rt, -stock, -time)
    validation_x <- scale(validation_x, center = col_means_train, scale = col_stddevs_train)
    validation_y <- validation$rt
      
    test_x <- test %>%
      dplyr::select(-rt, -stock, -time)
    test_x <- scale(test_x, center = col_means_train, scale = col_stddevs_train)
    test_y <- test$rt
    
    pooled_panel_trunc <- rbind(
      cbind(rt = train_y, time = train$time, train_x),
      cbind(rt = validation_y, time = validation$time, validation_x),
      cbind(rt = test_y, time = test$time, test_x)
    ) %>%
      data.frame
    
    ## Change this to array format for use with LSTMs
    
    length <- max(timeSlices[[set]]$test) - 1
    X_array <- array(data = 0, dim = c(length, 1, 80000))
  
    Y_array <- array(data = 0, dim = c(length, 200))
    
    for(t in 1:length) {
      X_array[t, 1, ] <- pooled_panel_trunc %>%
      dplyr::filter(time == t+1) %>%
      dplyr::select(-rt, -time) %>%
      as.matrix() %>%
      as.vector()
    }
    
    for(t in 1:length) {
      Y_array[t, ] <- pooled_panel_trunc %>%
        dplyr::filter(time == t+1) %>%
        dplyr::select(rt) %>%
        as.matrix() %>%
        as.vector()
    }
    
    X_array_train <- X_array[(timeSlices[[set]]$train - 1), , , drop = F]
    X_array_validation <- X_array[(timeSlices[[set]]$validation - 1), , , drop = F]
    X_array_test <- X_array[(timeSlices[[set]]$test - 1), , , drop = F]
    
    Y_array_train <- Y_array[(timeSlices[[set]]$train - 1), ]
    Y_array_validation <- Y_array[(timeSlices[[set]]$validation - 1), ]
    Y_array_test <- Y_array[(timeSlices[[set]]$test - 1), ]
    
    ## Early stopping Callback
    
    early_stop <- callback_early_stopping(monitor = "val_loss", patience = patience, restore_best_weights = TRUE)
    
    print_dot_callback <- callback_lambda(
      on_epoch_end = function(epoch, logs) {
        if (epoch %% 50 == 0) cat("\n")
        cat(".")
      }
    ) 
    
    ## Fit the LSTM Model
    
    l1_penalty <- 0.001

    lstm_model <- keras_model_sequential() %>%
      layer_lstm(units = 4, input_shape = c(1, dim(X_array_train)[3]),
                 return_sequences = TRUE,
                 bias_initializer = initializer_zeros(),
                 kernel_initializer = initializer_truncated_normal(mean = 0, stddev = 0.05, seed = NULL)) %>%
      layer_activation("tanh") %>%
      layer_activity_regularization(l1 = l1_penalty) %>%
      layer_batch_normalization() %>%
      
      layer_dense(units = 4,
                  bias_initializer = initializer_zeros(),
                  kernel_initializer = initializer_truncated_normal(mean = 0, stddev = 0.05, seed = NULL)) %>%
      layer_activation("tanh") %>%
      layer_activity_regularization(l1 = l1_penalty) %>%
      layer_batch_normalization() %>%
      
      layer_dense(units = 200) %>%
      layer_activation("linear") %>%
      
      layer_flatten()
    
    lstm_model %>% compile(loss = "mse",
                           optimizer = optimizer_adam(lr = 0.001, beta_1 = 0.9, beta_2 = 0.999),
                           metrics = list("mae", "mse"))
    
    lstm_model_fit <- lstm_model %>% 
      fit(X_array_train, Y_array_train, epochs = 500,
          validation_data = list(X_array_validation, Y_array_validation), 
          batch_size = 64, shuffle = FALSE, verbose = 0,
          callbacks = list(early_stop, print_dot_callback))
    
    ## Save stuff to list object
    
    #Train
    train_predict <- lstm_model %>% predict(X_array_train) %>%
      as.vector()
    
    LSTM_stats[[set]]$loss_stats$train_MAE <- mae(Y_array_train %>% as.vector(), train_predict)
    LSTM_stats[[set]]$loss_stats$train_MSE <- mse(Y_array_train %>% as.vector(), train_predict)
    LSTM_stats[[set]]$loss_stats$train_RMSE <- rmse(Y_array_train %>% as.vector(), train_predict)
    LSTM_stats[[set]]$loss_stats$train_RSquare <- R2(train_predict, Y_array_train %>% as.vector(), form = "traditional")
          
    #Validation
    validation_predict <- lstm_model %>% predict(X_array_validation) %>%
      as.vector()
    
    LSTM_stats[[set]]$loss_stats$validation_MAE <- mae(Y_array_validation %>% as.vector(), validation_predict)
    LSTM_stats[[set]]$loss_stats$validation_MSE <- mse(Y_array_validation %>% as.vector(), validation_predict)
    LSTM_stats[[set]]$loss_stats$validation_RMSE <- rmse(Y_array_validation %>% as.vector(), validation_predict)
    LSTM_stats[[set]]$loss_stats$validation_RSquare <- R2(validation_predict, Y_array_validation %>% as.vector(), form = "traditional")
      
    #Test
    test_predict <- lstm_model %>% predict(X_array_test) %>%
      as.vector()
    
    LSTM_stats[[set]]$loss_stats$test_MAE <- mae(Y_array_test %>% as.vector(), test_predict)
    LSTM_stats[[set]]$loss_stats$test_MSE <- mse(Y_array_test %>% as.vector(), test_predict)
    LSTM_stats[[set]]$loss_stats$test_RMSE <- rmse(Y_array_test %>% as.vector(), test_predict)
    LSTM_stats[[set]]$loss_stats$test_RSquare <- R2(test_predict, Y_array_test %>% as.vector(), form = "traditional")
        
    #Forecasts
    LSTM_stats[[set]]$forecasts <- test_predict
    
    #Variable Importance
    LSTM_stats[[set]]$variable_importance <- LSTM_variable_importance(X_array_test, Y_array_test, test_x, lstm_model)
    
    #Clear Kera Session
    k_clear_session()
  }
  LSTM_stats
}
```

```{r}

LSTM_stats <- LSTM_fit_stats(pooled_panel, timeSlices, loss_function = "mse", batch_size = 32, patience = 50)

LSTM_stats[[1]]$loss_stats
LSTM_stats[[2]]$loss_stats
LSTM_stats[[3]]$loss_stats

LSTM_stats[[1]]$forecasts
```

```{r}
#################################################################################################################
## FFORMA implementation, pretending that each stock return series is an individual time series
## External regressors are only avialable through dimensional reduction techniques
## Not ideal at all, but the only practical way to implement this
#################################################################################################################

## function which extracts fforma features for a single univariate time series, and returns it as a dataframe

fforma_features <- function(time_series) {
  cbind(
    # Series length
    length = length(time_series),
    # nperiods, seasonal_period, trend, seasonality, linearity, curvature, spikiness e_acr1 and e_acf10
    tsfeatures(time_series, features = "stl_features"),
    # Stability
    tsfeatures(time_series, features = "stability"),
    # Lumpiness
    tsfeatures(time_series, features = "lumpiness"),
    # Hurst
    tsfeatures(time_series, features = "hurst"),
    # Nonlinearity
    tsfeatures(time_series, features = "nonlinearity"),
    # Holt alpha and beta
    tsfeatures(time_series, features = "holt_parameters"),
    # HW alpha and beta, only applies for seasonal time series
    #tsfeatures(time_series, features = "hw_parameters"),
    # Unitroot statistics
    tsfeatures(time_series, features = c("unitroot_kpss", "unitroot_pp")),
    # ACF features
    tsfeatures(time_series, features = "acf_features"),
    # PACF features
    tsfeatures(time_series, features = "pacf_features"),
    # Crossing point
    tsfeatures(time_series, features = "crossing_points"),
    # flat_spots
    tsfeatures(time_series, features = "flat_spots"),
    # arch_lm
    tsfeatures(time_series, features = "arch_stat"),
    # Heterogeneity
    tsfeatures(time_series, features = "heterogeneity"),
    ####
    ## Extra Features, as it seems the above are not very sueful
    tsfeatures(time_series, features = "std1st_der"),
    tsfeatures(time_series, features = "outlierinclude_mdrmd"),
    tsfeatures(time_series, features = "localsimple_taures"),
    tsfeatures(time_series, features = "walker_propcross"),
    tsfeatures(time_series, features = c("max_level_shift", "max_var_shift", "max_kl_shift")),
    ## Custom Features
    data.frame(realized_volatility = sum(time_series^2))
  )
}
```

```{r}
####################
## Getting the data into the right format for use with xgboost and its custom loss function
## FFORMA requires a list with the entries:
# Data - a matrix with the features extracted via tsfeatures, one series per row
# Errors - a matrix of the errors (losses) produced by the forecasting methods, one series per row
# Labels - a numeric vector from 0 to nclass -1, representing the method with the lowest forecast error

## Function which fits individual consituent forecasting methods to a panel of time series and
## returns a list of the forecast errors for validation/test, and forecasts for validation/test

fforma_fit_individual <- function(panel, timeSlices, set) {
  ## Filter Dataset first
  pooled_panel_train <- pooled_panel %>%
    filter(time %in% timeSlices[[set]]$train)
  
  pooled_panel_validation <- pooled_panel %>%
    filter(time %in% timeSlices[[set]]$validation)
  
  pooled_panel_test <- pooled_panel %>%
    filter(time %in% timeSlices[[set]]$test)
  
  out.sample_length <- length(timeSlices[[set]]$validation) + length(timeSlices[[set]]$test)
  valid_index <- 1:length(timeSlices[[set]]$validation)
  test_index <- (1:out.sample_length)[-valid_index]
  
  ## Dimensional Reduction of dataset regressors
  
  umap_model <- umap(pooled_panel_train %>% dplyr::select(-rt, -stock, -time), ret_model = TRUE)
  
  pooled_panel_train_umap <- umap_model$embedding
  
  pooled_panel_validation_umap <- umap_transform(pooled_panel_validation %>% 
                                                   dplyr::select(-rt, -stock, -time), model = umap_model)
  
  pooled_panel_test_umap <- umap_transform(pooled_panel_test %>% 
                                             dplyr::select(-rt, -stock, -time), model = umap_model)
  
  ## 
  dataset_list <- foreach(i = 1:length(unique(pooled_panel$stock))) %dopar% {
    stock_id <- unique(pooled_panel$stock)

    time_series <- pooled_panel_train %>%
      filter(stock == stock_id[i]) %>%
      dplyr::select(rt) %>%
      ts()
    
    time_series_umap <- pooled_panel_train_umap %>%
      data.frame() %>%
      mutate(stock = pooled_panel_train$stock) %>%
      filter(stock == stock_id[i]) %>%
      dplyr::select(-stock)
    
    time_series_validation <- pooled_panel_validation %>%
      filter(stock == stock_id[i]) %>%
      dplyr::select(rt) %>%
      ts()
    
    time_series_validation_umap <- pooled_panel_validation_umap %>%
      data.frame() %>%
      mutate(stock = pooled_panel_validation$stock) %>%
      filter(stock == stock_id[i]) %>%
      dplyr::select(-stock)
  
    time_series_test <- pooled_panel_test %>%
      filter(stock == stock_id[i]) %>%
      dplyr::select(rt) %>%
      ts()
    
    time_series_test_umap <- pooled_panel_test_umap %>%
      data.frame() %>%
      mutate(stock = pooled_panel_test$stock) %>%
      filter(stock == stock_id[i]) %>%
      dplyr::select(-stock)
    
    ## Combining all sets, required for rugarch to function correctly
    time_series_all <- c(time_series, time_series_validation, time_series_test) %>%
      ts()
    
    time_series_umap_all <- rbind(time_series_umap, time_series_validation_umap, time_series_test_umap)
    
    ############################################
    ## Estimate the constituent models first
    ############################################
    
    # Naive
    naive_model <- naive(time_series, h = out.sample_length)
    
    # Random walk with drift
    random_walk_model <- rwf(time_series, h = out.sample_length, drift = TRUE)
    
    # Theta Method
    theta_model <- thetaf(time_series, h = out.sample_length)
    
    ## Auto arima
    auto_arima_model <- auto.arima(time_series)
    
    # Auto ETS
    auto_ets_model <- ets(time_series)
    
    # TBATS
    tbats_model <- tbats(time_series)
    
    # Neural Network time series forecasts
    nnetar_model <- nnetar(time_series)
    
    # ARMA 1, 1 with GARCH 1, 1 ged errors, no external regressors
    ugarch_spec <- ugarchspec(variance.model = list(model = "sGARCH",
                                                      garchOrder = c(0, 0), 
                                                      submodel = NULL, 
                                                      external.regressors = NULL, 
                                                      variance.targeting = FALSE), 
                                mean.model     = list(armaOrder = c(1, 1), 
                                                      include.mean = TRUE,
                                                      external.regressors = NULL),
                                distribution.model = "std")
    
    ugarch_spec_fit <- ugarchfit(ugarch_spec, data = time_series_all, out.sample = out.sample_length,
                                 solver = "hybrid")
    ugarch_forecast <- ugarchforecast(ugarch_spec_fit, n.ahead = 1, n.roll = out.sample_length - 1)
    
    # ARMA 1, 1 with GARCH 1, 1 ged errors with umap as external regressors
    ugarch_spec_umap <- ugarchspec(variance.model = list(model = "sGARCH",
                                                         garchOrder = c(0, 0), 
                                                         submodel = NULL, 
                                                         external.regressors = time_series_umap_all, 
                                                         variance.targeting = FALSE), 
                                   mean.model     = list(armaOrder = c(1, 1), 
                                                         include.mean = TRUE,
                                                         external.regressors = time_series_umap_all),
                                   distribution.model = "std")
    
    ugarch_spec_fit_umap <- ugarchfit(ugarch_spec, data = time_series_all, out.sample = out.sample_length,
                                      solver = "hybrid")
    ugarch_forecast_umap <- ugarchforecast(ugarch_spec_fit, n.ahead = 1, n.roll = out.sample_length - 1,
                                           external.forecasts = rbind(time_series_validation_umap, time_series_test_umap) %>% list())
    
    ## Use the estimated models to generate actual forecasts for validation and test
    ## No easy way to organise this, just following original FFORMA structure of a list for each time series
    ## each containing an ff matrix with F rows and h columns, with F being the number of forecast methods
    
    list(
      validation_forecasts = rbind(
        predict(naive_model)$mean[valid_index],
        predict(random_walk_model)$mean[valid_index],
        predict(theta_model)$mean[valid_index],
        forecast(auto_arima_model, h = out.sample_length)$mean[valid_index],
        forecast(tbats_model, h = out.sample_length)$mean[valid_index],
        forecast(nnetar_model, h = out.sample_length)$mean[valid_index],
        fitted(ugarch_forecast)[valid_index],
        fitted(ugarch_forecast_umap)[valid_index]
      ),
      
      validation_error_mse = data.frame(
        naive = mse(time_series_validation, predict(naive_model)$mean[valid_index]),
        random_walk = mse(time_series_validation, predict(random_walk_model)$mean[valid_index]),
        theta = mse(time_series_validation, predict(theta_model)$mean[valid_index]),
        auto_arima = mse(time_series_validation, forecast(auto_arima_model, h = out.sample_length)$mean[valid_index]),
        tbats = mse(time_series_validation, forecast(tbats_model, h = out.sample_length)$mean[valid_index]),
        nnetar = mse(time_series_validation, forecast(nnetar_model, h = out.sample_length)$mean[valid_index]),
        garch = mse(time_series_validation, fitted(ugarch_forecast)[valid_index]),
        garch_umap = mse(time_series_validation, fitted(ugarch_forecast_umap)[valid_index])
      ),
      
      validation_error_mae = data.frame(
        naive = mae(time_series_validation, predict(naive_model)$mean[valid_index]),
        random_walk = mae(time_series_validation, predict(random_walk_model)$mean[valid_index]),
        theta = mae(time_series_validation, predict(theta_model)$mean[valid_index]),
        auto_arima = mae(time_series_validation, forecast(auto_arima_model, h = out.sample_length)$mean[valid_index]),
        tbats = mae(time_series_validation, forecast(tbats_model, h = out.sample_length)$mean[valid_index]),
        nnetar = mae(time_series_validation, forecast(nnetar_model, h = out.sample_length)$mean[valid_index]),
        garch = mae(time_series_validation, fitted(ugarch_forecast)[valid_index]),
        garch_umap = mae(time_series_validation, fitted(ugarch_forecast_umap)[valid_index])
      ),
      
      validation_error_rsquare = data.frame(
        naive = R2(predict(naive_model)$mean[valid_index], time_series_validation %>% c(), 
                   form = "traditional"),
        random_walk = R2(predict(random_walk_model)$mean[valid_index], time_series_validation %>% c(), 
                         form = "traditional"),
        theta = R2(predict(theta_model)$mean[valid_index], time_series_validation %>% c(), 
                   form = "traditional"),
        auto_arima = R2(forecast(auto_arima_model, h = out.sample_length)$mean[valid_index], time_series_validation %>% c(), 
                        form = "traditional"),
        tbats = R2(forecast(tbats_model, h = out.sample_length)$mean[valid_index], time_series_validation %>% c(), 
                   form = "traditional"),
        nnetar = R2(forecast(nnetar_model, h = out.sample_length)$mean[valid_index], time_series_validation %>% c(), 
                    form = "traditional"),
        garch = R2(fitted(ugarch_forecast)[valid_index], time_series_validation %>% c(), 
                   form = "traditional"),
        garch_umap = R2(fitted(ugarch_forecast_umap)[valid_index], time_series_validation %>% c(), 
                        form = "traditional")
      ),
      
      test_forecasts = rbind(
        predict(naive_model)$mean[test_index],
        predict(random_walk_model)$mean[test_index],
        predict(theta_model)$mean[test_index],
        forecast(auto_arima_model, h = out.sample_length)$mean[test_index],
        forecast(tbats_model, h = out.sample_length)$mean[test_index],
        forecast(nnetar_model, h = out.sample_length)$mean[test_index],
        fitted(ugarch_forecast)[test_index],
        fitted(ugarch_forecast_umap)[test_index]
      ),
      
      test_error_mse = data.frame(
        naive = mse(time_series_test, predict(naive_model)$mean[test_index]),
        random_walk = mse(time_series_test, predict(random_walk_model)$mean[test_index]),
        theta = mse(time_series_test, predict(theta_model)$mean[test_index]),
        auto_arima = mse(time_series_test, forecast(auto_arima_model, h = out.sample_length)$mean[test_index]),
        tbats = mse(time_series_test, forecast(tbats_model, h = out.sample_length)$mean[test_index]),
        nnetar = mse(time_series_test, forecast(nnetar_model, h = out.sample_length)$mean[test_index]),
        garch = mse(time_series_test, fitted(ugarch_forecast)[test_index]),
        garch_umap = mse(time_series_test, fitted(ugarch_forecast_umap)[test_index])
      ),
      
      test_error_mae = data.frame(
        naive = mse(time_series_test, predict(naive_model)$mean[test_index]),
        random_walk = mse(time_series_test, predict(random_walk_model)$mean[test_index]),
        theta = mse(time_series_test, predict(theta_model)$mean[test_index]),
        auto_arima = mse(time_series_test, forecast(auto_arima_model, h = out.sample_length)$mean[test_index]),
        tbats = mse(time_series_test, forecast(tbats_model, h = out.sample_length)$mean[test_index]),
        nnetar = mse(time_series_test, forecast(nnetar_model, h = out.sample_length)$mean[test_index]),
        garch = mse(time_series_test, fitted(ugarch_forecast)[test_index]),
        garch_umap = mse(time_series_test, fitted(ugarch_forecast_umap)[test_index])
      ),
      
      test_error_rsquare = data.frame(
        naive = R2(predict(naive_model)$mean[test_index], time_series_test %>% c(), 
                   form = "traditional"),
        random_walk = R2(predict(random_walk_model)$mean[test_index], time_series_test %>% c(), 
                         form = "traditional"),
        theta = R2(predict(theta_model)$mean[test_index], time_series_test %>% c(), 
                   form = "traditional"),
        auto_arima = R2(forecast(auto_arima_model, h = out.sample_length)$mean[test_index], time_series_test %>% c(), 
                        form = "traditional"),
        tbats = R2(forecast(tbats_model, h = out.sample_length)$mean[test_index], time_series_test %>% c(), 
                   form = "traditional"),
        nnetar = R2(forecast(nnetar_model, h = out.sample_length)$mean[test_index], time_series_test %>% c(), 
                    form = "traditional"),
        garch = R2(fitted(ugarch_forecast)[test_index], time_series_test%>% c(), 
                   form = "traditional"),
        garch_umap = R2(fitted(ugarch_forecast_umap)[test_index], time_series_test %>% c(), 
                        form = "traditional")
      )
    )
  }
  dataset_list
}
```

```{r}
##
## Function which takes the previously estimated fforma_fit list object, and returns it into a usable format to train the meta-learner
# Ie outpit a list object with:
# Data - a matrix with the features extracted via tsfeatures, one series per row
# Errors - a matrix of the errors (losses) produced by the forecasting methods, one series per row
# Labels - a numeric vector from 0 to nclass -1, representing the method with the lowest forecast error

## Function to return a data matrix with features extracted via tsfeatures, one series per row, given panel data
# Remeber that this function doesn't do train/validation/split, so you'll have to do these outside of it
fforma_data_matrix <- function(pooled_panel) {
  data <- foreach(i = 1:length(unique(pooled_panel$stock)), .combine = "rbind") %do% {
  stock_id <- unique(pooled_panel$stock)

  univariate_time_series <- pooled_panel %>%
    filter(stock == stock_id[i]) %>%
    dplyr::select(rt) %>%
    ts()
  
  fforma_features(univariate_time_series)
  }
  data
}

## Function to return an errors matrix

fforma_errors <- function(fforma_fit, loss_function) {
  foreach(i = 1:length(fforma_fit), .combine = "rbind") %do% {
    # MSE
    if (loss_function == "mse") {
      fforma_fit[[i]]$validation_error_mse
    } else if (loss_function == "mae") {
      fforma_fit[[i]]$validation_error_mae
    }
  }
}

# Labels matrix (using the erros matrix)

fforma_labels <- function(errors) {
  foreach(i = 1:nrow(errors), .combine = "rbind") %dopar% {
    data.frame(label = which.min(errors[i, ]) - 1)
  }
}

## Create final list with everything needed in FFORMA
## Looks pretty good

fforma_data_list <- function(pooled_panel, loss_function, fforma_fit) {
  data <- fforma_data_matrix(pooled_panel)
  errors <- fforma_errors(fforma_fit, loss_function)
  labels <- fforma_labels(errors)
  
  list(data = data, errors = errors, labels = labels)
}

train_list <- pooled_panel %>%
  filter(time %in% timeSlices[[1]]$train) %>%
  fforma_data_list(loss_function = "mse", fforma_fit) 

validation_list <- pooled_panel %>%
  filter(time %in% timeSlices[[1]]$validation) %>%
  fforma_data_list(loss_function = "mse", fforma_fit) 

test_list <- pooled_panel %>%
  filter(time %in% timeSlices[[1]]$test) %>%
  fforma_data_list(loss_function = "mse", fforma_fit) 

```

```{r}
## Training of meta learning using features and errors

## Need to supply custom loss function
## This is because the vanilla softmax loss function does not take into account of the actual errors associated with each forecasting method
# A vanilla softmax xgboost model will simply to try to use the features to identify the best performing model, not accounting for how similarly performing the models may be

softmax_transform <- function(x) {
  exp(x) / sum(exp(x))
}

error_softmax_obj <- function(preds, dtrain) {
  labels <- xgboost::getinfo(dtrain, "label")
  errors <- attr(dtrain, "errors")

  preds <- exp(preds)
  sp <- rowSums(preds)
  preds <- preds / replicate(ncol(preds), sp)
  rowsumerrors <- replicate(ncol(preds), rowSums(preds * errors))

  grad <- preds*(errors - rowsumerrors)
  # Upper Bound for Hessian
  hess <- errors*preds*(1.0-preds) - grad*preds
  # Correct Hessian
  #hess <- grad*(1.0 - 2.0*preds)
  #hess <- pmax(hess, 1e-16)
  return(list(grad = t(grad), hess = t(hess)))
}

#########################################
## num_class = ncol(errors) for some reason decided not to work
## Changed to num_class = max(labels) instead, this seems to work


## IMPORTANT NOTES ON HYPERPARAMETERS
## YOU MUST SET colsample_bytree = 1 for the tree to grow sufficiently deep enough 
## so that a constant model isn't estimated
## Additional penalization via gamma, lambda and alpha seems to give very slightly better validation error,
## but this often makes the model severely underfit, so set these to 0 as well
## min_child weight seems to be important for feasibility, but does not seem to have much effect on actual validation error
## Set this to something low to intentional overfit the tree

## Turning all of the above into an easy fit wrapper function

fforma_fit_stats <- function(pooled_panel, timeSlices, loss_function) {
  fforma_fit_stats_list <- rep(list(0), length(timeSlices))
  
  for (set in 1:length(fforma_fit_stats_list)) {
    fforma_fit_stats_list[[set]] <- list(loss_stats = data.frame(train_MAE = 0, train_MSE = 0, 
                                                                 train_RMSE = 0, train_RSquare = 0, 
                                                                 validation_MAE = 0, validation_MSE = 0,
                                                                 validation_RMSE = 0, validation_RSquare = 0, 
                                                                 test_MAE = 0, test_MSE = 0, 
                                                                 test_RMSE = 0, test_RSquare = 0),
                                         forecasts = 0,
                                         forecast_resids = 0,
                                         model = 0,
                                         variable_importance = 0)
    
    ## Fit all the individual models to all the time series first
    fforma_fit <- fforma_fit_individual(pooled_panel, timeSlices, set)
    
    ## Using the fforma_fit object, then create the train, validation and test lists to use with XGBOOST
    train_list <- pooled_panel %>%
      filter(time %in% timeSlices[[set]]$train) %>%
      fforma_data_list(loss_function, fforma_fit) 
    
    validation_list <- pooled_panel %>%
      filter(time %in% timeSlices[[set]]$validation) %>%
      fforma_data_list(loss_function, fforma_fit) 
    
    test_list <- pooled_panel %>%
      filter(time %in% timeSlices[[set]]$test) %>%
      fforma_data_list(loss_function, fforma_fit) 
    #################################################################
    ## XGBOOST
    
    dtrain <- xgb.DMatrix(data = as.matrix(train_list$data), 
                          label = train_list$labels[, 1])
    labels <- train_list$labels[, 1]
    errors <- train_list$errors
    attr(dtrain, "errors") <- train_list$errors
    
    param <- list(booster = "gbtree",
                  max_depth = 6, eta = 0.3, gamma = 0,
                  lambda = 0, alpha = 0, nthread = 12,
                  min_child_weight = 0.01,
                      num_class = ncol(errors),
                  objective = error_softmax_obj,
                      subsample = 1,
                      colsample_bytree = 1)
    
    ## Fit
    bst <- xgboost::xgb.train(params = param, data = dtrain, nrounds = 100, verbose = 1)
    
    ## 
    
    ## Validation Predictions
    ## Note that this was originally used for the bayesian optimization of parameters
    ## As we are skipping that due to computational restrictions, this will also be commented out
    pred <- predict(bst, newdata = xgboost::xgb.DMatrix(as.matrix(validation_list$data)),
                    outputmargin = TRUE, reshape=TRUE)
    pred <- t(apply(pred, 1, softmax_transform))

    validation_forecasts <- foreach(i = 1:nrow(pred), .combine = "rbind") %do% {
      pred[i, ] %*% fforma_fit[[i]]$validation_forecasts
    }
    
    validation_forecasts <- validation_forecasts %>% t %>% c
    
    ## Test Set Predictions
    pred <- predict(bst, newdata = xgboost::xgb.DMatrix(as.matrix(test_list$data)), 
                    outputmargin = TRUE, reshape=TRUE)
    pred <- t(apply(pred, 1, softmax_transform))
    test_forecasts <- foreach(i = 1:nrow(pred), .combine = "rbind") %do% {
      pred[i, ] %*% fforma_fit[[i]]$test_forecasts
    }
    
    test_forecasts <- test_forecasts %>% t %>% c
    
    train_y <- pooled_panel %>%
      dplyr::filter(time %in% timeSlices[[set]]$train) %>%
      dplyr::select(rt)
    
    validation_y <- pooled_panel %>%
      dplyr::filter(time %in% timeSlices[[set]]$validation) %>%
      dplyr::select(rt)
    
    test_y <- pooled_panel %>%
      dplyr::filter(time %in% timeSlices[[set]]$test) %>%
      dplyr::select(rt)
    
    ## Note that these forecasting methods don't really have a "training" error, 
    ## so no in sample stats will be available
    ## Not a huge issue anyway, as no one cares
    
    fforma_fit_stats_list[[set]]$loss_stats$validation_MAE <- mae(validation_y$rt, validation_forecasts)
    fforma_fit_stats_list[[set]]$loss_stats$validation_MSE <- mse(validation_y$rt, validation_forecasts)
    fforma_fit_stats_list[[set]]$loss_stats$validation_RMSE <- rmse(validation_y$rt, validation_forecasts)
    fforma_fit_stats_list[[set]]$loss_stats$validation_RSquare <- R2(validation_forecasts, validation_y$rt, 
                                                                     form = "traditional")
    
    fforma_fit_stats_list[[set]]$loss_stats$test_MAE <- mae(test_y$rt, test_forecasts)
    fforma_fit_stats_list[[set]]$loss_stats$test_MSE <- mse(test_y$rt, test_forecasts)
    fforma_fit_stats_list[[set]]$loss_stats$test_RMSE <- rmse(test_y$rt, test_forecasts)
    fforma_fit_stats_list[[set]]$loss_stats$test_RSquare <- R2(test_forecasts, test_y$rt, 
                                                               form = "traditional")
    ## Variable importance skipped for now as its really annoying to implement, 
    ## and more importantly FFORMA doesn't care too much about external regressors anyway
    
    fforma_fit_stats_list[[set]]$forecasts <- test_forecasts %>% as.vector()
    
    fforma_fit_stats_list[[set]]$forecast_resids <- (test_y$rt - test_forecasts) %>% as.vector()
    
    fforma_fit_stats_list[[set]]$model <- bst
  }
  fforma_fit_stats_list
}

fforma <- fforma_fit_stats(pooled_panel, timeSlices, "mse")
## Checking results

fforma[[1]]$loss_stats

fforma[[2]]$loss_stats

fforma[[3]]$loss_stats
```


```{r}
## Bayesian gridsearch/optimisation
## Working, but VERY computationally intensive and as such would not scale at all
## Ran a few times, and just put the "best" hyperparameters directly into the xgboost params already
## namely, gamma = 0.2, subsample = 1, colsample_bytree = 1, nrounds = 100
## Commented out

# bayes_xgb <- function(max_depth, eta, gamma, subsample, colsample_bytree, 
#                       lambda, alpha, min_child_weight, nrounds) {
#   param <- list(max_depth = max_depth, eta = eta, 
#                 gamma = gamma, lambda = lambda, alpha = alpha, min_child_weight = min_child_weight, nthread = 12,
#                 objective = error_softmax_obj,
#                 num_class = max(labels),
#                 subsample = subsample,
#                 colsample_bytree = colsample_bytree)
#   
#   dtrain <- xgb.DMatrix(data = as.matrix(train_list$data), 
#                         label = train_list$labels[, 1])
#   
#   attr(dtrain, "errors") <- train_list$errors
#   
#   bst <- xgboost::xgb.train(params = param, data = dtrain, nrounds = 400, verbose = 1)
#   
#   pred <- predict(bst, newdata = xgboost::xgb.DMatrix(as.matrix(validation_list$data)), 
#                   outputmargin = TRUE, reshape=TRUE)
#   pred <- t(apply(pred, 1, softmax_transform))
#   
#   validation_forecasts <- foreach(i = 1:nrow(pred), .combine = "rbind") %do% {
#     pred[i, ] %*% set_1[[i]]$validation_forecasts
#   }
#   
#   validation_error <- foreach(i = 1:nrow(pred), .combine = "rbind") %do% {
#     t(pred[i, ]) %*% unlist(set_1[[i]]$validation_error_mse)
#   }
#   
#   final_error <- mean(validation_error)
#   
#   list(Score = -final_error, Pred = validation_forecasts)
# }
# 
# prefound_grid <- list(max_depth = 10L, eta = 0.3, gamma = 0, subsample = 0.9, colsample_bytree = 1.0, 
#                       lambda = 0, alpha = 0, min_child_weight = 0.001, nrounds = 200)
# 
# bayes_test <- BayesianOptimization(bayes_xgb, 
#                                    bounds = list(max_depth = c(6L, 14L), eta = c(0.001, 0.5), gamma = c(0, 1), 
#                                    subsample = c(0.1, 1.0), colsample_bytree = c(0.8, 1.0),
#                                    lambda = c(0, 0.5), alpha = c(0, 0.2), min_child_weight = c(0.001, 1),
#                                    nrounds = c(1L, 250L)),
#                                    init_grid_dt = prefound_grid, init_points = 5, n_iter = 30)
```

