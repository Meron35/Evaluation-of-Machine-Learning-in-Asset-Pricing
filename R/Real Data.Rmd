---
title: "Real Data"
author: "Ze Yu Zhong"
date: "22/07/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
################
##Load Libraries
################

library(tidyverse)
library(keras)
library(ggplot2)
library(forecast)
library(rlist)
library(Metrics)
library(ranger)
library(caret)
library(readr)
library(zoo)
library(readxl)
library(Matrix)

#Parallel Computing
library(foreach)
library(doFuture)
#Registering
registerDoFuture()
plan(multisession)

set.seed(27935248)
```

```{r}
#################################################
# Load RAW data courtesy of Gu et al
#################################################
# This is about ~ 3GB in size in total
# Note that because the honours lab computers don't have their own hard drives (files are stored on some network location), this initial loading processis very slow (bottlenecked by network speed)
datashare_RAW <- read_csv("data/datashare/datashare.csv",
                          # Always make sure to force col_types so that read_csv doesn't assume weird stuff
                          col_types = cols(.default = "d"))

# Export stock IDs so that we can query WRDS returns
# WRDS wants a txt file with one code per line and nothing else
datashare_stock_ids <- unique(datashare_RAW$permno)
write.table(datashare_stock_ids, file = "datashare_stock_ids.txt", sep = " ", 
            row.names = FALSE, col.names = FALSE)

# datashare with RET, Prices, and primary exchange
datashare_RET_RAW <- read_csv("data/datashare/datashare_PRC.csv") %>%
  # Rename some columns
  mutate(DATE = date) %>%
  select(-date) %>%
  mutate(permno = PERMNO) %>%
  select(-PERMNO)

# It seems that datashare with RET has more rows than the datashare file
# Full join them for now

datashare_RAW_join <- full_join(datashare_RAW, datashare_RET_RAW, by = c("permno", "DATE")) %>%
  # Reorder variables
  select(DATE, permno, PRC, everything())

colnames(datashare_RAW_join)

head(datashare_RAW_join)

individual_factor_names <- colnames(datashare_RAW)[3:96]

datashare <- datashare_RAW_join %>%
  mutate(Time = as.yearmon(as.character(DATE), "%Y%m%d")) %>%
  select(-DATE) %>%
  # Generate log prices
  # Drop RET which is monthly return
  select(Time, permno, PRC, -RET, everything()) %>%
  # Filter so that we only have NASDAQ stocks
  # Remember that the NASDAQ only opened in 1971 ish
  filter(PRIMEXCH == "Q") %>%
  select(-PRIMEXCH) %>%
  # Negative PRC means that there was no closing price, and the dash denotes the bid ask average instead
  # Not too unreasonable to just take this as the actual price 
  # Note that PRC has not been corrected for stock splits etc. This is more sensible for modelling purposes
  mutate(PRC = abs(PRC)) %>%
  # Prices of 0 mean that neither the bid ask average or price were available, ie missing
  filter(PRC != 0) %>%
  # Filter out PRC < 5 to get rid of penny stocks
  filter(PRC >= 5) %>%
  # Filter out so that only shares with share codes of 10 or 11 are included (other codes refer to other instruments such as REITs, etc)
  # Don't have any idea why Gu et al thought this was originally sensible
  filter(SHRCD == 10 | SHRCD == 11) %>%
  # SIC codes according to WRDS/CRSP are recommended to be used with caution, as they are not strictly enforced
  # Companies can also belong in multiple SIC codes, or change SIC codes over time. This is not adequately captured by CRSP
  # Also don't make much consistent sense either
  # Personal decision: drop them entirely
  select(-sic2) %>%
  # We will construct quarterly returns using the end of each month
  # Ie using the 3rd, 6th, 9th and 12th month of each year
  # yearmon stores year + 0 for January. Exploit this to filter out non-quarter months
  filter((as.yearmon(Time) %% 1) == 2/12 | (as.yearmon(Time) %% 1) == 5/12 | (as.yearmon(Time) %% 1) == 8/12 | (as.yearmon(Time) %% 1) == 11/12) %>%
  # Convert Time to yearqtr format
  # yearqtr stores data as year + 0/4 for q1, 1/4 for q2, etc
  mutate(Time = as.yearqtr(Time - 2/12)) %>%
  select(-RET)

datashare_sample <- sample_frac(datashare, 0.25)
write.csv(datashare_sample, file = "datashare_sample.csv")

## Generate quarterly returns
## This approach keeps companies that were were unlisted and relisted
## Sensible, because companies can be bought out and floated again, etc.
## Not likely to make a huge difference, but actual quarterly returns are computed, instead of log quarterly returns
## This is because stock prices can in some cases have rather large movements over a quarter
time_df <- data.frame(Time = unique(datashare$Time)) %>%
  mutate(Time = as.numeric(Time))

datashare <- foreach(i = (1:length(unique(datashare$permno))), .combine = rbind) %dopar% {
  datashare_stock <- datashare %>%
    mutate(Time = as.numeric(Time)) %>%
    filter(permno == unique(datashare$permno)[i]) %>%
    full_join(time_df, by = "Time") %>%
    arrange(Time) %>%
    mutate(RET_M = (PRC - lag(PRC))/lag(PRC)) %>%
    drop_na(RET_M)
  datashare_stock
}

# Reorder (again)
datashare <- datashare %>%
  select(Time, permno, PRC, RET_M, everything()) %>%
  mutate(Time = as.yearqtr(Time))

# Check Missing data stats
# Missing data tends to get a bit better as time goes forward
# Therefore conduct this with 
# Rule of thumb: drop any column missing more than 20% of data
# Best trade off between missing columns and maintaing a large dataset seems to be cutting it off ~1990s.

missing_factors <- datashare %>% 
  # Change time filtering here
  filter(Time >= 1993.5) %>%
  gather(col, value) %>%
  group_by(col) %>%
  summarize(missing_share = mean(is.na(value))) %>%
  filter(missing_share > 0.20)

# Double check against original RAW data
# Same story, original dataset is NOT GOOD
datashare_RAW %>% 
  mutate(Time = as.yearmon(as.character(DATE), "%Y%m%d")) %>%
  filter(Time > 1996) %>%
  gather(col, value) %>%
  group_by(col) %>%
  summarize(missing_share = mean(is.na(value))) %>%
  filter(missing_share > 0.20)

## Remove previously identified factors
datashare_filtered <- datashare %>%
  filter(Time >= 1993.5) %>%
  select(-c(missing_factors$col))

# Dealing with Missing characteristics
# Function to impute missing characteristics with their CROSS SECTIONAL median (as with gu et al) or mean (conventional) for each stock
# THIS FUNCTION WORKS PROPERLY, HOWEVER THE DATASET HAS TOO MUCH MISSING DATA
# Many of the earlier cross sections are completely missing any sort of data for some characteristics
# This means that it is impossible to impute a cross sectional mean/median
# The foreach loop may through some errors if this is the case but will still function correctly, check the output to see which exact factors are missing
# Don't know how Gu et al did this

# Function to calculate cross sectional medians/means from panel dataset, mainly for checking purposes
# Assumes that the dataset has a "Time" column
# doFuture has a maxsize default of around 500 MB
# In the current setting this is sufficient for our dataset, but you can set the maxsize to something larger like so:
#options(future.globals.maxSize = 3e+9)

cross_sectional_values <- function(dataset, impute_type) {
  
  time_periods <- unique(dataset$Time)
  
  cross_sectional_values_df <- foreach(t = 1:length(time_periods), .combine = "rbind") %dopar% {
    
    dataset_cross_section <- dataset %>%
      filter(Time == time_periods[t])
    
    # Mean Case
    if (impute_type == "mean") {
      cross_sectional_values <- dataset_cross_section %>% 
        summarize_all(mean, na.rm = TRUE) %>%
        mutate(Time = as.yearqtr(Time))
    }
    # Median Case
    else {
      cross_sectional_values <- dataset_cross_section %>% 
        summarize_all(median, na.rm = TRUE) %>%
        mutate(Time = as.yearqtr(Time))
    }
    cross_sectional_values
  }
  cross_sectional_values_df
}

test <- cross_sectional_values(datashare_filtered, impute_type = "median")

# Checks to see if there are any time periods where there is no cross sectional imputation possible due to missing data
# Looks all good
cbind(test$Time, (test)[colSums(is.na(test)) > 0])

colnames(test)[colSums(is.na(test)) > 0]

# Function to impute cross sectional values

impute_cross_section <- function(dataset, impute_type) {
  time_periods <- unique(dataset$Time)
  
  imputed_cross_section_df <- foreach(t = 1:length(time_periods), .combine = "rbind") %dopar% {
    dataset_cross_section <- dataset %>%
      filter(Time == time_periods[t])
    
    if (impute_type == "median") {
      # Impute the median for ALL columns
      dataset_cross_section %>%
        impute_median()
    } else {
      dataset_cross_section %>%
        impute_mean()
    }
  }
  imputed_cross_section_df
}

datashare_imputed <- impute_cross_section(datashare_filtered, impute_type = "median")

# Summary Statistics
summary(datashare_imputed)

length(unique(datashare_imputed$permno))

ncol(datashare_filtered)

## With that... FINISHED CLEANING DATASHARE
## We still have ~60 or so factors left, not too bad
# Final imputed dataset is ~100mb in size, quite manageable

```

```{r}
####################################################
# Load RAW Welch Goyal Data
####################################################
# Note that Gu et al only used:
# Dividend price ratio dp, earnings price ratio ep, book to market ratio bm,
# net equity expansion ntis, Treasury bill rate tbl, term spread tms,
# Default spread dfy, stock variance svar
PredictorData2017_RAW <- read_excel("data/factors/PredictorData2017.xlsx", 
                                    # Specify quarterly as we are now working with quarterly data
                                    sheet = "Quarterly",
                                    na = "NaN")

PredictorData2017 <- PredictorData2017_RAW %>%
  mutate(Time = as.yearqtr(as.character(yyyyq), "%Y%q")) %>%
  select(-yyyyq) %>%
  # Dividend price ratio is the difference between the log of dividends and the log of prices
  # Note that log(D12) - log(Index) is negative which doesn't make sense as a ratio
  mutate(dp = abs(log(D12) - log(Index))) %>%
  # earnings price ratio is the difference between the log of earnings and the log of prices
  # Note that log(E12) - log(Index) is negative which doesn't make sense as a ratio
  mutate(ep = abs(log(E12) - log(Index))) %>%
  # Term Spread is the difference between long term yield on gov bonds and treasury bills
  mutate(tms = lty - tbl) %>%
  # Default spread is the difference between BAA and AAA-rated corporate bond yields
  mutate(dfy = abs(BAA - AAA)) %>%
  # Rename b/m to bm
  rename(bm = `b/m`) %>%
  # Reorder
  select(Time, everything())

# Subset so that we have the predictors we actually care about
# Note that some of these by default have the same names as those in the individual factor set
# Therefore, prefix them with macro_ to make it clearer
# Also filter out all the entries that are not in the datashare dataset as they can't be used (Note that this also takes care of all missing values in this dataset)
# Filter it so that we only have the years we are interested in (same filtering as datashare)
macro_predictors <- PredictorData2017 %>%
  select(Time, dp, ep, bm, ntis, tbl, tms, dfy, svar) %>%
  rename_at(vars(-Time), function(x) paste0("macro_", x)) %>%
  # Create constant
  mutate(macro_constant = 1) %>%
  # Reorder
  select(Time, macro_constant, everything()) %>%
  filter(Time >= 1993.5)

# DONE WITH WELCH GOYAL
```


```{r}
#############################
# Combining everything
#############################
# macro predictors \kronecker individual factors (excluding ids)
# Kronecker product approach is waaay too involved. It would require sorting everything back into array form (i, j, t) and applying kronecker function to each individual stock's panel, as was the case with simulation
# Unlike the simulation code though, here we are starting from a panel dataframe which is significantly harder to turn back into an array
# Alternative approach: generate the individual factors * macro factors as a separate dataframe via model matrix (or similar), then combine them back together
# Remember that kronecker would give us constant*individual factors + individual factors*macro factors
# This EXCLUDES macro factors by themselves

macro_factor_names <- colnames(select(macro_predictors, -Time))
individual_factor_names <- colnames(select(datashare_imputed, -permno, -Time, -RET_M, -PRC))

interaction_terms <- rep(list(0), length(macro_factor_names))

for (i in 1:length(macro_factor_names)) {
  interaction_terms[[i]] <- paste(macro_factor_names[i], individual_factor_names, sep = ":", collapse = " + ")
}

f <- as.formula(c("~ ", paste(interaction_terms, collapse = "+")))

combined <- full_join(macro_predictors, datashare, by = "Time")

# Memory issues. This interaction matrix is way too large
interaction_matrix <- sparse.model.matrix(f, model.frame(~., combined))
dmy <- dummyVars(f, data = combined)

# This is pretty damn large at ~1.5GB. Interaction terms are brutal
final_dataset_x <- predict(dmy, combined)

# Double Check
colnames(final_dataset)
ncol(final_dataset)
```
