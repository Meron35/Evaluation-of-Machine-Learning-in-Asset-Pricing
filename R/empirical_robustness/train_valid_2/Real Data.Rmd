---
title: "Real Data"
author: "Ze Yu Zhong"
date: "22/07/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
################
##Load Libraries
################

library(keras)
library(ggplot2)
library(forecast)
library(rlist)
library(Metrics)
library(ranger)
library(caret)
library(readr)
library(zoo)
library(readxl)
library(Matrix)
library(speedglm)
library(tidyimpute)
library(tidyverse)
library(hqreg)
library(xtable)
library(randomForestSRC)

#Parallel Computing
library(foreach)
library(doFuture)
#Registering
registerDoFuture()
plan(multisession)

set.seed(27935248)
```

```{r gu_et_al_datashare}
#################################################
# Load RAW data courtesy of Gu et al
#################################################
# This is about ~ 3GB in size in total
# Note that because the honours lab computers don't have their own hard drives (files are stored on some network location), this initial loading processis very slow (bottlenecked by network speed)
datashare_RAW <- read_csv("~/GitHub/Evaluation-of-Machine-Learning-in-Asset-Pricing/R/data/datashare/datashare.csv",
                          # Always make sure to force col_types so that read_csv doesn't assume weird stuff
                          col_types = cols(.default = "d"))

# Export stock IDs so that we can query WRDS returns
# WRDS wants a txt file with one code per line and nothing else
datashare_stock_ids <- unique(datashare_RAW$permno)
write.table(datashare_stock_ids, file = "datashare_stock_ids.txt", sep = " ", 
            row.names = FALSE, col.names = FALSE)

# datashare with RET, Prices, and primary exchange
datashare_RET_RAW <- read_csv("~/GitHub/Evaluation-of-Machine-Learning-in-Asset-Pricing/R/data/datashare/datashare_PRC.csv") %>%
  # Rename some columns
  mutate(DATE = date) %>%
  dplyr::select(-date) %>%
  mutate(permno = PERMNO) %>%
  dplyr::select(-PERMNO)

# It seems that datashare with RET has more rows than the datashare file
# Full join them for now

datashare_RAW_join <- full_join(datashare_RAW, datashare_RET_RAW, by = c("permno", "DATE")) %>%
  # Reorder variables
  mutate(stock = permno) %>%
  dplyr::select(-permno) %>%
  dplyr::select(DATE, stock, PRC, everything())

colnames(datashare_RAW_join)

head(datashare_RAW_join)

rm(datashare_RAW)
rm(datashare_RET_RAW)

datashare <- datashare_RAW_join %>%
  mutate(time = as.yearmon(as.character(DATE), "%Y%m%d")) %>%
  dplyr::select(-DATE) %>%
  # Generate log prices
  # Drop RET which is monthly return
  dplyr::select(time, stock, PRC, -RET, everything()) %>%
  # Filter so that we only have NASDAQ stocks
  # Remember that the NASDAQ only opened in 1971 ish
  filter(PRIMEXCH == "Q") %>%
  dplyr::select(-PRIMEXCH) %>%
  # Negative PRC means that there was no closing price, and the dash denotes the bid ask average instead
  # Not too unreasonable to just take this as the actual price 
  # Note that PRC has not been corrected for stock splits etc. This is more sensible for modelling purposes
  mutate(PRC = abs(PRC)) %>%
  # Prices of 0 mean that neither the bid ask average or price were available, ie missing
  filter(PRC != 0) %>%
  # Filter out PRC < 5 to get rid of penny stocks
  filter(PRC >= 5) %>%
  # Filter out so that only shares with share codes of 10 or 11 are included (other codes refer to other instruments such as REITs, etc)
  # Don't have any idea why Gu et al thought this was originally sensible
  filter(SHRCD == 10 | SHRCD == 11) %>%
  dplyr::select(-SHRCD) %>%
  # SIC codes according to WRDS/CRSP are recommended to be used with caution, as they are not strictly enforced
  # Companies can also belong in multiple SIC codes, or change SIC codes over time. This is not adequately captured by CRSP
  # Also don't make much consistent sense either
  # Personal decision: drop them entirely
  dplyr::select(-sic2) %>%
  # Adjust lags for monthly data, such that they will correspond to quaterly conversion
  mutate(baspread = lag(baspread, 2)) %>%
  mutate(baspread = lag(beta, 2)) %>%
  mutate(baspread = lag(betasq, 2)) %>%
  mutate(idiovol = lag(idiovol, 2)) %>%
  mutate(ill = lag(ill, 2)) %>%
  mutate(indmom = lag(indmom, 2)) %>%
  mutate(mvel1 = lag(mvel1, 2)) %>%
  mutate(pricedelay = lag(pricedelay, 2)) %>%
  mutate(retvol = lag(retvol, 2)) %>%
  mutate(sp = lag(sp, 2)) %>%
  mutate(std_dolvol = lag(std_dolvol, 2)) %>%
  mutate(std_turn = lag(std_turn, 2)) %>%
  mutate(turn = lag(turn, 2)) %>%
  mutate(zerotrade = lag(zerotrade, 2)) %>%
  # We will construct quarterly returns using the end of each month
  # Ie using the 3rd, 6th, 9th and 12th month of each year
  # yearmon stores year + 0 for January. Exploit this to filter out non-quarter months
  filter((as.yearmon(time) %% 1) == 2/12 | (as.yearmon(time) %% 1) == 5/12 | (as.yearmon(time) %% 1) == 8/12 | (as.yearmon(time) %% 1) == 11/12) %>%
  # Convert time to yearqtr format
  # yearqtr stores data as year + 0/4 for q1, 1/4 for q2, etc
  mutate(time = as.yearqtr(time - 2/12)) %>%
  dplyr::select(-RET) %>%
  # Filter to contain more recent data
  filter(time >= 1993.25)

time_df <- data.frame(time = unique(datashare$time)) %>%
  mutate(time = as.numeric(time))

#####################################
## Generate quarterly returns
#####################################
## This approach drops all companies which do not have comlete stock price history
## This is because many methods require there to be an uniterreupted time series, e.g. GARCH
## Other approaches such as imputation, etc are less sensible due to the unpredictability of the data
## Has obvious drawbacks of introducing lots of survivorship, but what can you do

## Other drawbacks that are important
## Because the original dataset goes back to when the stock exchange was founded, the dataset needs to be filtered by time first
## Otherwise, you will inadventerdently filter out so that only stocks that have been listed on the exchange since the begginning are included
## The old apparoch kept ~ 20000 unique stocks
## This approach keeps a highly variable number of stocks, depending on when you filter the data from
## Going with a filter of >= 1993.5 gives us 195 unique stocks, just going with this for now for feasibility

datashare <- foreach(i = (1:length(unique(datashare$stock))), .combine = "rbind") %dopar% {
  datashare_stock <- datashare %>%
    mutate(time = as.numeric(time)) %>%
    filter(stock == unique(datashare$stock)[i]) %>%
    full_join(time_df, by = "time") %>%
    arrange(time) %>%
    mutate(rt = (PRC - lag(PRC))/lag(PRC)) %>%
    drop_na(rt)
}

length(unique(datashare$stock))

#####################################
## OLD, deprecated, NOT RUN
#####################################
## Generate quarterly returns
## This approach keeps companies that were were unlisted and relisted
## Sensible, because companies can be bought out and floated again, etc.
## Not likely to make a huge difference, but actual quarterly returns are computed, instead of log quarterly returns
## This is because stock prices can in some cases have rather large movements over a quarter
# datashare <- foreach(i = (1:length(unique(datashare$stock))), .combine = rbind) %dopar% {
#   datashare_stock <- datashare %>%
#     mutate(time = as.numeric(time)) %>%
#     filter(stock == unique(datashare$stock)[i]) %>%
#     full_join(time_df, by = "time") %>%
#     arrange(time) %>%
#     mutate(rt = (PRC - lag(PRC))/lag(PRC)) %>%
#     drop_na(rt)
#   datashare_stock
# }

# Reorder (again)
datashare <- datashare %>%
  dplyr::select(time, stock, rt, everything()) %>%
  dplyr::select(-PRC) %>%
  mutate(time = as.yearqtr(time))

# Check Missing data stats
# Missing data tends to get a bit better as time goes forward
# Therefore conduct this with 
# Rule of thumb: drop any column missing more than 20% of data
# Best trade off between missing columns and maintaing a large dataset seems to be cutting it off ~1990s.

missing_factors <- datashare %>% 
  # Change time filtering here
  filter(time >= 1993.5) %>%
  gather(col, value) %>%
  group_by(col) %>%
  summarize(missing_share = mean(is.na(value))) %>%
  filter(missing_share > 0.20)

write.csv(missing_factors, file = "missing_factors.csv")

## Remove previously identified factors
datashare_filtered <- datashare %>%
  filter(time >= 1993.5) %>%
  dplyr::select(-c(missing_factors$col))

# Dealing with Missing characteristics
# Function to impute missing characteristics with their CROSS SECTIONAL median (as with gu et al) or mean (conventional) for each stock
# THIS FUNCTION WORKS PROPERLY, HOWEVER THE DATASET HAS TOO MUCH MISSING DATA
# Many of the earlier cross sections are completely missing any sort of data for some characteristics
# This means that it is impossible to impute a cross sectional mean/median
# The foreach loop may through some errors if this is the case but will still function correctly, check the output to see which exact factors are missing
# Don't know how Gu et al did this

# Function to calculate cross sectional medians/means from panel dataset, mainly for checking purposes
# Assumes that the dataset has a "time" column
# doFuture has a maxsize default of around 500 MB
# In the current setting this is sufficient for our dataset, but you can set the maxsize to something larger like so:
#options(future.globals.maxSize = 3e+9)

cross_sectional_values <- function(dataset, impute_type) {
  
  time_periods <- unique(dataset$time)
  
  cross_sectional_values_df <- foreach(t = 1:length(time_periods), .combine = "rbind") %dopar% {
    
    dataset_cross_section <- dataset %>%
      filter(time == time_periods[t])
    
    # Mean Case
    if (impute_type == "mean") {
      cross_sectional_values <- dataset_cross_section %>% 
        summarize_all(mean, na.rm = TRUE) %>%
        mutate(time = as.yearqtr(time))
    }
    # Median Case
    else {
      cross_sectional_values <- dataset_cross_section %>% 
        summarize_all(median, na.rm = TRUE) %>%
        mutate(time = as.yearqtr(time))
    }
    cross_sectional_values
  }
  cross_sectional_values_df
}

test <- cross_sectional_values(datashare_filtered, impute_type = "median")

# Checks to see if there are any time periods where there is no cross sectional imputation possible due to missing data
# Looks all good
cbind(test$time, (test)[colSums(is.na(test)) > 0])

colnames(test)[colSums(is.na(test)) > 0]

# Function to impute cross sectional values

impute_cross_section <- function(dataset, impute_type) {
  time_periods <- unique(dataset$time)
  
  imputed_cross_section_df <- foreach(t = 1:length(time_periods), .combine = "rbind") %dopar% {
    dataset_cross_section <- dataset %>%
      filter(time == time_periods[t])
    
    if (impute_type == "median") {
      # Impute the median for ALL columns
      dataset_cross_section %>%
        impute_median()
    } else {
      dataset_cross_section %>%
        impute_mean()
    }
  }
  imputed_cross_section_df
}

datashare_imputed <- impute_cross_section(datashare_filtered, impute_type = "median") %>%
  mutate(time = as.yearqtr(time)) %>%
  # Rename some factors
  mutate(beta1 = beta) %>%
  dplyr::select(-beta) %>%
  # Make the names clearer
  rename_at(vars(-time, -stock, -rt), function(x) paste0("ind_", x))

# Summary Statistics
summary(datashare_imputed)

length(unique(datashare_imputed$stock))

ncol(datashare_filtered)

## We still have ~60 or so factors left, not too bad
# Final imputed dataset is ~100mb in size, quite manageable

## Function to cross sectionally normalize characteristics
# Lots of different approaches, we'll stick with the rank transformation of only the individual charcteristics for each cross section and leave the macro economic factors untouched
# This is consistent with what we did in the simulation exercise
cross_section_normalize <- function(dataset) {
  time_periods <- unique(dataset$time)
  
  normalize_cross_section_df <- foreach(t = 1:length(time_periods), .combine = "rbind") %dopar% {
    dataset_cross_section <- dataset %>%
      filter(time == time_periods[t])
    
    dataset_cross_section_y <- dataset_cross_section %>%
      dplyr::select(time, stock, rt)
    
    dataset_cross_section_x <- dataset_cross_section %>%
      dplyr::select(-time, -stock, -rt)
    
    # dataset_cross_section_x_norm <- data.frame(
    #   data = (2/(nrow(dataset_cross_section_x)*ncol(dataset_cross_section_x) + 1)) * 
    #     matrix(rank(data.frame(dataset_cross_section_x)),
    #            ncol = ncol(dataset_cross_section_x),
    #            nrow = nrow(dataset_cross_section_x)
    #            ) -
    #     matrix(data = 1, 
    #            ncol = ncol(dataset_cross_section_x), 
    #            nrow = nrow(dataset_cross_section_x)
    #            )
    #   )
    
    dataset_cross_section_x_norm <- scale(dataset_cross_section_x, center = FALSE, 
                                          scale = colSums(dataset_cross_section_x))
    
    colnames(dataset_cross_section_x_norm) <- colnames(dataset_cross_section_x)
    
    cbind(dataset_cross_section_y, dataset_cross_section_x_norm)
  }
  normalize_cross_section_df
}

# This function is working
# Some immediate problems: columns which were on a very large scale (such a company value mvel1) are still very large (ranging from 0.96 - 1 in value)
datashare_normalized <- cross_section_normalize(datashare_imputed) %>%
  mutate(time = as.yearqtr(time))

```

```{r welch_goyal_data}
####################################################
# Load RAW Welch Goyal Data
####################################################
# Note that Gu et al only used:
# Dividend price ratio dp, earnings price ratio ep, book to market ratio bm,
# net equity expansion ntis, Treasury bill rate tbl, term spread tms,
# Default spread dfy, stock variance svar
PredictorData2017_RAW <- read_excel("~/GitHub/Evaluation-of-Machine-Learning-in-Asset-Pricing/R/data/factors/PredictorData2017.xlsx", 
                                    # Specify quarterly as we are now working with quarterly data
                                    sheet = "Quarterly",
                                    na = "NaN")

PredictorData2017 <- PredictorData2017_RAW %>%
  mutate(time = as.yearqtr(as.character(yyyyq), "%Y%q")) %>%
  dplyr::select(-yyyyq) %>%
  # Dividend price ratio is the difference between the log of dividends and the log of prices
  # Note that log(D12) - log(Index) is negative which doesn't make sense as a ratio
  mutate(dp = abs(log(D12) - log(Index))) %>%
  # earnings price ratio is the difference between the log of earnings and the log of prices
  # Note that log(E12) - log(Index) is negative which doesn't make sense as a ratio
  mutate(ep = abs(log(E12) - log(Index))) %>%
  # Term Spread is the difference between long term yield on gov bonds and treasury bills
  mutate(tms = lty - tbl) %>%
  # Default spread is the difference between BAA and AAA-rated corporate bond yields
  mutate(dfy = abs(BAA - AAA)) %>%
  # Rename b/m to bm
  rename(bm = `b/m`) %>%
  # Reorder
  dplyr::select(time, everything())

# Subset so that we have the predictors we actually care about
# Note that some of these by default have the same names as those in the individual factor set
# Therefore, prefix them with macro_ to make it clearer
# Also filter out all the entries that are not in the datashare dataset as they can't be used (Note that this also takes care of all missing values in this dataset)
# Filter it so that we only have the years we are interested in (same filtering as datashare)
macro_predictors <- PredictorData2017 %>%
  dplyr::select(time, dp, ep, bm, ntis, tbl, tms, dfy, svar, Rfree) %>%
  # Lag Variables
  mutate(dp = lag(dp)) %>%
  mutate(ep = lag(ep)) %>%
  mutate(bm = lag(bm)) %>%
  mutate(ntis = lag(ntis)) %>%
  mutate(tbl = lag(tbl)) %>%
  mutate(tms = lag(tms)) %>%
  mutate(dfy = lag(dfy)) %>%
  mutate(svar = lag(svar)) %>%
  # Rename
  rename_at(vars(-time), function(x) paste0("macro_", x)) %>%
  # Create constant
  mutate(macro_constant = 1) %>%
  # Reorder
  dplyr::select(time, macro_constant, everything()) %>%
  filter(time >= 1993.5 & time < 2017)

# DONE WITH WELCH GOYAL
```

```{r combined_dataset}
#############################
# Combining everything
#############################
# macro predictors \kronecker individual factors (excluding ids)
# Kronecker product approach is waaay too involved. It would require sorting everything back into array form (i, j, t) and applying kronecker function to each individual stock's panel, as was the case with simulation
# Unlike the simulation code though, here we are starting from a panel dataframe which is significantly harder to turn back into an array
# Alternative approach: generate the individual factors * macro factors as a separate dataframe via model matrix (or similar), then combine them back together
# Remember that kronecker would give us constant*individual factors + individual factors*macro factors
# This EXCLUDES macro factors by themselves

macro_factor_names <- colnames(dplyr::select(macro_predictors, -time, -macro_Rfree))
write.csv(macro_factor_names, file = "macro_factor_names.csv")
individual_factor_names <- colnames(dplyr::select(datashare_imputed, -stock, -time, -rt))
write.csv(individual_factor_names, file = "individual_factor_names.csv")
```

```{r generate_final_dataset}
## This section builds in the macro dataset and egnerates the interaction terms
interaction_terms <- rep(list(0), length(macro_factor_names))

for (i in 1:length(macro_factor_names)) {
  interaction_terms[[i]] <- paste(macro_factor_names[i], individual_factor_names, sep = ":", collapse = " + ")
}

interaction_formula <- as.formula(c("~ ", paste(interaction_terms, collapse = "+")))

datashare_combined <- full_join(macro_predictors, datashare_normalized, by = "time") %>%
  # Create Excess Returns
  mutate(rt = rt - macro_Rfree) %>%
  select(-macro_Rfree) %>%
  # Reorder
  select(time, stock, rt, macro_constant, 
         macro_bm, macro_dfy, macro_dp, macro_ep, macro_ntis, macro_svar, macro_tbl, macro_tms,
         everything())

interaction_formula_dummyVars <- dummyVars(interaction_formula, data = datashare_combined)

# Testing if it works
final_dataset_x <- (predict(interaction_formula_dummyVars, datashare_combined))

# Strap on the stock, time and rt columns to this
final_dataset <- data.frame(stock = datashare_combined$stock, 
                            time = datashare_combined$time, 
                            rt = datashare_combined$rt, 
                            final_dataset_x)

rm(final_dataset_x)

saveRDS(final_dataset, file = "final_dataset.rds")
```

```{r}
# Double Check
colnames(final_dataset)
ncol(final_dataset) - 3
```

```{r timeslices}
## Setting up time slices, formulas, etc

## time slices function
## Different function needed due to different time format (inclusion of quarters)
## Makes it so that it works with all of our model fitting function from earlier
realdata_custom_timeslices <- function(start, initialWindow, horizon, validation_size, test_size, set_no) {
  time_slice <- list(train = 0, validation = 0, test = 0)
  time_slices <- rep(list(time_slice), set_no)
  
  for (t in 1:set_no) {
    time_slice$train <- seq(start, 
                            (start + initialWindow + (t-1) * horizon + 3/4), 
                            0.25)
    
    time_slice$validation <- seq(
      (start + initialWindow + (t-1) * horizon + 1),
      (start + initialWindow + (t-1) * horizon + validation_size + 3/4), 
      0.25
      )
    
    time_slice$test <- seq(
      (start + initialWindow + (t-1) * horizon + validation_size + 1),
      (start + (initialWindow + (t-1) * horizon) + validation_size + test_size + 3/4), 
      0.25
      )
    time_slices[[t]] <- time_slice
  }
  time_slices
}

# Formula

real_panel_formula <- function(panel){
  #Remove the first 3 colNames, as these correspond to the return, time and stock id
  panel_colnames <- colnames(panel)[-c(1:3)]
  f <- as.formula(c("rt ~", paste(panel_colnames, collapse = "+")))
  return(f)
}

# These are the settings working with a dataset going from 1993 - 2016
# Results in a traing:validation size ratio of 1.5
realdata_timeSlices <- realdata_custom_timeslices(
  start = 1994, initialWindow = 12, horizon = 1, validation_size = 7, test_size = 1, set_no = 3
  )

# Add the two quarters in from 1993
for(i in 1:3) {
  realdata_timeSlices[[i]]$train <- c(1993.5, 1993.75, realdata_timeSlices[[i]]$train)
}
```

```{r variable_importance_function}
##################################################################
## Function to conduct variable importance for real data
# This is slightly different from the functions used in the simulated context, as we do not care about the importance of ALL interaction terms
# Use the same function names as the simulated ones for simplicity, MAKE SURE YOU RUN THIS BEFORE FITTING EMPIRICAL DATA

# General plan:
# Use grepl & partial matching to match which columns contain which factor columns contain which factor
# Much more straightforward than generating a new panel containing a new set of interaction terms
# Slight problem, some of the column names can "overlap" E.g. bm and bm_ia
# Column names were changed is data prep step to get around this

################################################
# Linear Model
LM_variable_importance <- function(test, lm_model, 
                                   macro_factor_names, individual_factor_names) {
  # -1 to get rid of macro_constant
  all_factor_names <- c(macro_factor_names, individual_factor_names)[-1]
  test_x <- test[4:ncol(test)]
  
  # Specify .packages = "quantreg" here as it seems it isn't supported and therefore is missed by doFuture
  variable_importance_df <- foreach(i = (1:(length(all_factor_names))), .combine = "rbind", .packages = c("speedglm", "quantreg")) %dopar% {
    test_x_zero <- test_x
    
    zero_indices <- which(str_detect(colnames(test_x_zero), all_factor_names[i]) == 1)
    test_x_zero[, c(zero_indices)] <- 0
    
    original_R2 <- R2(predict(lm_model, newdata = test_x), test$rt, form = "traditional")
    
    new_R2 <- R2(predict(lm_model, newdata = test_x_zero), test$rt, form = "traditional")
    
    variable_importance <- data.frame(variable = all_factor_names[i], importance = (original_R2 - new_R2))
    
    variable_importance
  }
  variable_importance_df
}

# Penalized Linear Model

ELN_variable_importance <- function(test, eln_model, alpha, lambda,
                                    macro_factor_names, individual_factor_names) {
  all_factor_names <- c(macro_factor_names, individual_factor_names)[-1]
  test_x <- as.matrix(test[4:ncol(test)])
  
  variable_importance_df <- foreach(i = (1:(length(all_factor_names))), .combine = "rbind",
                                    .packages = "hqreg") %dopar% {
    test_x_zero <- test_x
    
    zero_indices <- which(str_detect(colnames(test_x_zero), all_factor_names[i]) == 1)
    test_x_zero[, zero_indices] <- 0
    
    original_R2 <- R2(predict(eln_model, test_x, alpha = alpha, lambda = lambda, type = "response"), test$rt, form = "traditional")
    
    new_R2 <- R2(predict(eln_model, test_x_zero, alpha = alpha, lambda = lambda, type = "response"), test$rt, form = "traditional")
    
    variable_importance <- data.frame(variable = all_factor_names[i], importance = (original_R2 - new_R2))
    
    variable_importance
  }
  variable_importance_df
}

# Random Forest

RF_variable_importance <- function(test, rf_model,
                                   macro_factor_names, individual_factor_names) {
  all_factor_names <- c(macro_factor_names, individual_factor_names)[-1]
  test_x <- test[4:ncol(test)]
  
  variable_importance_df <- foreach(i = (1:(length(all_factor_names))), .combine = "rbind", .packages = "randomForestSRC") %do% {
    test_x_zero <- test_x
    
    zero_indices <- which(str_detect(colnames(test_x_zero), all_factor_names[i]) == 1)
    test_x_zero[, zero_indices] <- 0
    
    original_R2 <- R2(predict(rf_model, newdata = test_x)$predicted, test$rt, form = "traditional")
    
    new_R2 <- R2(predict(rf_model, newdata = test_x_zero)$predicted, test$rt, form = "traditional")
    
    variable_importance <- data.frame(variable = all_factor_names[i], importance = (original_R2 - new_R2))
                                      
    variable_importance
  }
variable_importance_df
}

# Neural Network
# Unfortunately it seems that the tensorflow/keras backend does not work with foreach, so a standard for loop is needed instead

NNet_variable_importance <- function(test, nnet_model,
                                     macro_factor_names, individual_factor_names) {
  all_factor_names <- c(macro_factor_names, individual_factor_names)[-1]
  test_x <- test[4:ncol(test)]

  variable_importance_df <- foreach(i = (1:(length(all_factor_names))), .combine = "rbind", 
                                    .packages = c("keras", "tensorflow", "reticulate")) %do% {
    test_x_zero <- test_x
    
    zero_indices <- which(str_detect(colnames(test_x_zero), all_factor_names[i]) == 1)
    test_x_zero[, zero_indices] <- 0

    original_R2 <- R2(predict(nnet_model, as.matrix(test_x)), test$rt, form = "traditional")

    new_R2 <- R2(predict(nnet_model, as.matrix(test_x_zero)), test$rt, form = "traditional")

    variable_importance <- data.frame(variable = all_factor_names[i], importance = (original_R2 - new_R2))

    variable_importance
  }
variable_importance_df
}
```

```{r ave_forecast_resid_function}
# Forecasting Variable Importance Metrics

## Linear Model
# Given a an lm object and a test dataset, return the cross sectional average forecast errors

lm_ave_forecast_resids <- function(lm_model, test) {
  
  time_periods <- unique(test$time)
  
  # Set .combine to "c" to return a vector of results, which is what we want
  ave_forecast_resids_vector <- foreach(t = 1:length(time_periods), .combine = "c", .packages = c("speedglm", "quantreg")) %dopar% {
    
    test_cross_section <- test %>%
      filter(time == time_periods[t])
    
    residuals <- test_cross_section$rt - predict(lm_model, newdata = test_cross_section)
    
    mean(residuals)
  }
  ave_forecast_resids_vector
}

## Elastic Net

eln_ave_forecast_resids <- function(eln_model, test, alpha, lambda) {
  
  time_periods <- unique(test$time)
  
  # Set .combine to "c" to return a vector of results, which is what we want
  ave_forecast_resids_vector <- foreach(t = 1:length(time_periods), .combine = "c") %dopar% {
    
    test_cross_section <- test %>%
      filter(time == time_periods[t])

    test_cross_section_x <- as.matrix(test_cross_section[4:ncol(test_cross_section)])
    
    residuals <- test_cross_section$rt - predict(eln_model, test_cross_section_x,
                                                 alpha = alpha, lambda = lambda)
    
    mean(residuals)
  }
  ave_forecast_resids_vector
}

eln_ave_forecast_resids <- function(eln_model, test, alpha, lambda) {
  0
}

## Random Forest

rf_ave_forecast_resids <- function(rf_model, test) {
  
  time_periods <- unique(test$time)
  
  # Set .combine to "c" to return a vector of results, which is what we want
  ave_forecast_resids_vector <- foreach(t = 1:length(time_periods), .combine = "c") %do% {
    
    test_cross_section <- test %>%
      filter(time == time_periods[t])
    
    residuals <- test_cross_section$rt - predict(rf_model, newdata = test_cross_section)$predicted
    
    mean(residuals)
  }
  ave_forecast_resids_vector
}

## Neural Networks

nnet_ave_forecast_resids <- function(nnet_model, test) {
  
  time_periods <- unique(test$time)
  
  # Set .combine to "c" to return a vector of results, which is what we want
  ave_forecast_resids_vector <- foreach(t = 1:length(time_periods), .combine = "c") %do% {
    
    test_cross_section <- test %>%
      filter(time == time_periods[t])
    
    test_cross_section_x <- test_cross_section[4:ncol(test_cross_section)]
    
    residuals <- test_cross_section$rt - (nnet_model %>% predict(as.matrix(test_cross_section_x)))
    
    mean(residuals)
  }
  ave_forecast_resids_vector
}
```

```{r fit_functions}
# Mostly the same as the simulation ones, but slightly different due to different variable importance schemes

##############################################################################################################
LM_fit <- function(pooled_panel, timeSlices, loss_function, f, 
                   macro_factor_names, individual_factor_names) {
  #Initialize Loss Function Statistics
  LM_stats <- rep(list(0), 3)
  
  for (set in 1:3) {
    LM_stats[[set]] <- list(loss_stats = data.frame(train_MAE = 0, train_MSE = 0, train_RMSE = 0, train_RSquare = 0, 
                                                    validation_MAE = 0, validation_MSE = 0, validation_RMSE = 0, validation_RSquare = 0, 
                                                    test_MAE = 0, test_MSE = 0, test_RMSE = 0, test_RSquare = 0),
                            #Other useful things
                            forecast_resids = 0,
                            model = 0,
                            #Variable Importance
                            variable_importance = 0)
    
    #Load Training, validation and test sets
    train <- pooled_panel %>%
      filter(time %in% timeSlices[[set]]$train)
    validation <- pooled_panel %>%
      filter(time %in% timeSlices[[set]]$validation)
    test <- pooled_panel %>%
      filter(time %in% timeSlices[[set]]$test)
    
    #Train Model on training set
    
    # Set model = FALSE so that it won't save a copy of the training data in the model object
    # Doen for memory efficiency
    #MSE case
    if (loss_function == "mse") {
      lm <- lm(f, data = train, model = FALSE, x = FALSE, y = FALSE)
    } else {
      # Use pfn as method here for much faster computation
      lm <- rq(f, data = train, tau = 0.5, method = "pfn", eps = 1e-03)
    }
    
    #LM_stats[[set]]$model <- lm
    
    #No Tuning Needed
    
    #Statistics
    #Training Set
    train_predict <- predict(lm, newdata = train)
    LM_stats[[set]]$loss_stats$train_MAE <- mae(train$rt, train_predict)
    LM_stats[[set]]$loss_stats$train_MSE <- mse(train$rt, train_predict)
    LM_stats[[set]]$loss_stats$train_RMSE <- rmse(train$rt, train_predict)
    LM_stats[[set]]$loss_stats$train_RSquare <- R2(train_predict, train$rt, form = "traditional")
    
    #Validation Set Statistics
    validation_predict <- predict(lm, newdata = validation)
    LM_stats[[set]]$loss_stats$validation_MAE <- mae(validation$rt, validation_predict)
    LM_stats[[set]]$loss_stats$validation_MSE <- mse(validation$rt, validation_predict)
    LM_stats[[set]]$loss_stats$validation_RMSE <- rmse(validation$rt, validation_predict)
    LM_stats[[set]]$loss_stats$validation_RSquare <- R2(validation_predict, validation$rt, form = "traditional")
      
    #Test Set Statistics
    test_predict <- predict(lm, newdata = test)
    LM_stats[[set]]$loss_stats$test_MAE <- mae(test$rt, test_predict)
    LM_stats[[set]]$loss_stats$test_MSE <- mse(test$rt, test_predict)
    LM_stats[[set]]$loss_stats$test_RMSE <- rmse(test$rt, test_predict)
    LM_stats[[set]]$loss_stats$test_RSquare <- R2(test_predict, test$rt, form = "traditional")

    #Forecast Residuals
    LM_stats[[set]]$forecast_resids <- lm_ave_forecast_resids(lm_model = lm, test = test)
    #Variable Importance
    LM_stats[[set]]$variable_importance <- LM_variable_importance(test, lm,
                                                                  macro_factor_names, individual_factor_names)
  }
  return(LM_stats)
}
###################################################################################################################

ELN_model_grid <- function(alpha_grid, train_x, train_y, validation_x, validation_y, loss_function, nlamb) {
  
  ELN_model_grid_list <- foreach(i = (1:length(alpha_grid))) %dopar% {
    ELN_model_grid <- list(ELN_grid = 0, model = 0)
    
    #MSE Case
    if (loss_function == "mse") {
      ELN <- hqreg(train_x, train_y, method = "ls", alpha = alpha_grid[i], nlambda = nlamb)
      ELN_model_grid$model <- ELN
    
      ELN_model_grid$ELN_grid <- cbind(alpha = alpha_grid[i], lambda = ELN$lambda,
                                       train_loss = apply(predict(ELN, train_x), 2, mse, actual = train_y), 
                                       validation_loss = apply(predict(ELN, validation_x), 2, mse, actual = validation_y))
    } else {
      #MAE Case
      ELN <- hqreg(train_x, train_y, method = "quantile", tau = 0.5, alpha = alpha_grid[i], nlambda = nlamb,
                   eps = 1e-4, max.iter = 1000)
      ELN_model_grid$model <- ELN
    
      ELN_model_grid$ELN_grid <- cbind(alpha = alpha_grid[i], lambda = ELN$lambda,
                                       train_loss = apply(predict(ELN, train_x), 2, mse, actual = train_y), 
                                       validation_loss = apply(predict(ELN, validation_x), 2, mae, actual = validation_y))
    }
    ELN_model_grid
  }
  ELN_model_grid_list
}

# Given a list containing the model grid, return the "best" model according to validation loss
# Use in conjunction with previous function

get_ELN_best_tune <- function(model_grid) {
  ELN_tune_dataframe <- cbind(model_grid[[1]]$ELN_grid, list_index = 1)
  for (i in 2:length(model_grid)) {
    ELN_tune_dataframe <- rbind(ELN_tune_dataframe, cbind(model_grid[[i]]$ELN_grid, list_index = i))
  }
  ELN_tune_dataframe <- data.frame(ELN_tune_dataframe)
  return(ELN_tune_dataframe[which.min(ELN_tune_dataframe$validation_loss), ])
}

ELN_fit_stats <- function(alpha_grid, nlamb, timeSlices, pooled_panel, loss_function,
                          macro_factor_names, individual_factor_names) {
  ELN_stats <- rep(list(0), 3)
  
  for (set in 1:3) {
    #Load Training, validation and test sets
    train <- pooled_panel %>%
      filter(time %in% timeSlices[[set]]$train)
    validation <- pooled_panel %>%
      filter(time %in% timeSlices[[set]]$validation)
    test <- pooled_panel %>%
      filter(time %in% timeSlices[[set]]$test)
    
    train_x <- as.matrix(train[4:ncol(train)])
    train_y <- as.matrix(train$rt)
    validation_x <- as.matrix(validation[4:ncol(validation)])
    validation_y <- as.matrix(validation$rt)
    test_x <- as.matrix(test[4:ncol(test)])
    test_y <- as.matrix(test$rt)
      
    #Get models fit over the grid of hyperparameters
    ELN_model_grid <- ELN_model_grid(alpha_grid, train_x, train_y, validation_x, validation_y, loss_function, nlamb = nlamb)
        
    #Get the best tuning parameters
    best_model_params <- get_ELN_best_tune(ELN_model_grid)
    
    #Initialize stats list
    ELN_stats[[set]] <- list(loss_stats = data.frame(train_MAE = 0, train_MSE = 0, train_RMSE = 0, train_RSquare = 0, 
                                                     validation_MAE = 0, validation_MSE = 0, validation_RMSE = 0, validation_RSquare = 0, 
                                                     test_MAE = 0, test_MSE = 0, test_RMSE = 0, test_RSquare = 0),
                             #Other useful things
                             forecast_resids = 0,
                             model = 0,
                             hyperparameters = 0,
                             variable_importance = 0)
      
    #Model
    model <- ELN_model_grid[[best_model_params$list_index]]$model
    #ELN_stats[[set]]$model <- model
    
    #Hyperparameters
    ELN_stats[[set]]$hyperparameters <- best_model_params
      
    #Loss Stats Dataframe
    #Train
    train_predict <- predict(model, train_x, alpha = best_model_params$alpha, lambda = best_model_params$lambda)
    ELN_stats[[set]]$loss_stats$train_MAE <- mae(train_y, train_predict)
    ELN_stats[[set]]$loss_stats$train_MSE <- mse(train_y, train_predict)
    ELN_stats[[set]]$loss_stats$train_RMSE <- rmse(train_y, train_predict)
    ELN_stats[[set]]$loss_stats$train_RSquare <- (1 - sse(train_y, train_predict) / sum((train_y - mean(train_y))^2))
        
    #Validation
    valid_predict <- predict(model, validation_x, alpha = best_model_params$alpha, lambda = best_model_params$lambda)
    ELN_stats[[set]]$loss_stats$validation_MAE <- mae(validation_y, valid_predict)
    ELN_stats[[set]]$loss_stats$validation_MSE <- mse(validation_y, valid_predict)
    ELN_stats[[set]]$loss_stats$validation_RMSE <- rmse(validation_y, valid_predict)
    ELN_stats[[set]]$loss_stats$validation_RSquare <- (1 - sse(validation_y, valid_predict) / sum((validation_y - mean(validation_y))^2))
    
    #Test
    test_predict <- predict(model, test_x, alpha = best_model_params$alpha, lambda = best_model_params$lambda)
    ELN_stats[[set]]$loss_stats$test_MAE <- mae(test_y, test_predict)
    ELN_stats[[set]]$loss_stats$test_MSE <- mse(test_y, test_predict)
    ELN_stats[[set]]$loss_stats$test_RMSE <- rmse(test_y, test_predict)
    ELN_stats[[set]]$loss_stats$test_RSquare <- (1 - sse(test_y, test_predict) / sum((test_y - mean(test_y))^2))
    
    #Forecast residuals
    ELN_stats[[set]]$forecast_resids <- eln_ave_forecast_resids(eln_model = model, test, 
                                                                # Hyperparameters
                                                                alpha = best_model_params$alpha, 
                                                                lambda = best_model_params$lambda)
    
    #Variable Importance
    ELN_stats[[set]]$variable_importance <- ELN_variable_importance(test, model, 
                                                                    alpha = best_model_params$alpha, lambda = best_model_params$lambda,
                                                                    macro_factor_names, individual_factor_names)
  }
  return(ELN_stats)
}



################################################################################################################################

RF_fit_model_grid <- function(f, train, validation, RF_grid, loss_function) {
  #Initialize List
  RF_model_grid <- rep(list(0), nrow(RF_grid))
  
  for (i in 1:nrow(RF_grid)) {
    
    RF_model_grid[[i]] <- list(RF_grid = 0, model = 0)
    
    #MSE Case
    if (loss_function == "mse") {
      RF <- rfsrc(f, train, 
                  #Hyperparameters
                  ntree = RF_grid$ntree[i],
                  mtry = RF_grid$mtry[i],
                  nodesize = RF_grid$nodesize[i],
                  splitrule = "mse"
                  )
      
      #RF_model_grid[[i]]$model <- RF
      RF_model_grid[[i]]$RF_grid <- cbind(RF_grid[i, ],
                                          #Train Loss
                                          train_loss = mse(train$rt, predict(RF)$predicted),
                                          #Validation Loss
                                          validation_loss = mse(validation$rt, predict(RF, newdata = validation)$predicted)
                                          )
      
    } else {
      #MAE Case
      RF <- rfsrc(f, train, 
                  #Hyperparameters
                  ntree = RF_grid$ntree[i],
                  mtry = RF_grid$mtry[i],
                  nodesize = RF_grid$nodesize[i],
                  splitrule = "quantile.regr",
                  prob = 0.5
                  )
      
      #RF_model_grid[[i]]$model <- RF
      RF_model_grid[[i]]$RF_grid <- cbind(RF_grid[i, ],
                                          #Train Loss
                                          train_loss = mae(train$rt, predict(RF)$predicted),
                                          #Validation Loss
                                          validation_loss = mae(validation$rt, predict(RF, newdata = validation)$predicted)
                                          )
    }
  }
  return(RF_model_grid)
}

#Returns the dataframe row containing the "best" hyperparameters

get_RF_best_tune <- function(RF_model_grid) {
  RF_tune_grid <- RF_model_grid[[1]]$RF_grid
  for (i in 2:length(RF_model_grid)) {
    RF_tune_grid <- rbind(RF_tune_grid, RF_model_grid[[i]]$RF_grid)
  }
  return(RF_tune_grid[which.min(RF_tune_grid$validation_loss), ])
}

RF_fit_stats <- function(pooled_panel, RF_grid, timeSlices, loss_function, f,
                         macro_factor_names, individual_factor_names) {
  #Initialize
  RF_stats <- rep(list(0), 3)
  
  #Load training, validation and test sets
  
  for (set in 1:3) {
    
    RF_stats[[set]] <- list(loss_stats = data.frame(train_MAE = 0, train_MSE = 0, train_RMSE = 0, train_RSquare = 0, 
                                                    validation_MAE = 0, validation_MSE = 0, validation_RMSE = 0, validation_RSquare = 0, 
                                                    test_MAE = 0, test_MSE = 0, test_RMSE = 0, test_RSquare = 0),
                            #Other useful things
                            forecast_resids = 0,
                            model = 0,
                            hyperparameters = 0,
                            variable_importance = 0, vimp = 0)
    
    train <- pooled_panel %>%
      filter(time %in% timeSlices[[set]]$train)
    validation <- pooled_panel %>%
      filter(time %in% timeSlices[[set]]$validation)
    test <- pooled_panel %>%
      filter(time %in% timeSlices[[set]]$test)
    
    #Fit on training Set over grid of hyperparameters
    
    model_grid <- RF_fit_model_grid(f, train, validation, RF_grid, loss_function)
    
    #Get the best hyperparameters
    
    best_model_params <- get_RF_best_tune(model_grid)
    RF_stats[[set]]$hyperparameters <- best_model_params
    
    #Compute the optimal model
    
    if (loss_function == "mse") {
      model <- rfsrc(f, train, 
                     #Hyperparameters
                     ntree = best_model_params$ntree,
                     mtry = best_model_params$mtry,
                     nodesize = best_model_params$nodesize,
                     splitrule = "mse",
                     bootstrap = "by.root", samptype = "swr",
                     forest = TRUE
                     )
    } else {
      model <- rfsrc(f, train, 
                     #Hyperparameters
                     ntree = best_model_params$ntree,
                     mtry = best_model_params$mtry,
                     nodesize = best_model_params$nodesize,
                     splitrule = "quantile.regr",
                     prob = 0.5,
                     bootstrap = "by.root", samptype = "swr", 
                     forest = TRUE
                     )
    }
    
    #RF_stats[[set]]$model <- model
    
    #Train
    train_predict <- predict(model, train)$predicted
    RF_stats[[set]]$loss_stats$train_MAE <- mae(train$rt, train_predict)
    RF_stats[[set]]$loss_stats$train_MSE <- mse(train$rt, train_predict)
    RF_stats[[set]]$loss_stats$train_RMSE <- rmse(train$rt, train_predict)
    RF_stats[[set]]$loss_stats$train_RSquare <- (1 - sse(train$rt, train_predict) / sum((train$rt - mean(train$rt))^2))
        
    #Validation
    valid_predict <- predict(model, newdata = validation)$predicted
    RF_stats[[set]]$loss_stats$validation_MAE <- mae(validation$rt, valid_predict)
    RF_stats[[set]]$loss_stats$validation_MSE <- mse(validation$rt, valid_predict)
    RF_stats[[set]]$loss_stats$validation_RMSE <- rmse(validation$rt, valid_predict)
    RF_stats[[set]]$loss_stats$validation_RSquare <- (1 - sse(validation$rt, valid_predict) / sum((validation$rt - mean(validation$rt))^2))
    
    #Test
    test_predict <- predict(model, newdata = test)$predicted
    RF_stats[[set]]$loss_stats$test_MAE <- mae(test$rt, test_predict)
    RF_stats[[set]]$loss_stats$test_MSE <- mse(test$rt, test_predict)
    RF_stats[[set]]$loss_stats$test_RMSE <- rmse(test$rt, test_predict)
    RF_stats[[set]]$loss_stats$test_RSquare <- (1 - sse(test$rt, test_predict) / sum((test$rt - mean(test$rt))^2))
    
    #Forecast residuals
    RF_stats[[set]]$forecast_resids <- rf_ave_forecast_resids(rf_model = model, test = test)
    
    #Variable Importance 
    RF_stats[[set]]$variable_importance <- RF_variable_importance(test, model,
                                                                  macro_factor_names, individual_factor_names)
    
    ## vimp
    RF_stats[[set]]$vimp <- list(breiman_cutler = vimp(model, importance = "permute", 
                                                       block.size = 1,
                                                       newdata = test)$importance,
                                 ishwaran_kogalur = vimp(model, importance = "permute", 
                                                         block.size = best_model_params$ntree,
                                                         newdata = test)$importance)
  }
  return(RF_stats)
}

#########################################################################################################################

NNet_fit_stats <- function(pooled_panel, timeSlices, hidden_layers, loss_function, batch_size, patience,
                           macro_factor_names, individual_factor_names) {
  #Initialize
  
  NNet_stats <- rep(list(0), 3)
  
  for (set in 1:3) {
    NNet_stats[[set]] <- list(loss_stats = data.frame(train_MAE = 0, train_MSE = 0, train_RMSE = 0, train_RSquare = 0, 
                                                      validation_MAE = 0, validation_MSE = 0, validation_RMSE = 0, validation_RSquare = 0, 
                                                      test_MAE = 0, test_MSE = 0, test_RMSE = 0, test_RSquare = 0),
                              #Other useful things
                              # Keep forecasts here in for nnet objects to make sure they aren't doing something stupid
                              forecasts = 0,
                              forecast_resids = 0,
                              model = 0,
                              variable_importance = 0)
      
    #Load Training, validation and test sets
    train <- pooled_panel %>%
      filter(time %in% timeSlices[[set]]$train)
    validation <- pooled_panel %>%
      filter(time %in% timeSlices[[set]]$validation)
    test<- pooled_panel %>%
      filter(time %in% timeSlices[[set]]$test)
      
    train_x <- train[4:ncol(train)]
    train_x <- scale(train_x)
    col_means_train <- attr(train_x, "scaled:center") 
    col_stddevs_train <- attr(train_x, "scaled:scale")
    train_y <- train$rt
    
    validation_x <- validation[4:ncol(validation)]
    validation_x <- scale(validation_x, center = col_means_train, scale = col_stddevs_train)
    validation_y <- validation$rt
      
    test_x <- test[4:ncol(test)]
    test_x <- scale(test_x, center = col_means_train, scale = col_stddevs_train)
    test_y <- test$rt
      
    # Fit the model
    # The patience parameter is the amount of epochs to check for improvement.
    # Gu et al don't say what their early stopping parameter p is
    early_stop <- callback_early_stopping(monitor = "val_loss", patience = patience, restore_best_weights = TRUE)
    lr_reduce <- callback_reduce_lr_on_plateau(monitor = "val_loss", factor = 0.5,
                                               patience = 5, verbose = 0, mode = c("auto", "min", "max"),
                                               min_delta = 1e-04, cooldown = 0, min_lr = 0)
    
    print_dot_callback <- callback_lambda(
      on_epoch_end = function(epoch, logs) {
        if (epoch %% 50 == 0) cat("\n")
        cat(".")
      }
    ) 
    
    l1_penalty <- 0.05
    
    build_NN <- function(hidden_layers, loss_function) {
        
      if (hidden_layers == 1) {
        
        model <- keras_model_sequential() %>%
        
        # Layer 1
        layer_dense(units = 32, input_shape = ncol(train_x),
                    bias_initializer = initializer_zeros(),
                    kernel_initializer = initializer_truncated_normal(mean = 0, stddev = 0.05, seed = NULL)) %>%
          layer_activation(activation = "tanh") %>%
          layer_activity_regularization(l1 = l1_penalty) %>%
          layer_batch_normalization() %>%
        # Output Layer
          layer_dense(units = 1, activation = "linear")
        
      } else if (hidden_layers == 2) {
        
        model <- keras_model_sequential() %>%
          
        # Layer 1
        layer_dense(units = 32, input_shape = ncol(train_x),
                    bias_initializer = initializer_zeros(),
                    kernel_initializer = initializer_truncated_normal(mean = 0, stddev = 0.05, seed = NULL)) %>%
          layer_activation(activation = "tanh") %>%
          layer_activity_regularization(l1 = l1_penalty) %>%
          layer_batch_normalization() %>%
        # Layer 2
        layer_dense(units = 16,
                    bias_initializer = initializer_zeros(),
                    kernel_initializer = initializer_truncated_normal(mean = 0, stddev = 0.05, seed = NULL)) %>%
          layer_activation(activation = "tanh") %>%
          layer_activity_regularization(l1 = l1_penalty) %>%
          layer_batch_normalization() %>%
        # Output Layer
        layer_dense(units = 1, activation = "linear")
          
      } else if (hidden_layers == 3) {
          
        model <- keras_model_sequential() %>%
          
        # Layer 1
        layer_dense(units = 32, input_shape = ncol(train_x),
                    bias_initializer = initializer_zeros(),
                    kernel_initializer = initializer_truncated_normal(mean = 0, stddev = 0.05, seed = NULL)) %>%
          layer_activation(activation = "tanh") %>%
          layer_activity_regularization(l1 = l1_penalty) %>%
          layer_batch_normalization() %>%
        # Layer 2
        layer_dense(units = 16,
                    bias_initializer = initializer_zeros(),
                    kernel_initializer = initializer_truncated_normal(mean = 0, stddev = 0.05, seed = NULL)) %>%
          layer_activation(activation = "tanh") %>%
          layer_activity_regularization(l1 = l1_penalty) %>%
          layer_batch_normalization() %>%
        #Layer 3
        layer_dense(units = 8,
                    bias_initializer = initializer_zeros(),
                    kernel_initializer = initializer_truncated_normal(mean = 0, stddev = 0.05, seed = NULL)) %>%
          layer_activation(activation = "tanh") %>%
          layer_activity_regularization(l1 = l1_penalty) %>%
          layer_batch_normalization() %>%
        # Output Layer
        layer_dense(units = 1, activation = "linear")
          
      } else if (hidden_layers == 4) {
          
        model <- keras_model_sequential() %>%
        
        # Layer 1
        layer_dense(units = 32, input_shape = ncol(train_x),
                    bias_initializer = initializer_zeros(),
                    kernel_initializer = initializer_truncated_normal(mean = 0, stddev = 0.05, seed = NULL)) %>%
          layer_activation(activation = "tanh") %>%
          layer_activity_regularization(l1 = l1_penalty) %>%
          layer_batch_normalization() %>%
        # Layer 2
        layer_dense(units = 16,
                    bias_initializer = initializer_zeros(),
                    kernel_initializer = initializer_truncated_normal(mean = 0, stddev = 0.05, seed = NULL)) %>%
          layer_activation(activation = "tanh") %>%
          layer_activity_regularization(l1 = l1_penalty) %>%
          layer_batch_normalization() %>%
        #Layer 3
        layer_dense(units = 8,
                    bias_initializer = initializer_zeros(),
                    kernel_initializer = initializer_truncated_normal(mean = 0, stddev = 0.05, seed = NULL)) %>%
          layer_activation(activation = "tanh") %>%
          layer_activity_regularization(l1 = l1_penalty) %>%
          layer_batch_normalization() %>%
        #Layer 4
        layer_dense(units = 4,
                    bias_initializer = initializer_zeros(),
                    kernel_initializer = initializer_truncated_normal(mean = 0, stddev = 0.05, seed = NULL)) %>%
          layer_activation(activation = "tanh") %>%
          layer_activity_regularization(l1 = l1_penalty) %>%
          layer_batch_normalization() %>%
        # Output Layer
        layer_dense(units = 1, activation = "linear")
          
      } else {
        
        model <- keras_model_sequential() %>%
        
        # Layer 1
        layer_dense(units = 32, input_shape = ncol(train_x),
                    bias_initializer = initializer_zeros(),
                    kernel_initializer = initializer_truncated_normal(mean = 0, stddev = 0.05, seed = NULL)) %>%
          layer_activation(activation = "tanh") %>%
          layer_activity_regularization(l1 = l1_penalty) %>%
          layer_batch_normalization() %>%
        # Layer 2
        layer_dense(units = 16,
                    bias_initializer = initializer_zeros(),
                    kernel_initializer = initializer_truncated_normal(mean = 0, stddev = 0.05, seed = NULL)) %>%
          layer_activation(activation = "tanh") %>%
          layer_activity_regularization(l1 = l1_penalty) %>%
          layer_batch_normalization() %>%
        #Layer 3
        layer_dense(units = 8,
                    bias_initializer = initializer_zeros(),
                    kernel_initializer = initializer_truncated_normal(mean = 0, stddev = 0.05, seed = NULL)) %>%
          layer_activation(activation = "tanh") %>%
          layer_activity_regularization(l1 = l1_penalty) %>%
          layer_batch_normalization() %>%
        #Layer 4
        layer_dense(units = 4,
                    bias_initializer = initializer_zeros(),
                    kernel_initializer = initializer_truncated_normal(mean = 0, stddev = 0.05, seed = NULL)) %>%
          layer_activation(activation = "tanh") %>%
          layer_activity_regularization(l1 = l1_penalty) %>%
          layer_batch_normalization() %>%
        #Layer 5
        layer_dense(units = 2,
                    bias_initializer = initializer_zeros(),
                    kernel_initializer = initializer_truncated_normal(mean = 0, stddev = 0.05, seed = NULL)) %>%
          layer_activation(activation = "tanh") %>%
          layer_activity_regularization(l1 = l1_penalty) %>%
          layer_batch_normalization() %>%
        # Output Layer
        layer_dense(units = 1, activation = "linear")
          
      }
      model %>% compile(
        loss = loss_function,
        optimizer = optimizer_adam(lr = 0.001, beta_1 = 0.9, beta_2 = 0.999,
                                   epsilon = NULL, decay = 0, amsgrad = FALSE, clipnorm = NULL,
                                   clipvalue = NULL),
        metrics = list("mae", "mse")
      )
      model
    }
      
    neural_network <- build_NN(hidden_layers, loss_function)
    
    # Other options used throughout the neural network fitting process are specified here
    # Namely, batch size
    # In general, larger batch sizes result in faster progress in training, but don't always converge as fast. Smaller batch sizes train slower, but can converge faster.
    # Default batch size is 32
    neural_network %>% fit(as.matrix(train_x), as.matrix(train_y), 
                           batch_size = batch_size, epochs = 500, verbose = 1, 
                           validation_data = list(as.matrix(validation_x), as.matrix(validation_y)),
                           callbacks = list(early_stop, print_dot_callback))
    
    # Model
    # NNet_stats[[set]]$model <- neural_network
    
    #Train
    train_predict <- neural_network %>% predict(as.matrix(train_x))
    
    NNet_stats[[set]]$loss_stats$train_MAE <- mae(train$rt, train_predict)
    NNet_stats[[set]]$loss_stats$train_MSE <- mse(train$rt, train_predict)
    NNet_stats[[set]]$loss_stats$train_RMSE <- rmse(train$rt, train_predict)
    NNet_stats[[set]]$loss_stats$train_RSquare <- R2(train_predict, train$rt, form = "traditional")
          
    #Validation
    validation_predict <- neural_network %>% predict(as.matrix(validation_x))
    
    NNet_stats[[set]]$loss_stats$validation_MAE <- mae(validation$rt, validation_predict)
    NNet_stats[[set]]$loss_stats$validation_MSE <- mse(validation$rt, validation_predict)
    NNet_stats[[set]]$loss_stats$validation_RMSE <- rmse(validation$rt, validation_predict)
    NNet_stats[[set]]$loss_stats$validation_RSquare <- R2(validation_predict, validation$rt, form = "traditional")
      
    #Test
    test_predict <- neural_network %>% predict(as.matrix(test_x))
    
    NNet_stats[[set]]$loss_stats$test_MAE <- mae(test$rt, test_predict)
    NNet_stats[[set]]$loss_stats$test_MSE <- mse(test$rt, test_predict)
    NNet_stats[[set]]$loss_stats$test_RMSE <- rmse(test$rt, test_predict)
    NNet_stats[[set]]$loss_stats$test_RSquare <- R2(test_predict, test$rt, form = "traditional")
        
    #Forecasts
    NNet_stats[[set]]$forecasts <- test_predict
      
    #Forecast residuals
    NNet_stats[[set]]$forecast_resids <- nnet_ave_forecast_resids(nnet_model = neural_network, test = test)
    
    #Variable Importance
    NNet_stats[[set]]$variable_importance <- NNet_variable_importance(test, neural_network,
                                                                      macro_factor_names, individual_factor_names)
  }
  # clear Keras session
  k_clear_session()
  NNet_stats
}

###################################################################################################################
```

```{r, eval = FALSE}
# Test on fitting actual data
# Lots of memory issues, need to change fit functions so that they are more memory efficient

# Line which changes doFuture max memory limits (1.5GB currently)
options(future.globals.maxSize = 1.5e+9)
real_data_results <- fit_all_models_real_data(final_dataset,
                                              LM = 1, ELN = 0, RF = 0, NNet = 0)
```

```{r, eval = FALSE}
f <- real_panel_formula(final_dataset)
LM_stats_mse <- LM_fit(final_dataset, realdata_timeSlices, "mse", f,
                       macro_factor_names, individual_factor_names)
saveRDS(LM_stats_mse, file = "~/GitHub/Evaluation-of-Machine-Learning-in-Asset-Pricing/R/Model_results/empirical_train_valid_2/LM_stats_mse.rds")

LM_stats_mae <- LM_fit(final_dataset, realdata_timeSlices, "mae", f,
                       macro_factor_names, individual_factor_names)
saveRDS(LM_stats_mae, file = "~/GitHub/Evaluation-of-Machine-Learning-in-Asset-Pricing/R/Model_results/empirical_train_valid_2/LM_stats_mae.rds")

####

LM_stats_mse[[1]]$loss_stats
LM_stats_mse[[1]]$variable_importance %>%
  arrange(desc(importance))

LM_stats_mse[[2]]$loss_stats
LM_stats_mse[[2]]$variable_importance %>%
  arrange(desc(importance))

LM_stats_mse[[3]]$loss_stats
LM_stats_mse[[3]]$variable_importance %>%
  arrange(desc(importance))
######################################################################################################################
ELN_stats_mse <- ELN_fit_stats(alpha_grid = seq(0, 1, 0.02), nlamb = 50, 
                               realdata_timeSlices, final_dataset, "mse",
                               macro_factor_names, individual_factor_names)
saveRDS(ELN_stats_mse, file = "~/GitHub/Evaluation-of-Machine-Learning-in-Asset-Pricing/R/Model_results/empirical_train_valid_2/ELN_stats_mse.rds")
ELN_stats_mae <- ELN_fit_stats(alpha_grid = seq(0, 1, 0.02), nlamb = 50, 
                               realdata_timeSlices, final_dataset, "mae",
                               macro_factor_names, individual_factor_names)
saveRDS(ELN_stats_mae, file = "~/GitHub/Evaluation-of-Machine-Learning-in-Asset-Pricing/R/Model_results/empirical_train_valid_2/ELN_stats_mae.rds")

ELN_stats_mse[[1]]$loss_stats
ELN_stats_mse[[1]]$hyperparameters
ELN_stats_mse[[1]]$variable_importance %>%
  arrange(desc(importance))

ELN_stats_mse[[2]]$loss_stats
ELN_stats_mse[[2]]$hyperparameters
ELN_stats_mse[[2]]$variable_importance %>%
  arrange(desc(importance))

ELN_stats_mse[[3]]$loss_stats
ELN_stats_mse[[3]]$hyperparameters
ELN_stats_mse[[3]]$variable_importance %>%
  arrange(desc(importance))

ELN_stats_mae[[1]]$loss_stats
ELN_stats_mae[[1]]$hyperparameters
ELN_stats_mae[[1]]$variable_importance %>%
  arrange(desc(importance))

ELN_stats_mae[[2]]$loss_stats
ELN_stats_mae[[2]]$hyperparameters
ELN_stats_mae[[2]]$variable_importance %>%
  arrange(desc(importance))

ELN_stats_mae[[3]]$loss_stats
ELN_stats_mae[[3]]$hyperparameters
ELN_stats_mae[[3]]$variable_importance %>%
  arrange(desc(importance))

######################################################################################################################

# Random Forests
RF_grid <- expand.grid(
  #ntree usually isn't tuned. Just set to max of computationally feasible
  ntree = 50,
  mtry = seq(20, round(ncol(final_dataset[4:ncol(final_dataset)])/4), 20), 
  nodesize = seq(6, 15, 3)
  # nodedepth recommended not to be changed
  #nodedepth = 1
  )

RF_mse_stats <- RF_fit_stats(final_dataset, RF_grid, realdata_timeSlices, "mse", f,
                             macro_factor_names, individual_factor_names)
saveRDS(RF_mse_stats, file = "~/GitHub/Evaluation-of-Machine-Learning-in-Asset-Pricing/R/Model_results/empirical_train_valid_2/RF_mse_stats_1.5_train_valid.rds")
RF_mae_stats <- RF_fit_stats(final_dataset, RF_grid, realdata_timeSlices, "mae", f,
                             macro_factor_names, individual_factor_names)
saveRDS(RF_mae_stats, file = "~/GitHub/Evaluation-of-Machine-Learning-in-Asset-Pricing/R/Model_results/empirical_train_valid_2/RF_mae_stats_1.5_train_valid.rds")

RF_mse_stats[[1]]$loss_stats
RF_mse_stats[[1]]$hyperparameters
RF_mse_stats[[1]]$variable_importance %>%
  arrange(desc(importance))

RF_mae_stats[[1]]$loss_stats
RF_mae_stats[[1]]$hyperparameters
RF_mae_stats[[1]]$variable_importance %>%
  arrange(desc(importance))
```

```{r, eval = FALSE}

######################################################################################################################

NNet_1_mse_stats <- NNet_fit_stats(final_dataset, realdata_timeSlices, 1, "mse", batch_size = 512, 20,
                                   macro_factor_names, individual_factor_names)
NNet_1_mae_stats <- NNet_fit_stats(final_dataset, realdata_timeSlices, 1, "mae", batch_size = 256, 20,
                                   macro_factor_names, individual_factor_names)
saveRDS(NNet_1_mse_stats, file = "~/GitHub/Evaluation-of-Machine-Learning-in-Asset-Pricing/R/Model_results/empirical_train_valid_2/NNet_1_mse_stats_1.5.rds")
saveRDS(NNet_1_mae_stats, file = "~/GitHub/Evaluation-of-Machine-Learning-in-Asset-Pricing/R/Model_results/empirical_train_valid_2/NNet_1_mae_stats_1.5.rds")

NNet_1_mse_stats[[1]]$loss_stats
NNet_1_mse_stats[[1]]$forecasts
NNet_1_mse_stats[[1]]$variable_importance %>%
  arrange(desc(importance))

NNet_1_mae_stats[[1]]$loss_stats
NNet_1_mae_stats[[1]]$forecasts
NNet_1_mae_stats[[1]]$variable_importance %>%
  arrange(desc(importance))



NNet_2_mse_stats <- NNet_fit_stats(final_dataset, realdata_timeSlices, 2, "mse", batch_size = 256, 20,
                                   macro_factor_names, individual_factor_names)
NNet_2_mae_stats <- NNet_fit_stats(final_dataset, realdata_timeSlices, 2, "mae", batch_size = 256, 20,
                                   macro_factor_names, individual_factor_names)
saveRDS(NNet_2_mse_stats, file = "~/GitHub/Evaluation-of-Machine-Learning-in-Asset-Pricing/R/Model_results/empirical_train_valid_2/NNet_2_mse_stats_1.5.rds")
saveRDS(NNet_2_mae_stats, file = "~/GitHub/Evaluation-of-Machine-Learning-in-Asset-Pricing/R/Model_results/empirical_train_valid_2/NNet_2_mae_stats_1.5.rds")

NNet_2_mse_stats[[1]]$loss_stats
NNet_2_mse_stats[[1]]$forecasts
NNet_2_mse_stats[[1]]$variable_importance %>%
  arrange(desc(importance))

NNet_2_mae_stats[[1]]$loss_stats
NNet_2_mae_stats[[1]]$forecasts
NNet_2_mae_stats[[1]]$variable_importance %>%
  arrange(desc(importance))

NNet_3_mse_stats <- NNet_fit_stats(final_dataset, realdata_timeSlices, 3, "mse", batch_size = 256, 20,
                                   macro_factor_names, individual_factor_names)
NNet_3_mae_stats <- NNet_fit_stats(final_dataset, realdata_timeSlices, 3, "mae", batch_size = 256, 20,
                                   macro_factor_names, individual_factor_names)
saveRDS(NNet_3_mse_stats, file = "~/GitHub/Evaluation-of-Machine-Learning-in-Asset-Pricing/R/Model_results/empirical_train_valid_2/NNet_3_mse_stats_1.5.rds")
saveRDS(NNet_3_mae_stats, file = "~/GitHub/Evaluation-of-Machine-Learning-in-Asset-Pricing/R/Model_results/empirical_train_valid_2/NNet_3_mae_stats_1.5.rds")

NNet_3_mse_stats[[1]]$loss_stats
NNet_3_mse_stats[[1]]$forecasts
NNet_3_mse_stats[[1]]$variable_importance %>%
  arrange(desc(importance))

NNet_3_mae_stats[[1]]$loss_stats
NNet_3_mae_stats[[1]]$forecasts
NNet_3_mae_stats[[1]]$variable_importance %>%
  arrange(desc(importance))

NNet_4_mse_stats <- NNet_fit_stats(final_dataset, realdata_timeSlices, 4, "mse", batch_size = 256, 20,
                                   macro_factor_names, individual_factor_names)
NNet_4_mae_stats <- NNet_fit_stats(final_dataset, realdata_timeSlices, 4, "mae", batch_size = 256, 20,
                                   macro_factor_names, individual_factor_names)
saveRDS(NNet_4_mse_stats, file = "~/GitHub/Evaluation-of-Machine-Learning-in-Asset-Pricing/R/Model_results/empirical_train_valid_2/NNet_4_mse_stats_1.5.rds")
saveRDS(NNet_4_mae_stats, file = "~/GitHub/Evaluation-of-Machine-Learning-in-Asset-Pricing/R/Model_results/empirical_train_valid_2/NNet_4_mae_stats_1.5.rds")

NNet_4_mse_stats[[1]]$loss_stats
NNet_4_mse_stats[[1]]$forecasts
NNet_4_mse_stats[[1]]$variable_importance %>%
  arrange(desc(importance))

NNet_4_mae_stats[[1]]$loss_stats
NNet_4_mae_stats[[1]]$forecasts
NNet_4_mae_stats[[1]]$variable_importance %>%
  arrange(desc(importance))

NNet_5_mse_stats <- NNet_fit_stats(final_dataset, realdata_timeSlices, 5, "mse", batch_size = 256, 20,
                                   macro_factor_names, individual_factor_names)
NNet_5_mae_stats <- NNet_fit_stats(final_dataset, realdata_timeSlices, 5, "mae", batch_size = 256, 20,
                                   macro_factor_names, individual_factor_names)
saveRDS(NNet_5_mse_stats, file = "~/GitHub/Evaluation-of-Machine-Learning-in-Asset-Pricing/R/Model_results/empirical_train_valid_2/NNet_5_mse_stats_1.5.rds")
saveRDS(NNet_5_mae_stats, file = "~/GitHub/Evaluation-of-Machine-Learning-in-Asset-Pricing/R/Model_results/empirical_train_valid_2/NNet_5_mae_stats_1.5.rds")

NNet_5_mse_stats[[1]]$loss_stats
NNet_5_mse_stats[[1]]$forecasts
NNet_5_mse_stats[[1]]$variable_importance %>%
  arrange(desc(importance))

NNet_5_mae_stats[[1]]$loss_stats
NNet_5_mae_stats[[1]]$forecasts
NNet_5_mae_stats[[1]]$variable_importance %>%
  arrange(desc(importance))
```
