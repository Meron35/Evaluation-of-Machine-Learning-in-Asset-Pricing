layer_batch_normalization() %>%
# Output Layer
layer_dense(units = 1, activation = "linear")
}
model %>% compile(
loss = loss_function,
optimizer = optimizer_adam(lr = 0.001, beta_1 = 0.9, beta_2 = 0.999,
epsilon = NULL, clipnorm = NULL,
clipvalue = NULL),
metrics = list("mae", "mse")
)
model
}
neural_network <- build_NN(hidden_layers, loss_function)
# Other options used throughout the neural network fitting process are specified here
# Namely, batch size
# In general, larger batch sizes result in faster progress in training, but don't always converge as fast. Smaller batch sizes train slower, but can converge faster.
# Default batch size is 32
neural_network %>% fit(as.matrix(train_x), as.matrix(train_y),
batch_size = batch_size, epochs = 500, verbose = 0,
validation_data = list(as.matrix(validation_x), as.matrix(validation_y)),
callbacks = list(early_stop, print_dot_callback))
#Model
#NNet_stats[[set]]$model <- neural_network
#Train
train_predict <- neural_network %>% predict(as.matrix(train_x))
NNet_stats[[set]]$loss_stats$train_MAE <- mae(train$rt, train_predict)
NNet_stats[[set]]$loss_stats$train_MSE <- mse(train$rt, train_predict)
NNet_stats[[set]]$loss_stats$train_RMSE <- rmse(train$rt, train_predict)
NNet_stats[[set]]$loss_stats$train_RSquare <- R2(train_predict, train$rt, form = "traditional")
#Validation
validation_predict <- neural_network %>% predict(as.matrix(validation_x))
NNet_stats[[set]]$loss_stats$validation_MAE <- mae(validation$rt, validation_predict)
NNet_stats[[set]]$loss_stats$validation_MSE <- mse(validation$rt, validation_predict)
NNet_stats[[set]]$loss_stats$validation_RMSE <- rmse(validation$rt, validation_predict)
NNet_stats[[set]]$loss_stats$validation_RSquare <- R2(validation_predict, validation$rt, form = "traditional")
#Test
test_predict <- neural_network %>% predict(as.matrix(test_x))
NNet_stats[[set]]$loss_stats$test_MAE <- mae(test$rt, test_predict)
NNet_stats[[set]]$loss_stats$test_MSE <- mse(test$rt, test_predict)
NNet_stats[[set]]$loss_stats$test_RMSE <- rmse(test$rt, test_predict)
NNet_stats[[set]]$loss_stats$test_RSquare <- R2(test_predict, test$rt, form = "traditional")
#Forecasts
#NNet_stats[[set]]$forecasts <- test_predict
#Forecast residuals
NNet_stats[[set]]$forecast_resids <- nnet_ave_forecast_resids(nnet_model = neural_network, test = test)
#Variable Importance
NNet_stats[[set]]$variable_importance <- NNet_variable_importance(test, neural_network)
}
#Clear Kera Session
k_clear_session()
NNet_stats
}
NNet_1_mse_stats <- NNet_fit_stats(pooled_panel, timeSlices, 1, "mse", batch_size = 32, 40)
library(tidyverse)
knitr::opts_chunk$set(echo = TRUE)
################
##Load Libraries
################
library(tidyverse)
library(keras)
library(quantreg)
library(ggplot2)
library(forecast)
library(rlist)
library(Metrics)
library(ranger)
library(caret)
library(forcats)
library(xtable)
library(randomForestSRC)
#Parallel Computing
library(foreach)
library(doFuture)
#Registering
registerDoFuture()
plan(multisession)
set.seed(27935248)
## Chunk to load in simulated datasets
g1_A1_nosv_0 <- readRDS("~/GitHub/Evaluation-of-Machine-Learning-in-Asset-Pricing/R/Simulated_dataset/g1_A1_nosv_0.RDS")
pooled_panel <- g1_A1_nosv_0[[1]]$panel
NNet_1_mse_stats <- NNet_fit_stats(pooled_panel, timeSlices, 1, "mse", batch_size = 32, 40)
# Foreach parallel implementation of previous section
# There are a LOT of predictors to inerate over and often this is the bottleneck for the model fitting procedure
# Therefore, use these parallel implementations if possible
# Also, the code is noticeably neater
# Variable Importance Metric Functions
# Given a single model and test set, return a variable importance dataframe (don't bother sorting them now)
# Note that a training set is NOT needed because this is to be called AFTER the model is fit
# These functions were originally written after the actul fitting functions were
# These are used by the fitting functions, so they need to be declared before them
# Similarly, different functions are needed for all model types because all the model objects have slightly different predict methods
################################################
# Linear Model
LM_variable_importance <- function(test, lm_model) {
test_x <- test[4:ncol(test)]
# Specify .packages = "quantreg" here as it seems it isn't supported and therefore is missed by doFuture
variable_importance_df <- foreach(i = (1:ncol(test_x)), .combine = "rbind", .packages = c("speedglm", "quantreg")) %dopar% {
test_x_zero <- test_x
test_x_zero[, i] <- 0
original_R2 <- R2(predict(lm_model, newdata = test_x), test$rt, form = "traditional")
new_R2 <- R2(predict(lm_model, newdata = test_x_zero), test$rt, form = "traditional")
variable_importance <- data.frame(variable = colnames(test_x)[i], importance = (original_R2 - new_R2))
variable_importance
}
variable_importance_df
}
# Penalized Linear Model
ELN_variable_importance <- function(test, eln_model, alpha, lambda) {
test_x <- as.matrix(test[4:ncol(test)])
variable_importance_df <- foreach(i = (1:ncol(test_x)), .combine = "rbind") %dopar% {
test_x_zero <- test_x
test_x_zero[, i] <- 0
original_R2 <- R2(predict(eln_model, test_x, alpha = alpha, lambda = lambda), test$rt, form = "traditional")
new_R2 <- R2(predict(eln_model, test_x_zero, alpha = alpha, lambda = lambda), test$rt, form = "traditional")
variable_importance <- data.frame(variable = colnames(test_x)[i], importance = (original_R2 - new_R2))
variable_importance
}
variable_importance_df
}
# Random Forest
RF_variable_importance <- function(test, rf_model) {
test_x <- test[4:ncol(test)]
variable_importance_df <- foreach(i = (1:ncol(test_x)), .combine = "rbind", .packages = "randomForestSRC") %do% {
test_x_zero <- test_x
test_x_zero[, i] <- 0
original_R2 <- R2(predict(rf_model, newdata = test_x)$predicted, test$rt, form = "traditional")
new_R2 <- R2(predict(rf_model, newdata = test_x_zero)$predicted, test$rt, form = "traditional")
variable_importance <- data.frame(variable = colnames(test_x)[i], importance = (original_R2 - new_R2))
variable_importance
}
variable_importance_df
}
# Neural Network
# Unfortunately it seems that the tensorflow/keras backend does not work with foreach, so a standard for loop is needed instead
NNet_variable_importance <- function(test, nnet_model) {
test_x <- test[4:ncol(test)]
variable_importance_df <- foreach(i = (1:ncol(test_x)), .combine = "rbind", .packages = c("keras", "tensorflow", "reticulate")) %do% {
test_x_zero <- test_x
test_x_zero[, i] <- 0
original_R2 <- R2(predict(nnet_model, as.matrix(test_x)), test$rt, form = "traditional")
new_R2 <- R2(predict(nnet_model, as.matrix(test_x_zero)), test$rt, form = "traditional")
variable_importance <- data.frame(variable = colnames(test_x)[i], importance = (original_R2 - new_R2))
variable_importance
}
variable_importance_df
}
#Create Training + Test Sets
# Gu et al set a training, validation and test sample equal in length for their simulations. Not the most sensible idea
#$T = 180$ monthly periods corresponds to 15 years. The training sample was set to start from $T = 108$ or 9 years, a validation set 1 year in length. The last 3 years were reserved as a test set never to be used for validation or training.
#This means that there are only 3 sample periods to train on, yay
#Caret and other packages are not so good at dealing with custom training/validation/test sets. Have to specify resampling, tuning and testing manually (ugh)
# Change this to 9 + 3 + 3, maybe more stable procedure
customTimeSlices <- function(start, initialWindow, horizon, validation_size, test_size, set_no) {
time_slice <- list(train = 0, validation = 0, test = 0)
time_slices <- rep(list(time_slice), set_no)
for (t in 1:set_no) {
time_slice$train <- c(start:(initialWindow + (t-1) * horizon + 1))
time_slice$validation <- c((initialWindow + (t-1) * horizon + 2):((initialWindow + (t-1) * horizon) + validation_size + 1))
time_slice$test <- c((initialWindow + (t-1) * horizon) + validation_size + 2):((initialWindow + (t-1) * horizon) + validation_size + test_size + 1)
time_slices[[t]] <- time_slice
}
time_slices
}
#Create custom time slices
# These parameters give you the following train/validation/test splits (in terms of years)
# 6 4 3
# 7 4 3
# 8 4 3
timeSlices <- customTimeSlices(start = 2, initialWindow = 84, horizon = 12, validation_size = 60, test_size = 12, set_no = 3)
#Formula Function, makes it easier for those packages with a formula interface
panel_formula <- function(panel){
#Remove the first 3 colNames, as these correspond to the return, time and stock id
panel_colnames <- colnames(panel)[-c(1:3)]
f <- as.formula(c("rt ~", paste(panel_colnames, collapse = "+")))
return(f)
}
NNet_1_mse_stats <- NNet_fit_stats(pooled_panel, timeSlices, 1, "mse", batch_size = 32, 40)
library(kera)
library(tensorflow)
NNet_1_mse_stats <- NNet_fit_stats(pooled_panel, timeSlices, 1, "mse", batch_size = 32, 40)
sys <- import("sys")
sys$path
Sys.which("python")
library(reticulate)
NNet_1_mse_stats <- NNet_fit_stats(pooled_panel, timeSlices, 1, "mse", batch_size = 32, 40)
knitr::opts_chunk$set(echo = TRUE)
################
##Load Libraries
################
library(tidyverse)
install.packages("tidyverse")
library(tidyverse)
library(keras)
library(quantreg)
library(ggplot2)
library(forecast)
library(forecast)
library(rlist)
library(Metrics)
library(ranger)
library(caret)
library(forcats)
library(xtable)
library(randomForestSRC)
#Parallel Computing
library(foreach)
library(doFuture)
#Registering
registerDoFuture()
plan(multisession)
################
##Load Libraries
################
library(speedglm)
library(tidyverse)
NNet_fit_stats <- function(pooled_panel, timeSlices, hidden_layers, loss_function, batch_size, patience) {
#Initialize
NNet_stats <- rep(list(0), 3)
for (set in 1:3) {
NNet_stats[[set]] <- list(loss_stats = data.frame(train_MAE = 0, train_MSE = 0, train_RMSE = 0, train_RSquare = 0,
validation_MAE = 0, validation_MSE = 0, validation_RMSE = 0, validation_RSquare = 0,
test_MAE = 0, test_MSE = 0, test_RMSE = 0, test_RSquare = 0),
#Other useful things
# Keep forecasts here in for nnet objects to make sure they aren't doing something stupid
forecasts = 0,
forecast_resids = 0,
model = 0,
variable_importance = 0)
#Load Training, validation and test sets
train <- pooled_panel %>%
filter(time %in% timeSlices[[set]]$train)
validation <- pooled_panel %>%
filter(time %in% timeSlices[[set]]$validation)
test<- pooled_panel %>%
filter(time %in% timeSlices[[set]]$test)
train_x <- train[4:ncol(train)]
train_x <- scale(train_x)
col_means_train <- attr(train_x, "scaled:center")
col_stddevs_train <- attr(train_x, "scaled:scale")
train_y <- train$rt
validation_x <- validation[4:ncol(validation)]
validation_x <- scale(validation_x, center = col_means_train, scale = col_stddevs_train)
validation_y <- validation$rt
test_x <- test[4:ncol(test)]
test_x <- scale(test_x, center = col_means_train, scale = col_stddevs_train)
test_y <- test$rt
# Fit the model
# The patience parameter is the amount of epochs to check for improvement.
# Gu et al don't say what their early stopping parameter p is
early_stop <- callback_early_stopping(monitor = "val_loss", patience = patience, restore_best_weights = TRUE)
print_dot_callback <- callback_lambda(
on_epoch_end = function(epoch, logs) {
if (epoch %% 50 == 0) cat("\n")
cat(".")
}
)
l1_penalty <- 0.01
build_NN <- function(hidden_layers, loss_function) {
if (hidden_layers == 1) {
model <- keras_model_sequential() %>%
# Layer 1
layer_dense(units = 32, input_shape = ncol(train_x),
bias_initializer = initializer_zeros(),
kernel_initializer = initializer_truncated_normal(mean = 0, stddev = 0.05, seed = NULL)) %>%
layer_activation("tanh") %>%
layer_activity_regularization(l1 = l1_penalty) %>%
layer_batch_normalization() %>%
# Output Layer
layer_dense(units = 1, activation = "linear")
} else if (hidden_layers == 2) {
model <- keras_model_sequential() %>%
# Layer 1
layer_dense(units = 32, input_shape = ncol(train_x),
bias_initializer = initializer_zeros(),
kernel_initializer = initializer_truncated_normal(mean = 0, stddev = 0.05, seed = NULL)) %>%
layer_activation("tanh") %>%
layer_activity_regularization(l1 = l1_penalty) %>%
layer_batch_normalization() %>%
# Layer 2
layer_dense(units = 16,
bias_initializer = initializer_zeros(),
kernel_initializer = initializer_truncated_normal(mean = 0, stddev = 0.05, seed = NULL)) %>%
layer_activation("tanh") %>%
layer_activity_regularization(l1 = l1_penalty) %>%
layer_batch_normalization() %>%
# Output Layer
layer_dense(units = 1, activation = "linear")
} else if (hidden_layers == 3) {
model <- keras_model_sequential() %>%
# Layer 1
layer_dense(units = 32, input_shape = ncol(train_x),
bias_initializer = initializer_zeros(),
kernel_initializer = initializer_truncated_normal(mean = 0, stddev = 0.05, seed = NULL)) %>%
layer_activation("tanh") %>%
layer_activity_regularization(l1 = l1_penalty) %>%
layer_batch_normalization() %>%
# Layer 2
layer_dense(units = 16,
bias_initializer = initializer_zeros(),
kernel_initializer = initializer_truncated_normal(mean = 0, stddev = 0.05, seed = NULL)) %>%
layer_activation("tanh") %>%
layer_activity_regularization(l1 = l1_penalty) %>%
layer_batch_normalization() %>%
#Layer 3
layer_dense(units = 8,
bias_initializer = initializer_zeros(),
kernel_initializer = initializer_truncated_normal(mean = 0, stddev = 0.05, seed = NULL)) %>%
layer_activation("tanh") %>%
layer_activity_regularization(l1 = l1_penalty) %>%
layer_batch_normalization() %>%
# Output Layer
layer_dense(units = 1, activation = "linear")
} else if (hidden_layers == 4) {
model <- keras_model_sequential() %>%
# Layer 1
layer_dense(units = 32, input_shape = ncol(train_x),
bias_initializer = initializer_zeros(),
kernel_initializer = initializer_truncated_normal(mean = 0, stddev = 0.05, seed = NULL)) %>%
layer_activation("tanh") %>%
layer_activity_regularization(l1 = l1_penalty) %>%
layer_batch_normalization() %>%
# Layer 2
layer_dense(units = 16,
bias_initializer = initializer_zeros(),
kernel_initializer = initializer_truncated_normal(mean = 0, stddev = 0.05, seed = NULL)) %>%
layer_activation("tanh") %>%
layer_activity_regularization(l1 = l1_penalty) %>%
layer_batch_normalization() %>%
#Layer 3
layer_dense(units = 8,
bias_initializer = initializer_zeros(),
kernel_initializer = initializer_truncated_normal(mean = 0, stddev = 0.05, seed = NULL)) %>%
layer_activation("tanh") %>%
layer_activity_regularization(l1 = l1_penalty) %>%
layer_batch_normalization() %>%
#Layer 4
layer_dense(units = 4,
bias_initializer = initializer_zeros(),
kernel_initializer = initializer_truncated_normal(mean = 0, stddev = 0.05, seed = NULL)) %>%
layer_activation("tanh") %>%
layer_activity_regularization(l1 = l1_penalty) %>%
layer_batch_normalization() %>%
# Output Layer
layer_dense(units = 1, activation = "linear")
} else {
model <- keras_model_sequential() %>%
# Layer 1
layer_dense(units = 32, input_shape = ncol(train_x),
bias_initializer = initializer_zeros(),
kernel_initializer = initializer_truncated_normal(mean = 0, stddev = 0.05, seed = NULL)) %>%
layer_activation("tanh") %>%
layer_activity_regularization(l1 = l1_penalty) %>%
layer_batch_normalization() %>%
# Layer 2
layer_dense(units = 16,
bias_initializer = initializer_zeros(),
kernel_initializer = initializer_truncated_normal(mean = 0, stddev = 0.05, seed = NULL)) %>%
layer_activation("tanh") %>%
layer_activity_regularization(l1 = l1_penalty) %>%
layer_batch_normalization() %>%
#Layer 3
layer_dense(units = 8,
bias_initializer = initializer_zeros(),
kernel_initializer = initializer_truncated_normal(mean = 0, stddev = 0.05, seed = NULL)) %>%
layer_activation("tanh") %>%
layer_activity_regularization(l1 = l1_penalty) %>%
layer_batch_normalization() %>%
#Layer 4
layer_dense(units = 4,
bias_initializer = initializer_zeros(),
kernel_initializer = initializer_truncated_normal(mean = 0, stddev = 0.05, seed = NULL)) %>%
layer_activation("tanh") %>%
layer_activity_regularization(l1 = l1_penalty) %>%
layer_batch_normalization() %>%
#Layer 5
layer_dense(units = 2,
bias_initializer = initializer_zeros(),
kernel_initializer = initializer_truncated_normal(mean = 0, stddev = 0.05, seed = NULL)) %>%
layer_activation("tanh") %>%
layer_activity_regularization(l1 = l1_penalty) %>%
layer_batch_normalization() %>%
# Output Layer
layer_dense(units = 1, activation = "linear")
}
model %>% compile(
loss = loss_function,
optimizer = optimizer_adam(lr = 0.001, beta_1 = 0.9, beta_2 = 0.999,
epsilon = NULL, clipnorm = NULL,
clipvalue = NULL),
metrics = list("mae", "mse")
)
model
}
neural_network <- build_NN(hidden_layers, loss_function)
# Other options used throughout the neural network fitting process are specified here
# Namely, batch size
# In general, larger batch sizes result in faster progress in training, but don't always converge as fast. Smaller batch sizes train slower, but can converge faster.
# Default batch size is 32
neural_network %>% fit(as.matrix(train_x), as.matrix(train_y),
batch_size = batch_size, epochs = 500, verbose = 0,
validation_data = list(as.matrix(validation_x), as.matrix(validation_y)),
callbacks = list(early_stop, print_dot_callback))
#Model
#NNet_stats[[set]]$model <- neural_network
#Train
train_predict <- neural_network %>% predict(as.matrix(train_x))
NNet_stats[[set]]$loss_stats$train_MAE <- mae(train$rt, train_predict)
NNet_stats[[set]]$loss_stats$train_MSE <- mse(train$rt, train_predict)
NNet_stats[[set]]$loss_stats$train_RMSE <- rmse(train$rt, train_predict)
NNet_stats[[set]]$loss_stats$train_RSquare <- R2(train_predict, train$rt, form = "traditional")
#Validation
validation_predict <- neural_network %>% predict(as.matrix(validation_x))
NNet_stats[[set]]$loss_stats$validation_MAE <- mae(validation$rt, validation_predict)
NNet_stats[[set]]$loss_stats$validation_MSE <- mse(validation$rt, validation_predict)
NNet_stats[[set]]$loss_stats$validation_RMSE <- rmse(validation$rt, validation_predict)
NNet_stats[[set]]$loss_stats$validation_RSquare <- R2(validation_predict, validation$rt, form = "traditional")
#Test
test_predict <- neural_network %>% predict(as.matrix(test_x))
NNet_stats[[set]]$loss_stats$test_MAE <- mae(test$rt, test_predict)
NNet_stats[[set]]$loss_stats$test_MSE <- mse(test$rt, test_predict)
NNet_stats[[set]]$loss_stats$test_RMSE <- rmse(test$rt, test_predict)
NNet_stats[[set]]$loss_stats$test_RSquare <- R2(test_predict, test$rt, form = "traditional")
#Forecasts
#NNet_stats[[set]]$forecasts <- test_predict
#Forecast residuals
NNet_stats[[set]]$forecast_resids <- nnet_ave_forecast_resids(nnet_model = neural_network, test = test)
#Variable Importance
NNet_stats[[set]]$variable_importance <- NNet_variable_importance(test, neural_network)
}
#Clear Kera Session
k_clear_session()
NNet_stats
}
NNet_1_mse_stats <- NNet_fit_stats(pooled_panel, timeSlices, 1, "mse", batch_size = 32, 40)
pooled_panel <- g1_A1_nosv_0[[1]]$panel
## Chunk to load in simulated datasets
g1_A1_nosv_0 <- readRDS("~/GitHub/Evaluation-of-Machine-Learning-in-Asset-Pricing/R/Simulated_dataset/g1_A1_nosv_0.RDS")
# Foreach parallel implementation of previous section
# There are a LOT of predictors to inerate over and often this is the bottleneck for the model fitting procedure
# Therefore, use these parallel implementations if possible
# Also, the code is noticeably neater
# Variable Importance Metric Functions
# Given a single model and test set, return a variable importance dataframe (don't bother sorting them now)
# Note that a training set is NOT needed because this is to be called AFTER the model is fit
# These functions were originally written after the actul fitting functions were
# These are used by the fitting functions, so they need to be declared before them
# Similarly, different functions are needed for all model types because all the model objects have slightly different predict methods
################################################
# Linear Model
LM_variable_importance <- function(test, lm_model) {
test_x <- test[4:ncol(test)]
# Specify .packages = "quantreg" here as it seems it isn't supported and therefore is missed by doFuture
variable_importance_df <- foreach(i = (1:ncol(test_x)), .combine = "rbind", .packages = c("speedglm", "quantreg")) %dopar% {
test_x_zero <- test_x
test_x_zero[, i] <- 0
original_R2 <- R2(predict(lm_model, newdata = test_x), test$rt, form = "traditional")
new_R2 <- R2(predict(lm_model, newdata = test_x_zero), test$rt, form = "traditional")
variable_importance <- data.frame(variable = colnames(test_x)[i], importance = (original_R2 - new_R2))
variable_importance
}
variable_importance_df
}
# Penalized Linear Model
ELN_variable_importance <- function(test, eln_model, alpha, lambda) {
test_x <- as.matrix(test[4:ncol(test)])
variable_importance_df <- foreach(i = (1:ncol(test_x)), .combine = "rbind") %dopar% {
test_x_zero <- test_x
test_x_zero[, i] <- 0
original_R2 <- R2(predict(eln_model, test_x, alpha = alpha, lambda = lambda), test$rt, form = "traditional")
new_R2 <- R2(predict(eln_model, test_x_zero, alpha = alpha, lambda = lambda), test$rt, form = "traditional")
variable_importance <- data.frame(variable = colnames(test_x)[i], importance = (original_R2 - new_R2))
variable_importance
}
variable_importance_df
}
# Random Forest
RF_variable_importance <- function(test, rf_model) {
test_x <- test[4:ncol(test)]
variable_importance_df <- foreach(i = (1:ncol(test_x)), .combine = "rbind", .packages = "randomForestSRC") %do% {
test_x_zero <- test_x
test_x_zero[, i] <- 0
original_R2 <- R2(predict(rf_model, newdata = test_x)$predicted, test$rt, form = "traditional")
new_R2 <- R2(predict(rf_model, newdata = test_x_zero)$predicted, test$rt, form = "traditional")
variable_importance <- data.frame(variable = colnames(test_x)[i], importance = (original_R2 - new_R2))
variable_importance
}
variable_importance_df
}
# Neural Network
# Unfortunately it seems that the tensorflow/keras backend does not work with foreach, so a standard for loop is needed instead
NNet_variable_importance <- function(test, nnet_model) {
test_x <- test[4:ncol(test)]
variable_importance_df <- foreach(i = (1:ncol(test_x)), .combine = "rbind", .packages = c("keras", "tensorflow", "reticulate")) %do% {
test_x_zero <- test_x
test_x_zero[, i] <- 0
original_R2 <- R2(predict(nnet_model, as.matrix(test_x)), test$rt, form = "traditional")
new_R2 <- R2(predict(nnet_model, as.matrix(test_x_zero)), test$rt, form = "traditional")
variable_importance <- data.frame(variable = colnames(test_x)[i], importance = (original_R2 - new_R2))
variable_importance
}
variable_importance_df
}
NNet_1_mse_stats <- NNet_fit_stats(pooled_panel, timeSlices, 1, "mse", batch_size = 32, 40)
pooled_panel <- g1_A1_nosv_0[[1]]$panel
NNet_1_mse_stats <- NNet_fit_stats(pooled_panel, timeSlices, 1, "mse", batch_size = 32, 40)
install.packages("dplyr")
library(dplyr)
NNet_1_mse_stats <- NNet_fit_stats(pooled_panel, timeSlices, 1, "mse", batch_size = 32, 40)
find.package("tidyverse")
install.packages("tidyverse")
library(tidyverse)
