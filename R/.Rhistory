for (set in 1:3) {
# Linear Models
if (typeof(results_list[[batch]]$LM_MSE) == "list") {
LM <- rbind(
data.frame(cbind(results_list[[batch]]$LM_MSE[[set]]$loss_stats, model = "LM_MSE")),
data.frame(cbind(results_list[[batch]]$LM_MAE[[set]]$loss_stats, model = "LM_MAE"))
)
} else {LM <- NULL}
# Elastic Net
if (typeof(results_list[[batch]]$ELN_MSE) == "list") {
ELN <- rbind(
data.frame(cbind(results_list[[batch]]$ELN_MSE[[set]]$loss_stats, model = "ELN_MSE")),
data.frame(cbind(results_list[[batch]]$ELN_MAE[[set]]$loss_stats, model = "ELN_MAE"))
)
} else {ELN <- NULL}
# Random Forest
if (typeof(results_list[[batch]]$RF_MSE) == "list") {
RF <- rbind(
data.frame(cbind(results_list[[batch]]$RF_MSE[[set]]$loss_stats, model = "RF_MSE")),
data.frame(cbind(results_list[[batch]]$RF_MAE[[set]]$loss_stats, model = "RF_MAE"))
)
} else {RF <- NULL}
# Neural Networks
if (typeof(results_list[[batch]]$NN1_MSE) == "list") {
NN <- rbind(
data.frame(cbind(results_list[[batch]]$NN1_MSE[[set]]$loss_stats, model = "NN1_MSE")),
data.frame(cbind(results_list[[batch]]$NN1_MAE[[set]]$loss_stats, model = "NN1_MAE")),
data.frame(cbind(results_list[[batch]]$NN2_MSE[[set]]$loss_stats, model = "NN2_MSE")),
data.frame(cbind(results_list[[batch]]$NN2_MAE[[set]]$loss_stats, model = "NN2_MAE")),
data.frame(cbind(results_list[[batch]]$NN3_MSE[[set]]$loss_stats, model = "NN3_MSE")),
data.frame(cbind(results_list[[batch]]$NN3_MAE[[set]]$loss_stats, model = "NN3_MAE")),
data.frame(cbind(results_list[[batch]]$NN4_MSE[[set]]$loss_stats, model = "NN4_MSE")),
data.frame(cbind(results_list[[batch]]$NN4_MAE[[set]]$loss_stats, model = "NN4_MAE")),
data.frame(cbind(results_list[[batch]]$NN5_MSE[[set]]$loss_stats, model = "NN5_MSE")),
data.frame(cbind(results_list[[batch]]$NN5_MAE[[set]]$loss_stats, model = "NN5_MAE"))
)
} else {NN <- NULL}
results_loss[[batch]][[set]] <- rbind(LM, ELN, RF, NN)
}
}
results_loss
}
gu_et_al_g2_loss <- combined_loss_stats(gu_et_al_g2_results)
gu_et_al_g2_loss[[1]][[1]]
pooled_panel <- gu_et_al_g2[[1]]$panel
knitr::opts_chunk$set(echo = TRUE)
################
##Load Libraries
################
library(tidyverse)
library(keras)
library(ggplot2)
library(forecast)
library(rlist)
library(Metrics)
library(tensorflow)
library(quantreg)
library(randomForestSRC)
library(hqreg)
library(caret)
#Parallel Computing
library(foreach)
library(doFuture)
#Registering
registerDoFuture()
plan(multisession)
# NNet_4_mse_stats[[1]]$loss_stats
# NNet_4_mse_stats[[2]]$loss_stats
# NNet_4_mse_stats[[3]]$loss_stats
#
# NNet_4_mae_stats[[1]]$loss_stats
# NNet_4_mae_stats[[2]]$loss_stats
# NNet_4_mae_stats[[3]]$loss_stats
#
# ##
#
NNet_5_mse_stats <- NNet_fit_stats(pooled_panel, timeSlices, 5, "mse", batch_size = 32, 20)
# NNet_5_mae_stats <- NNet_fit_stats(pooled_panel, timeSlices, 5, "mae", batch_size = 32, 20)
#
NNet_5_mse_stats[[1]]$loss_stats
NNet_5_mse_stats[[2]]$loss_stats
NNet_5_mse_stats[[3]]$loss_stats
NNet_5_mse_stats[[1]]$variable_importance %>%
arrange(desc(importance))
NNet_fit_stats <- function(pooled_panel, timeSlices, hidden_layers, loss_function, batch_size, patience) {
#Initialize
NNet_stats <- rep(list(0), 3)
for (set in 1:3) {
NNet_stats[[set]] <- list(loss_stats = data.frame(train_MAE = 0, train_MSE = 0, train_RMSE = 0, train_RSquare = 0,
validation_MAE = 0, validation_MSE = 0, validation_RMSE = 0, validation_RSquare = 0,
test_MAE = 0, test_MSE = 0, test_RMSE = 0, test_RSquare = 0),
#Other useful things
forecasts = 0,
forecast_resids = 0,
model = 0,
variable_importance = 0)
#Load Training, validation and test sets
train <- pooled_panel %>%
filter(time %in% timeSlices[[set]]$train)
validation <- pooled_panel %>%
filter(time %in% timeSlices[[set]]$validation)
test<- pooled_panel %>%
filter(time %in% timeSlices[[set]]$test)
train_x <- train[4:ncol(train)]
train_y <- train$rt
validation_x <- validation[4:ncol(validation)]
validation_y <- validation$rt
test_x <- test[4:ncol(test)]
test_y <- test$rt
# Fit the model
# The patience parameter is the amount of epochs to check for improvement.
# Gu et al don't say what their early stopping parameter p is
early_stop <- callback_early_stopping(monitor = "val_loss", patience = patience, restore_best_weights = TRUE)
print_dot_callback <- callback_lambda(
on_epoch_end = function(epoch, logs) {
if (epoch %% 50 == 0) cat("\n")
cat(".")
}
)
l1_penalty <- 0.1
build_NN <- function(hidden_layers, loss_function) {
if (hidden_layers == 1) {
model <- keras_model_sequential() %>%
# Layer 1
layer_dense(units = 32, input_shape = ncol(train_x)) %>%
layer_activation("relu") %>%
layer_activity_regularization(l1 = l1_penalty) %>%
layer_batch_normalization() %>%
# Output Layer
layer_dense(units = 1, activation = "linear")
} else if (hidden_layers == 2) {
model <- keras_model_sequential() %>%
# Layer 1
layer_dense(units = 32, input_shape = ncol(train_x)) %>%
layer_activation("relu") %>%
layer_activity_regularization(l1 = l1_penalty) %>%
layer_batch_normalization() %>%
# Layer 2
layer_dense(units = 16) %>%
layer_activation("relu") %>%
layer_activity_regularization(l1 = l1_penalty) %>%
layer_batch_normalization() %>%
# Output Layer
layer_dense(units = 1, activation = "linear")
} else if (hidden_layers == 3) {
model <- keras_model_sequential() %>%
# Layer 1
layer_dense(units = 32, input_shape = ncol(train_x)) %>%
layer_activation("relu") %>%
layer_activity_regularization(l1 = l1_penalty) %>%
layer_batch_normalization() %>%
# Layer 2
layer_dense(units = 16) %>%
layer_activation("relu") %>%
layer_activity_regularization(l1 = l1_penalty) %>%
layer_batch_normalization() %>%
#Layer 3
layer_dense(units = 8) %>%
layer_activation("relu") %>%
layer_activity_regularization(l1 = l1_penalty) %>%
layer_batch_normalization() %>%
# Output Layer
layer_dense(units = 1, activation = "linear")
} else if (hidden_layers == 4) {
model <- keras_model_sequential() %>%
# Layer 1
layer_dense(units = 32, input_shape = ncol(train_x)) %>%
layer_activation("relu") %>%
layer_activity_regularization(l1 = l1_penalty) %>%
layer_batch_normalization() %>%
# Layer 2
layer_dense(units = 16) %>%
layer_activation("relu") %>%
layer_activity_regularization(l1 = l1_penalty) %>%
layer_batch_normalization() %>%
#Layer 3
layer_dense(units = 8) %>%
layer_activation("relu") %>%
layer_activity_regularization(l1 = l1_penalty) %>%
layer_batch_normalization() %>%
#Layer 4
layer_dense(units = 4) %>%
layer_activation("relu") %>%
layer_activity_regularization(l1 = l1_penalty) %>%
layer_batch_normalization() %>%
# Output Layer
layer_dense(units = 1, activation = "linear")
} else {
model <- keras_model_sequential() %>%
# Layer 1
layer_dense(units = 32, input_shape = ncol(train_x)) %>%
layer_activation("relu") %>%
layer_activity_regularization(l1 = l1_penalty) %>%
layer_batch_normalization() %>%
# Layer 2
layer_dense(units = 16) %>%
layer_activation("relu") %>%
layer_activity_regularization(l1 = l1_penalty) %>%
layer_batch_normalization() %>%
#Layer 3
layer_dense(units = 8) %>%
layer_activation("relu") %>%
layer_activity_regularization(l1 = l1_penalty) %>%
layer_batch_normalization() %>%
#Layer 4
layer_dense(units = 4) %>%
layer_activation("relu") %>%
layer_activity_regularization(l1 = l1_penalty) %>%
layer_batch_normalization() %>%
#Layer 5
layer_dense(units = 2) %>%
layer_activation("relu") %>%
layer_activity_regularization(l1 = l1_penalty) %>%
layer_batch_normalization() %>%
# Output Layer
layer_dense(units = 1, activation = "linear")
}
model %>% compile(
loss = loss_function,
optimizer = "adam",
metrics = list("mae", "mse")
)
model
}
neural_network <- build_NN(hidden_layers, loss_function)
# Other options used throughout the neural network fitting process are specified here
# Namely, batch size
# In general, larger batch sizes result in faster progress in training, but don't always converge as fast. Smaller batch sizes train slower, but can converge faster.
# Default batch size is 32
neural_network %>% fit(as.matrix(train_x), as.matrix(train_y),
batch_size = batch_size, epochs = 500, verbose = 0,
validation_data = list(as.matrix(validation_x), as.matrix(validation_y)),
callbacks = list(early_stop, print_dot_callback))
#Model
NNet_stats[[set]]$model <- neural_network
#Train
train_predict <- neural_network %>% predict(as.matrix(train_x))
NNet_stats[[set]]$loss_stats$train_MAE <- mae(train$rt, train_predict)
NNet_stats[[set]]$loss_stats$train_MSE <- mse(train$rt, train_predict)
NNet_stats[[set]]$loss_stats$train_RMSE <- rmse(train$rt, train_predict)
NNet_stats[[set]]$loss_stats$train_RSquare <- R2(train_predict, train$rt, form = "traditional")
#Validation
validation_predict <- neural_network %>% predict(as.matrix(validation_x))
NNet_stats[[set]]$loss_stats$validation_MAE <- mae(validation$rt, validation_predict)
NNet_stats[[set]]$loss_stats$validation_MSE <- mse(validation$rt, validation_predict)
NNet_stats[[set]]$loss_stats$validation_RMSE <- rmse(validation$rt, validation_predict)
NNet_stats[[set]]$loss_stats$validation_RSquare <- R2(validation_predict, validation$rt, form = "traditional")
#Test
test_predict <- neural_network %>% predict(as.matrix(test_x))
NNet_stats[[set]]$loss_stats$test_MAE <- mae(test$rt, test_predict)
NNet_stats[[set]]$loss_stats$test_MSE <- mse(test$rt, test_predict)
NNet_stats[[set]]$loss_stats$test_RMSE <- rmse(test$rt, test_predict)
NNet_stats[[set]]$loss_stats$test_RSquare <- R2(test_predict, test$rt, form = "traditional")
#Forecasts
NNet_stats[[set]]$forecasts <- test_predict
#Forecast residuals
NNet_stats[[set]]$forecast_resids <- test$rt - test_predict
#Variable Importance
NNet_stats[[set]]$variable_importance <- NNet_variable_importance(test, neural_network)
}
NNet_stats
}
# NNet_4_mse_stats[[1]]$loss_stats
# NNet_4_mse_stats[[2]]$loss_stats
# NNet_4_mse_stats[[3]]$loss_stats
#
# NNet_4_mae_stats[[1]]$loss_stats
# NNet_4_mae_stats[[2]]$loss_stats
# NNet_4_mae_stats[[3]]$loss_stats
#
# ##
#
NNet_5_mse_stats <- NNet_fit_stats(pooled_panel, timeSlices, 5, "mse", batch_size = 32, 10)
NNet_5_mse_stats[[1]]$variable_importance %>%
arrange(desc(importance))
# NNet_5_mae_stats <- NNet_fit_stats(pooled_panel, timeSlices, 5, "mae", batch_size = 32, 20)
#
NNet_5_mse_stats[[1]]$loss_stats
NNet_5_mse_stats[[2]]$loss_stats
NNet_5_mse_stats[[3]]$loss_stats
NNet_fit_stats <- function(pooled_panel, timeSlices, hidden_layers, loss_function, batch_size, patience) {
#Initialize
NNet_stats <- rep(list(0), 3)
for (set in 1:3) {
NNet_stats[[set]] <- list(loss_stats = data.frame(train_MAE = 0, train_MSE = 0, train_RMSE = 0, train_RSquare = 0,
validation_MAE = 0, validation_MSE = 0, validation_RMSE = 0, validation_RSquare = 0,
test_MAE = 0, test_MSE = 0, test_RMSE = 0, test_RSquare = 0),
#Other useful things
forecasts = 0,
forecast_resids = 0,
model = 0,
variable_importance = 0)
#Load Training, validation and test sets
train <- pooled_panel %>%
filter(time %in% timeSlices[[set]]$train)
validation <- pooled_panel %>%
filter(time %in% timeSlices[[set]]$validation)
test<- pooled_panel %>%
filter(time %in% timeSlices[[set]]$test)
train_x <- train[4:ncol(train)]
train_y <- train$rt
validation_x <- validation[4:ncol(validation)]
validation_y <- validation$rt
test_x <- test[4:ncol(test)]
test_y <- test$rt
# Fit the model
# The patience parameter is the amount of epochs to check for improvement.
# Gu et al don't say what their early stopping parameter p is
early_stop <- callback_early_stopping(monitor = "val_loss", patience = patience, restore_best_weights = TRUE)
print_dot_callback <- callback_lambda(
on_epoch_end = function(epoch, logs) {
if (epoch %% 50 == 0) cat("\n")
cat(".")
}
)
l1_penalty <- 0.001
build_NN <- function(hidden_layers, loss_function) {
if (hidden_layers == 1) {
model <- keras_model_sequential() %>%
# Layer 1
layer_dense(units = 32, input_shape = ncol(train_x)) %>%
layer_activation("relu") %>%
layer_activity_regularization(l1 = l1_penalty) %>%
layer_batch_normalization() %>%
# Output Layer
layer_dense(units = 1, activation = "linear")
} else if (hidden_layers == 2) {
model <- keras_model_sequential() %>%
# Layer 1
layer_dense(units = 32, input_shape = ncol(train_x)) %>%
layer_activation("relu") %>%
layer_activity_regularization(l1 = l1_penalty) %>%
layer_batch_normalization() %>%
# Layer 2
layer_dense(units = 16) %>%
layer_activation("relu") %>%
layer_activity_regularization(l1 = l1_penalty) %>%
layer_batch_normalization() %>%
# Output Layer
layer_dense(units = 1, activation = "linear")
} else if (hidden_layers == 3) {
model <- keras_model_sequential() %>%
# Layer 1
layer_dense(units = 32, input_shape = ncol(train_x)) %>%
layer_activation("relu") %>%
layer_activity_regularization(l1 = l1_penalty) %>%
layer_batch_normalization() %>%
# Layer 2
layer_dense(units = 16) %>%
layer_activation("relu") %>%
layer_activity_regularization(l1 = l1_penalty) %>%
layer_batch_normalization() %>%
#Layer 3
layer_dense(units = 8) %>%
layer_activation("relu") %>%
layer_activity_regularization(l1 = l1_penalty) %>%
layer_batch_normalization() %>%
# Output Layer
layer_dense(units = 1, activation = "linear")
} else if (hidden_layers == 4) {
model <- keras_model_sequential() %>%
# Layer 1
layer_dense(units = 32, input_shape = ncol(train_x)) %>%
layer_activation("relu") %>%
layer_activity_regularization(l1 = l1_penalty) %>%
layer_batch_normalization() %>%
# Layer 2
layer_dense(units = 16) %>%
layer_activation("relu") %>%
layer_activity_regularization(l1 = l1_penalty) %>%
layer_batch_normalization() %>%
#Layer 3
layer_dense(units = 8) %>%
layer_activation("relu") %>%
layer_activity_regularization(l1 = l1_penalty) %>%
layer_batch_normalization() %>%
#Layer 4
layer_dense(units = 4) %>%
layer_activation("relu") %>%
layer_activity_regularization(l1 = l1_penalty) %>%
layer_batch_normalization() %>%
# Output Layer
layer_dense(units = 1, activation = "linear")
} else {
model <- keras_model_sequential() %>%
# Layer 1
layer_dense(units = 32, input_shape = ncol(train_x)) %>%
layer_activation("relu") %>%
layer_activity_regularization(l1 = l1_penalty) %>%
layer_batch_normalization() %>%
# Layer 2
layer_dense(units = 16) %>%
layer_activation("relu") %>%
layer_activity_regularization(l1 = l1_penalty) %>%
layer_batch_normalization() %>%
#Layer 3
layer_dense(units = 8) %>%
layer_activation("relu") %>%
layer_activity_regularization(l1 = l1_penalty) %>%
layer_batch_normalization() %>%
#Layer 4
layer_dense(units = 4) %>%
layer_activation("relu") %>%
layer_activity_regularization(l1 = l1_penalty) %>%
layer_batch_normalization() %>%
#Layer 5
layer_dense(units = 2) %>%
layer_activation("relu") %>%
layer_activity_regularization(l1 = l1_penalty) %>%
layer_batch_normalization() %>%
# Output Layer
layer_dense(units = 1, activation = "linear")
}
model %>% compile(
loss = loss_function,
optimizer = "adam",
metrics = list("mae", "mse")
)
model
}
neural_network <- build_NN(hidden_layers, loss_function)
# Other options used throughout the neural network fitting process are specified here
# Namely, batch size
# In general, larger batch sizes result in faster progress in training, but don't always converge as fast. Smaller batch sizes train slower, but can converge faster.
# Default batch size is 32
neural_network %>% fit(as.matrix(train_x), as.matrix(train_y),
batch_size = batch_size, epochs = 500, verbose = 0,
validation_data = list(as.matrix(validation_x), as.matrix(validation_y)),
callbacks = list(early_stop, print_dot_callback))
#Model
NNet_stats[[set]]$model <- neural_network
#Train
train_predict <- neural_network %>% predict(as.matrix(train_x))
NNet_stats[[set]]$loss_stats$train_MAE <- mae(train$rt, train_predict)
NNet_stats[[set]]$loss_stats$train_MSE <- mse(train$rt, train_predict)
NNet_stats[[set]]$loss_stats$train_RMSE <- rmse(train$rt, train_predict)
NNet_stats[[set]]$loss_stats$train_RSquare <- R2(train_predict, train$rt, form = "traditional")
#Validation
validation_predict <- neural_network %>% predict(as.matrix(validation_x))
NNet_stats[[set]]$loss_stats$validation_MAE <- mae(validation$rt, validation_predict)
NNet_stats[[set]]$loss_stats$validation_MSE <- mse(validation$rt, validation_predict)
NNet_stats[[set]]$loss_stats$validation_RMSE <- rmse(validation$rt, validation_predict)
NNet_stats[[set]]$loss_stats$validation_RSquare <- R2(validation_predict, validation$rt, form = "traditional")
#Test
test_predict <- neural_network %>% predict(as.matrix(test_x))
NNet_stats[[set]]$loss_stats$test_MAE <- mae(test$rt, test_predict)
NNet_stats[[set]]$loss_stats$test_MSE <- mse(test$rt, test_predict)
NNet_stats[[set]]$loss_stats$test_RMSE <- rmse(test$rt, test_predict)
NNet_stats[[set]]$loss_stats$test_RSquare <- R2(test_predict, test$rt, form = "traditional")
#Forecasts
NNet_stats[[set]]$forecasts <- test_predict
#Forecast residuals
NNet_stats[[set]]$forecast_resids <- test$rt - test_predict
#Variable Importance
NNet_stats[[set]]$variable_importance <- NNet_variable_importance(test, neural_network)
}
NNet_stats
}
# NNet_4_mse_stats[[1]]$loss_stats
# NNet_4_mse_stats[[2]]$loss_stats
# NNet_4_mse_stats[[3]]$loss_stats
#
# NNet_4_mae_stats[[1]]$loss_stats
# NNet_4_mae_stats[[2]]$loss_stats
# NNet_4_mae_stats[[3]]$loss_stats
#
# ##
#
NNet_5_mse_stats <- NNet_fit_stats(pooled_panel, timeSlices, 5, "mse", batch_size = 32, 10)
# NNet_5_mae_stats <- NNet_fit_stats(pooled_panel, timeSlices, 5, "mae", batch_size = 32, 20)
#
NNet_5_mse_stats[[1]]$loss_stats
NNet_5_mse_stats[[2]]$loss_stats
NNet_5_mse_stats[[3]]$loss_stats
NNet_5_mse_stats[[1]]$variable_importance %>%
arrange(desc(importance))
# NNet_4_mse_stats[[1]]$loss_stats
# NNet_4_mse_stats[[2]]$loss_stats
# NNet_4_mse_stats[[3]]$loss_stats
#
# NNet_4_mae_stats[[1]]$loss_stats
# NNet_4_mae_stats[[2]]$loss_stats
# NNet_4_mae_stats[[3]]$loss_stats
#
# ##
#
NNet_5_mse_stats <- NNet_fit_stats(pooled_panel, timeSlices, 5, "mse", batch_size = 1024, 10)
# NNet_5_mae_stats <- NNet_fit_stats(pooled_panel, timeSlices, 5, "mae", batch_size = 32, 20)
#
NNet_5_mse_stats[[1]]$loss_stats
NNet_5_mse_stats[[2]]$loss_stats
NNet_5_mse_stats[[3]]$loss_stats
NNet_5_mse_stats[[1]]$variable_importance %>%
arrange(desc(importance))
# NNet_4_mse_stats[[1]]$loss_stats
# NNet_4_mse_stats[[2]]$loss_stats
# NNet_4_mse_stats[[3]]$loss_stats
#
# NNet_4_mae_stats[[1]]$loss_stats
# NNet_4_mae_stats[[2]]$loss_stats
# NNet_4_mae_stats[[3]]$loss_stats
#
# ##
#
NNet_5_mse_stats <- NNet_fit_stats(pooled_panel, timeSlices, 5, "mse", batch_size = 16, 20)
# NNet_5_mae_stats <- NNet_fit_stats(pooled_panel, timeSlices, 5, "mae", batch_size = 32, 20)
#
NNet_5_mse_stats[[1]]$loss_stats
NNet_5_mse_stats[[2]]$loss_stats
NNet_5_mse_stats[[3]]$loss_stats
NNet_5_mse_stats[[1]]$variable_importance %>%
arrange(desc(importance))
