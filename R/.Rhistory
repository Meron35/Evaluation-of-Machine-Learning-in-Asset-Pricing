univariate_time_series <- pooled_panel %>%
filter(stock == stock_id[i]) %>%
filter(time %in% timeSlices[[1]]$train | time %in% timeSlices[[1]]$validation) %>%
select(rt) %>%
ts()
univariate_time_series_test <- pooled_panel %>%
filter(stock == stock_id[i]) %>%
filter(time %in% timeSlices[[1]]$test) %>%
select(rt) %>%
ts()
fforma_features(univariate_time_series)
}
### Errors
# This is trickier
errors <- data.frame(naive = mse(as.vector(univariate_time_series_test), predict(naive_model)$mean),
random_walk = mse(as.vector(univariate_time_series_test), predict(random_walk_model)$mean),
theta = mse(as.vector(univariate_time_series_test), predict(theta_model)$mean),
auto_arima = mse(as.vector(univariate_time_series_test),
forecast(auto_arima_model, h = 12)$mean),
ets = mse(as.vector(univariate_time_series_test), forecast(auto_ets_model, h = 12)$mean),
tbats = mse(as.vector(univariate_time_series_test), forecast(tbats_model, h = 12)$mean),
nnetar = mse(as.vector(univariate_time_series_test), forecast(nnetar_model, h = 12)$mean),
garch_11 = mse(as.vector(univariate_time_series_test), fitted(ugarch_forecast)[1, ]),
garch_11_t = mse(as.vector(univariate_time_series_test), fitted(ugarch_t_forecast)[1, ]),
stochvol = mse(as.vector(univariate_time_series_test),
apply(X = predict(stochvol_model, steps = 12)$y, MARGIN = 2, FUN = mean)))
### Errors
# This is trickier
errors <- data.frame(naive = mse(as.vector(univariate_time_series_test), predict(naive_model)$mean),
random_walk = mse(as.vector(univariate_time_series_test), predict(random_walk_model)$mean),
theta = mse(as.vector(univariate_time_series_test), predict(theta_model)$mean),
auto_arima = mse(as.vector(univariate_time_series_test),
forecast(auto_arima_model, h = 12)$mean),
ets = mse(as.vector(univariate_time_series_test), forecast(auto_ets_model, h = 12)$mean),
tbats = mse(as.vector(univariate_time_series_test), forecast(tbats_model, h = 12)$mean),
nnetar = mse(as.vector(univariate_time_series_test), forecast(nnetar_model, h = 12)$mean),
garch_11 = mse(as.vector(univariate_time_series_test), fitted(ugarch_forecast)[1, ]),
garch_11_t = mse(as.vector(univariate_time_series_test), fitted(ugarch_t_forecast)[1, ]),
stochvol = mse(as.vector(univariate_time_series_test),
apply(X = predict(stochvol_model, steps = 12)$y, MARGIN = 2, FUN = mean)))
library(stochvol)
stochvol_model <- svlsample(univariate_time_series, draws = 10000, burnin = 1000, designmatrix = "ar1",
priormu = c(0, 100), priorphi = c(5, 1.5), priorsigma = 1,
priorrho = c(4, 4), priorbeta = c(0, 10000), thinpara = 1,
thinlatent = 1, thintime = NULL, keeptime = "all", quiet = FALSE)
### Data matrix
# Just use fforma_features function from before, easy
data <- foreach(i = 1:length(unique(pooled_panel$stock)), .combine = "rbind") %dopar% {
stock_id <- unique(pooled_panel$stock)
univariate_time_series <- pooled_panel %>%
filter(stock == stock_id[i]) %>%
filter(time %in% timeSlices[[1]]$train | time %in% timeSlices[[1]]$validation) %>%
select(rt) %>%
ts()
univariate_time_series_test <- pooled_panel %>%
filter(stock == stock_id[i]) %>%
filter(time %in% timeSlices[[1]]$test) %>%
select(rt) %>%
ts()
fforma_features(univariate_time_series)
}
### Data matrix
# Just use fforma_features function from before, easy
data <- foreach(i = 1:length(unique(pooled_panel$stock)), .combine = "rbind") %dopar% {
stock_id <- unique(pooled_panel$stock)
univariate_time_series <- pooled_panel %>%
filter(stock == stock_id[i]) %>%
filter(time %in% timeSlices[[1]]$train | time %in% timeSlices[[1]]$validation) %>%
select(rt) %>%
ts()
univariate_time_series_test <- pooled_panel %>%
filter(stock == stock_id[i]) %>%
filter(time %in% timeSlices[[1]]$test) %>%
select(rt) %>%
ts()
fforma_features(univariate_time_series)
}
### Errors
# This is trickier
errors <- data.frame(naive = mse(as.vector(univariate_time_series_test), predict(naive_model)$mean),
random_walk = mse(as.vector(univariate_time_series_test), predict(random_walk_model)$mean),
theta = mse(as.vector(univariate_time_series_test), predict(theta_model)$mean),
auto_arima = mse(as.vector(univariate_time_series_test),
forecast(auto_arima_model, h = 12)$mean),
ets = mse(as.vector(univariate_time_series_test), forecast(auto_ets_model, h = 12)$mean),
tbats = mse(as.vector(univariate_time_series_test), forecast(tbats_model, h = 12)$mean),
nnetar = mse(as.vector(univariate_time_series_test), forecast(nnetar_model, h = 12)$mean),
garch_11 = mse(as.vector(univariate_time_series_test), fitted(ugarch_forecast)[1, ]),
garch_11_t = mse(as.vector(univariate_time_series_test), fitted(ugarch_t_forecast)[1, ]),
stochvol = mse(as.vector(univariate_time_series_test),
apply(X = predict(stochvol_model, steps = 12)$y, MARGIN = 2, FUN = mean)))
### Labels
# Easy, but requires the setup of the error matrix earlier
labels <- data.frame(label = which.min(errors))
errors <- foreach(i = 1:length(unique(pooled_panel$stock)), .combine = "rbind") %dopar% {
## Filter Data first
stock_id <- unique(pooled_panel$stock)
univariate_time_series <- pooled_panel %>%
filter(stock == stock_id[i]) %>%
filter(time %in% timeSlices[[1]]$train | time %in% timeSlices[[1]]$validation) %>%
select(rt) %>%
ts()
univariate_time_series_test <- pooled_panel %>%
filter(stock == stock_id[i]) %>%
filter(time %in% timeSlices[[1]]$test) %>%
select(rt) %>%
ts()
## Estimation of consituent models
# Naive
naive_model <- naive(univariate_time_series, h = 12)
# Random walk with drift
random_walk_model <- rwf(univariate_time_series, h = 12, drift = TRUE)
# Seasonal Naive, this ends up being the same as naive
seasonal_naive_model <- snaive(univariate_time_series, h = 12)
# Theta Method
theta_model <- thetaf(univariate_time_series, h = 12)
## Auto arima
auto_arima_model <- auto.arima(univariate_time_series)
# Auto ETS
auto_ets_model <- ets(univariate_time_series)
# TBATS
tbats_model <- tbats(univariate_time_series)
# Neural Network time series forecasts
nnetar_model <- nnetar(univariate_time_series)
# Naive model that forecasts 0
# Neat idea, but practically speaking ends up being the sam eas auto arima's 0, 0, 0 model most of the time
# naive0_model <- rep(0, 12)
# mse(as.vector(univariate_time_series_test), naive0_model)
# ARMA 1, 1 with GARCH 1, 1 gaussian errors
ugarch_spec <- ugarchspec(variance.model = list(model = "sGARCH",
garchOrder = c(1, 1),
submodel = NULL,
external.regressors = NULL,
variance.targeting = FALSE),
mean.model     = list(armaOrder = c(1, 1),
include.mean = TRUE,
external.regressors = NULL),
distribution.model = "norm")
ugarch_spec_fit <- ugarchfit(ugarch_spec, data = univariate_time_series, out.sample = 12,
solver = "hybrid")
ugarch_forecast <- ugarchforecast(ugarch_spec_fit, n.ahead = 12, n.roll = 11)
## Stochastic volatility model
## Theoretically the most correctly specified model, but very computatioally intensive, and may be infeasible
## Takes ~ 14 seconds to estimate
stochvol_model <- svlsample(univariate_time_series, draws = 10000, burnin = 1000, designmatrix = "ar1",
priormu = c(0, 100), priorphi = c(5, 1.5), priorsigma = 1,
priorrho = c(4, 4), priorbeta = c(0, 10000), thinpara = 1,
thinlatent = 1, thintime = NULL, keeptime = "all", quiet = TRUE)
## Put errors into a single row
errors <- data.frame(naive = mse(as.vector(univariate_time_series_test), predict(naive_model)$mean),
random_walk = mse(as.vector(univariate_time_series_test), predict(random_walk_model)$mean),
theta = mse(as.vector(univariate_time_series_test), predict(theta_model)$mean),
auto_arima = mse(as.vector(univariate_time_series_test),
forecast(auto_arima_model, h = 12)$mean),
ets = mse(as.vector(univariate_time_series_test), forecast(auto_ets_model, h = 12)$mean),
tbats = mse(as.vector(univariate_time_series_test), forecast(tbats_model, h = 12)$mean),
nnetar = mse(as.vector(univariate_time_series_test), forecast(nnetar_model, h = 12)$mean),
garch_11 = mse(as.vector(univariate_time_series_test), fitted(ugarch_forecast)[1, ]),
stochvol = mse(as.vector(univariate_time_series_test),
apply(X = predict(stochvol_model, steps = 12)$y, MARGIN = 2, FUN = mean)))
errors
}
errors <- foreach(i = 1:length(unique(pooled_panel$stock)), .combine = "rbind") %dopar% {
## Filter Data first
stock_id <- unique(pooled_panel$stock)
univariate_time_series <- pooled_panel %>%
filter(stock == stock_id[i]) %>%
filter(time %in% timeSlices[[1]]$train | time %in% timeSlices[[1]]$validation) %>%
select(rt) %>%
ts()
univariate_time_series_test <- pooled_panel %>%
filter(stock == stock_id[i]) %>%
filter(time %in% timeSlices[[1]]$test) %>%
select(rt) %>%
ts()
## Estimation of consituent models
# Naive
naive_model <- naive(univariate_time_series, h = 12)
# Random walk with drift
random_walk_model <- rwf(univariate_time_series, h = 12, drift = TRUE)
# Seasonal Naive, this ends up being the same as naive
seasonal_naive_model <- snaive(univariate_time_series, h = 12)
# Theta Method
theta_model <- thetaf(univariate_time_series, h = 12)
## Auto arima
auto_arima_model <- auto.arima(univariate_time_series)
# Auto ETS
auto_ets_model <- ets(univariate_time_series)
# TBATS
tbats_model <- tbats(univariate_time_series)
# Neural Network time series forecasts
nnetar_model <- nnetar(univariate_time_series)
# Naive model that forecasts 0
# Neat idea, but practically speaking ends up being the sam eas auto arima's 0, 0, 0 model most of the time
# naive0_model <- rep(0, 12)
# mse(as.vector(univariate_time_series_test), naive0_model)
# ARMA 1, 1 with GARCH 1, 1 gaussian errors
ugarch_spec <- ugarchspec(variance.model = list(model = "sGARCH",
garchOrder = c(1, 1),
submodel = NULL,
external.regressors = NULL,
variance.targeting = FALSE),
mean.model     = list(armaOrder = c(1, 1),
include.mean = TRUE,
external.regressors = NULL),
distribution.model = "norm")
ugarch_spec_fit <- ugarchfit(ugarch_spec, data = univariate_time_series, out.sample = 12,
solver = "hybrid")
ugarch_forecast <- ugarchforecast(ugarch_spec_fit, n.ahead = 12, n.roll = 11)
## Stochastic volatility model
## Theoretically the most correctly specified model, but very computatioally intensive, and may be infeasible
## Takes ~ 14 seconds to estimate
stochvol_model <- svlsample(univariate_time_series, draws = 10000, burnin = 1000, designmatrix = "ar1",
priormu = c(0, 100), priorphi = c(5, 1.5), priorsigma = 1,
priorrho = c(4, 4), priorbeta = c(0, 10000), thinpara = 1,
thinlatent = 1, thintime = NULL, keeptime = "all", quiet = TRUE)
## Put errors into a single row
errors <- data.frame(naive = mse(as.vector(univariate_time_series_test), predict(naive_model)$mean),
random_walk = mse(as.vector(univariate_time_series_test), predict(random_walk_model)$mean),
theta = mse(as.vector(univariate_time_series_test), predict(theta_model)$mean),
auto_arima = mse(as.vector(univariate_time_series_test),
forecast(auto_arima_model, h = 12)$mean),
ets = mse(as.vector(univariate_time_series_test), forecast(auto_ets_model, h = 12)$mean),
tbats = mse(as.vector(univariate_time_series_test), forecast(tbats_model, h = 12)$mean),
nnetar = mse(as.vector(univariate_time_series_test), forecast(nnetar_model, h = 12)$mean),
garch_11 = mse(as.vector(univariate_time_series_test), fitted(ugarch_forecast)[1, ]),
stochvol = mse(as.vector(univariate_time_series_test),
apply(X = predict(stochvol_model, steps = 12)$y, MARGIN = 2, FUN = mean)))
errors
}
labels <- foreach(i = 1:nrow(errors), .combine = "rbind") %dopar% {
data.frame(label = which.min(errors[i, ]) - 1)
}
## Create final list with everything needed in FFORMA
## Looks pretty good
train_list <- list(
data = data,
errors = errors,
labels = labels
)
error_softmax_obj <- function(preds, dtrain) {
labels <- xgboost::getinfo(dtrain, "label")
errors <- attr(dtrain, "errors")
preds <- exp(preds)
sp <- rowSums(preds)
for (n in 1:nrow(preds)) {
preds[n] <- preds/sp[n]
}
rowsumerrors <- rowSums(preds * errors)
grad <- preds*(errors - rowsumerrors)
hess <- errors*preds*(1.0-preds) - grad*preds
#hess <- grad*(1.0 - 2.0*preds)
#hess <- pmax(hess, 1e-16)
#the true hessian should be grad*(1.0 - 2.0*preds) but it produces numerical problems
#what we use here is a upper bound
#print(mean(rowSums(preds*errors)))
return(list(grad = t(grad), hess = t(hess)))
}
param <- list(booster = "gbtree",
max_depth = 6, eta = 0.3, nthread = 12,
num_class = ncol(errors),
objective = error_softmax_obj,
subsample = 1,
colsample_bytree = 1)
dtrain <- xgb.DMatrix(data = as.matrix(train_list$data),
label = train_list$labels[, 1])
attr(dtrain, "errors") <- train_list$errors
bst <- xgboost::xgb.train(param, dtrain, 94,
verbose = 1)
install.packages("xgboost_0.90.0.2.tar")
install.packages("./xgboost_0.90.0.2.tar")
install.packages(".\xgboost_0.90.0.2.tar")
install.packages("~./xgboost_0.90.0.2.tar")
install.packages("~/xgboost_0.90.0.2.tar")
install.packages(""~/GitHub/Evaluation-of-Machine-Learning-in-Asset-Pricing/R/xgboost_0.90.0.2.tar")
install.packages("~/GitHub/Evaluation-of-Machine-Learning-in-Asset-Pricing/R/xgboost_0.90.0.2.tar")
install.packages("xgboost_0.90.0.2.tar.gz")
?install.packages
install.packages("xgboost_0.90.0.2.tar.gz", type = NULL)
install.packages("xgboost_0.90.0.2.tar.gz", repos = NULL)
?setwd
setwd("C:\")
w
...
)
ASasA
install.packages("xgboost_0.90.0.2.tar", repos = NULL)
install.packages("xgboost_0.90.0.2.tar.gz", repos = NULL)
install.packages("jsonlite")
library(tidyverse)
library(tidyverse)
library(jsonlite)
library(foreach)
################
##Load Libraries
################
library(speedglm)
library(tidyverse)
library(keras)
library(ggplot2)
library(forecast)
library(rlist)
library(Metrics)
library(tensorflow)
library(quantreg)
library(randomForestSRC)
library(hqreg)
library(caret)
library(tsfeatures)
## Vector arima/garch packages
library(xts)
library(rmgarch)
library(marima)
library(xgboost)
#Parallel Computing
library(foreach)
library(doFuture)
#Registering
registerDoFuture()
plan(multisession)
g1_A1_sv_0.1 <- readRDS("~/GitHub/Evaluation-of-Machine-Learning-in-Asset-Pricing/R/Simulated_dataset/g1_A1_sv_0.1.RDS")
pooled_panel <- g1_A1_sv_0.1[[1]]$panel
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(jsonlite)
library(foreach)
stock_id <- pooled_panel$stock %>%
unique()
pooled_panel_json <- pooled_panel %>%
dplyr::select(-time) %>%
filter(stock == stock_id[1])
data(iris)
iris <- data(iris)
iris_nest <- iris %>%
nest(-Species)
iris <- iris
iris
iris <- data("iris")
iris
data("iris")
library(datasets)
data("iris")
?data
data(risi)
data(iris)
iris <- data("iris")
iris <- data(iris)
iris
data(iris)
force(iris)
iris
iris_nest <- iris %>%
nest(-Species)
View(iris_nest)
View(iris_nest[[2]][[1]])
pooled_panel_json <- pooled_panel %>%
dplyr::select(-time) %>%
filter(stock == stock_id[1])
pooled_panel_json %>%
tidyr::nest(-stock)
View(pooled_panel_json)
pooled_panel_json <- pooled_panel_json %>%
tidyr::nest(-stock)
View(pooled_panel_json)
View(pooled_panel_json[[2]][[1]])
pooled_panel_json <- pooled_panel %>%
dplyr::select(-time) %>%
filter(stock == stock_id[1])
pooled_panel_json <- pooled_panel_json %>%
tidyr::nest(-stock, -rt)
View(pooled_panel_json)
pooled_panel_json <- pooled_panel_json %>%
tidyr::nest(-stock, -rt) %>%
nest(-rt)
pooled_panel_json <- pooled_panel %>%
dplyr::select(-time) %>%
filter(stock == stock_id[1])
pooled_panel_json <- pooled_panel_json %>%
tidyr::nest(-stock, -rt) %>%
nest(-rt)
View(pooled_panel_json)
pooled_panel_json <- pooled_panel %>%
dplyr::select(-time) %>%
filter(stock == stock_id[1])
pooled_panel_json <- pooled_panel_json %>%
tidyr::nest(-stock, -rt) %>%
nest(-stock)
View(pooled_panel_json)
View(pooled_panel_json[[2]][[1]])
pooled_panel_json <- pooled_panel %>%
dplyr::select(-time) %>%
filter(stock == stock_id[1])
pooled_panel_json <- pooled_panel_json %>%
tidyr::nest(-stock, rt)
View(pooled_panel_json)
View(pooled_panel_json[[2]][[1]])
pooled_panel_json <- pooled_panel_json %>%
tidyr::nest(rt = rt, features = -stock)
View(pooled_panel)
pooled_panel_json <- pooled_panel_json %>%
tidyr::nest(rt = rt)
pooled_panel_json <- pooled_panel_json %>%
tidyr::nest(data = rt)
pooled_panel_json <- pooled_panel_json %>%
tidyr::nest(features = rt)
View(pooled_panel_json)
pooled_panel_json <- pooled_panel %>%
dplyr::select(-time) %>%
filter(stock == stock_id[1])
pooled_panel_json <- pooled_panel_json %>%
tidyr::nest(features = rt)
View(pooled_panel_json)
pooled_panel_json <- pooled_panel_json %>%
tidyr::nest(features = -stock)
View(pooled_panel_json)
View(pooled_panel_json[[2]][[1]])
pooled_panel_json <- pooled_panel %>%
dplyr::select(-time) %>%
filter(stock == stock_id[1])
pooled_panel_json <- pooled_panel_json %>%
tidyr::nest(features = -c(stock, rt))
View(pooled_panel_json)
pooled_panel_json <- pooled_panel %>%
dplyr::select(-time) %>%
filter(stock == stock_id[1])
pooled_panel_json <- pooled_panel_json %>%
tidyr::nest(features = -c(stock, rt), rt = rt)
View(pooled_panel_json[[2]][[1]])
pooled_panel_json <- pooled_panel %>%
dplyr::select(-time) %>%
filter(stock == stock_id[1])
pooled_panel_json <- pooled_panel_json %>%
tidyr::nest(rt = rt, features = -c(stock, rt))
View(pooled_panel_json[[1]][[1]])
View(pooled_panel_json)
pooled_panel_json <- pooled_panel %>%
dplyr::select(-time) %>%
filter(stock == stock_id[1])
pooled_panel_json <- pooled_panel_json %>%
tidyr::nest(rt = -stock, features = -c(stock, rt))
View(pooled_panel_json)
View(pooled_panel_json[[3]][[1]])
pooled_panel_json <- pooled_panel %>%
filter(stock == stock_id[1])
View(pooled_panel_json)
pooled_panel_json <- pooled_panel_json %>%
tidyr::nest(rt = -stock, features = -c(stock, rt))
View(pooled_panel_json)
View(pooled_panel)
View(pooled_panel_json[[3]][[1]])
pooled_panel_json <- pooled_panel %>%
filter(stock == stock_id[1])
pooled_panel_json %>% stream_out(file("~/GitHub/Evaluation-of-Machine-Learning-in-Asset-Pricing/R/pooled_panel.json"))
pooled_panel_json <- pooled_panel %>%
filter(stock == stock_id[1]) %>%
dplyr::select(-stock)
pooled_panel_json <- pooled_panel_json %>%
tidyr::nest(rt = rt, features = -rt)
View(pooled_panel_json)
View(pooled_panel_json[[1]][[1]])
pooled_panel_json <- pooled_panel_json %>%
tidyr::nest(rt = rt, features = -rt) %>%
chop(rt)
json_test <- stream_in(file("~/GitHub/Evaluation-of-Machine-Learning-in-Asset-Pricing/R/train_new_features.json"))
View(json_test)
json_test[1]
typeof(json_test$target)
typeof(json_test$dynamic_feat)
typeof(json_test[1, $dynamic_feat)
typeof(json_test[1, ]$dynamic_feat)
json_test[1, ]$dynamic_feat
json_test[1, ]$dynamic_feat[1,]
json_test[1, ]$dynamic_feat[,1]
json_test[1, ]$dynamic_feat[[1]]
json_test[1, ]$dynamic_feat[[2]]
pooled_panel_json <- pooled_panel %>%
filter(stock == stock_id[1]) %>%
dplyr::select(-stock)
pooled_panel_json <- pooled_panel_json %>%
chop(c(rt))
?chop
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
install.packages("rlang")
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
install.packages("tidyverse")
install.packages("tidyverse")
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(rlang)
install.packages("rlang")
install.packages("rlang")
install.packages("rlang")
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(jsonlite)
library(foreach)
json_test[1, ]$dynamic_feat
json_test <- stream_in(file("~/GitHub/Evaluation-of-Machine-Learning-in-Asset-Pricing/R/train_new_features.json")) %>%
tibble()
json_test <- stream_in(file("~/GitHub/Evaluation-of-Machine-Learning-in-Asset-Pricing/R/train_new_features.json"))
View(json_test)
json_test <- stream_in(file("~/GitHub/Evaluation-of-Machine-Learning-in-Asset-Pricing/R/train_new_features.json"))
json_test[1, ]$dynamic_feat
json_test <- stream_in(file("~/GitHub/Evaluation-of-Machine-Learning-in-Asset-Pricing/R/train_new_features.json"))
json_test[1, ]$dynamic_feat[[1]] %>%
length
all.equal(file("~/GitHub/Evaluation-of-Machine-Learning-in-Asset-Pricing/R/train_new_features.json"),
stream_out(json_test))
