---
title: "Simulation Models"
author: "Ze Yu Zhong"
date: "19 June 2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

################
##Load Libraries
################

library(glmnet)
library(tidyverse)
library(keras)
library(ggplot2)
library(caret)
library(forecast)
library(rlist)
library(plm)
library(Metrics)
library(ranger)
library(glmnetUtils)

set.seed(27935248)
```

```{r train_valid_test}
#Create Training + Test Sets

#$T = 180$ monthly periods corresponds to 15 years. The training sample was set to start from $T = 108$ or 9 years, a validation set 1 year in length. The last 3 years were reserved as a test set never to be used for validation or training.

#This means that there are only 3 sample periods to train on, yay

#Caret and other packages are not so good at dealing with custom training/validation/test sets. Have to specify resampling, tuning and testing manually (ugh)

customTimeSlices <- function(start, end, initialWindow, horizon, test_size) {
  total <- c(start:end)
  offset <- start - 1
  set_num <- (end - start + offset - test_size - initialWindow) / horizon
  
  time_slice <- list(train = 0, validation = 0, test = 0)
  time_slices <- rep(list(time_slice), 3)

  for (t in 1:set_num) {
    time_slice$train <- c(start:(initialWindow + (t-1) * horizon + offset))
    time_slice$validation <- c((initialWindow + (t-1) * horizon + offset + 1):(initialWindow + (t) * horizon + offset))
    time_slice$test <- c((initialWindow + (t) * horizon + offset + 1):end)
    
    time_slices[[t]] <- time_slice
  }
  return(time_slices)
}

#Create custom time slices
timeSlices <- customTimeSlices(start = 2, end = 181, initialWindow = 108, horizon = 12, test_size = 36)

```

```{r pooled_ols}
#POLS

#Use panel_g1_A1 generated from Simulation.rmd for now. Generalize it later

pooled_panel <- panel_g1_A1

#Expand formula manually this way because PLM is dumb and gets confused by y ~ .
plm_formula <- function(panel){
  #Remove the first 3 colNames, as these correspond to the return, time and stock id
  panel_colnames <- colnames(panel)[-c(1:3)]
  f <- as.formula(c("rt ~", paste(panel_colnames, collapse = "+")))
  return(f)
}

f <- plm_formula(pooled_panel)

POLS_stats <- data.frame(
  set = c(1:3),
  #Training
  train_MAE = c(1:3),
  train_MSE = c(1:3),
  train_RMSE = c(1:3),
  train_R2 = c(1:3),
  #Validation
  valid_MAE = c(1:3),
  valid_MSE = c(1:3),
  valid_RMSE = c(1:3),
  valid_R2 = c(1:3),
  #Test
  test_MAE = c(1:3),
  test_MSE = c(1:3),
  test_RMSE = c(1:3),
  test_R2 = c(1:3)
)

#Initialize Forecasts

POLS_forecasts <- rep(list(0), 3)

#Initialize Models

POLS_models <- rep(list(0), 3)

for (set in 1:3) {
  #Load Training, validation and test sets

  train <- pooled_panel %>%
    filter(time %in% timeSlices[[set]]$train)
  validation <- pooled_panel %>%
    filter(time %in% timeSlices[[set]]$validation)
  test<- pooled_panel %>%
    filter(time %in% timeSlices[[set]]$test)
  
  #Train Model on training set
  
  pols <- plm(f, data = train, model = "pooling", index = c("time", "stock"))
  
  POLS_models[[set]] <- pols
  
  #No Tuning Needed
  
  #Statistics
  
  #Training Set Statistics
  POLS_stats$train_MAE[set] <- mae(train$rt, predict(pols))
  POLS_stats$train_MSE[set] <- mse(train$rt, predict(pols))
  POLS_stats$train_RMSE[set] <- rmse(train$rt, predict(pols))
  error <- (train$rt - predict(pols))
  SSR <- t(error) %*% (error)
  SST <- t(train$rt - mean(train$rt)) %*% (train$rt - mean(train$rt))
  POLS_stats$train_R2[set] <- (1 - SSR/SST)
      
  #Validation Set Statistics
  POLS_stats$valid_MAE[set] <- mae(validation$rt, predict(pols, newdata = validation))
  POLS_stats$valid_MSE[set] <- mse(validation$rt, predict(pols, newdata = validation))
  POLS_stats$valid_RMSE[set] <- rmse(validation$rt, predict(pols, newdata = validation))
      
  error <- (validation$rt - predict(pols, newdata = validation))
  SSR <- t(error) %*% (error)
  SST <- t(validation$rt - mean(validation$rt)) %*% (validation$rt - mean(validation$rt))
  POLS_stats$valid_R2[set] <- (1 - SSR/SST)
    
  #Test Set Statistics
  POLS_stats$test_MAE[set] <- mae(test$rt, predict(pols, newdata = test))
  POLS_stats$test_MSE[set] <- mse(test$rt, predict(pols, newdata = test))
  POLS_stats$test_RMSE[set] <- rmse(test$rt, predict(pols, newdata = test))
      
  error <- (test$rt - predict(pols, newdata = test))
  SSR <- t(error) %*% (error)
  SST <- t(test$rt - mean(test$rt)) %*% (test$rt - mean(test$rt))
  POLS_stats$test_R2[set] <- (1 - SSR/SST)
      
  #Forecasts
  POLS_forecasts[[set]]$forecast <- predict(pols, newdata = test)
  POLS_forecasts[[set]]$forecast_error <- test$rt - predict(pols, newdata = test)
}

POLS_stats

#Just breaking into one of the POLS models
#Seems to be picking up on strange things
summary(POLS_models[[1]])
```

```{r elastic_net}
#Elasticnet

#Can't seem to find anything online that tunes ELN for you with specified validation sets, instead of straight-forward cross validation
#DOuble check with David maybe, on hold for now

#Elasticnet only has alpha and lambda to tune

ELN_grid <- expand.grid(
  alpha = seq(0, 1, 0.01),
  lambda = seq(0, 100, 1)
)

#Initialize Loss Function Statistics

ELN_stats <- data.frame(
  set = c(1:3),
  #Training
  train_MAE = c(1:3),
  train_MSE = c(1:3),
  train_RMSE = c(1:3),
  train_R2 = c(1:3),
  #Validation
  valid_MAE = c(1:3),
  valid_MSE = c(1:3),
  valid_RMSE = c(1:3),
  valid_R2 = c(1:3),
  #Test
  test_MAE = c(1:3),
  test_MSE = c(1:3),
  test_RMSE = c(1:3),
  test_R2 = c(1:3)
)

#Initialize Forecasts

ELN_forecasts <- rep(list(0), 3)

#Initialize Models

ELN_models <- rep(list(0), 3)

####
ELN_tune_list <- rep(list(0), nrow(ELN_grid))

#Set up a tuning list with the tuning hyperparameters, training loss and validation loss
#Unfortunately the grid part of this is strictly non-numeric (it's a 2 element vector in this case), and so we'll have to use lists instead of arrays

for (i in 1:nrow(ELN_grid)) {
  ELN_tune_list[[i]] <- list(ELN_grid = ELN_grid[i,], )
}

for (set in 1:3) {
  #Load training, validation and test sets
    train <- pooled_panel %>%
      filter(time %in% unlist(time_slices$train[set]))
    train_x <- train %>%
      select(-return, -time, -stock)
    train_y <- train %>%
      select(return)
    
    validation <- pooled_panel %>%
      filter(time %in% unlist(time_slices$validation[set]))
    validation_x <- validation %>%
      select(-return, -time, -stock)
    validation_y <- validation %>%
      select(return)
    
    test <- pooled_panel %>%
      filter(time %in% unlist(time_slices$test[set]))
    test_x <- test %>%
      select(-return, -time, -stock)
    test_y <- test %>%
      select(return)
    
  #Initialize forecasts
    elasticnet_forecasts[[set]] <- data.frame(
      time = test$time,
      forecast = rep(0, length(test$time)),
      forecast_error = rep(0, length(test$time))
    )

  #Train model on training set
  
    elasticnet <- glmnet(x = train_x, y = train_y)
    POLS_models[[set]] <- pols
    
  #Usually Tuning via Validation set would be here, but POLS does not need tuning
  
  #Training Set Statistics
    POLS_stats$train_MAE[set] <- mae(train$return, predict(pols))
    POLS_stats$train_MSE[set] <- mse(train$return, predict(pols))
    POLS_stats$train_RMSE[set] <- rmse(train$return, predict(pols))
    error <- (train$return - predict(pols))
    SSR <- t(error) %*% (error)
    SST <- t(train$return - mean(train$return)) %*% (train$return - mean(train$return))
    POLS_stats$train_R2[set] <- (1 - SSR/SST)
    
  #Validation Set Statistics
    POLS_stats$valid_MAE[set] <- mae(validation$return, predict(pols, newdata = validation))
    POLS_stats$valid_MSE[set] <- mse(validation$return, predict(pols, newdata = validation))
    POLS_stats$valid_RMSE[set] <- rmse(validation$return, predict(pols, newdata = validation))
    
    error <- (validation$return - predict(pols, newdata = validation))
    SSR <- t(error) %*% (error)
    SST <- t(validation$return - mean(validation$return)) %*% (validation$return - mean(validation$return))
    POLS_stats$valid_R2[set] <- (1 - SSR/SST)
  
  #Test Set Statistics
    POLS_stats$test_MAE[set] <- mae(test$return, predict(pols, newdata = test))
    POLS_stats$test_MSE[set] <- mse(test$return, predict(pols, newdata = test))
    POLS_stats$test_RMSE[set] <- rmse(test$return, predict(pols, newdata = test))
    
    error <- (test$return - predict(pols, newdata = test))
    SSR <- t(error) %*% (error)
    SST <- t(test$return - mean(test$return)) %*% (test$return - mean(test$return))
    POLS_stats$test_R2[set] <- (1 - SSR/SST)
    
  #Forecasts
    POLS_forecasts[[set]]$forecast <- predict(pols, newdata = test)
    POLS_forecasts[[set]]$forecast_error <- test$return - predict(pols, newdata = test)
}

```

```{r random_forest}
#Random Forest

#Random Forest should also be fairly straightforward, only two tuning parameters mtry and ntree (usually)

#The vanilla randomForest package is very basic and too slow
#We will use the ranger package, which is apparently a C++ implementation of the random forest algorithm and should be faster
#Another alternative is to use XGBoost (which is very very fast) and set boosting rounds = 1, because in this case it should be a standard random forest

#Compute power is a real struggle, and the aim is just to get the code basis working for the moment. Very conservative grids are therefore used for now

#Default ntree = 500
#Default mtry = count(predictors/3) = 400/3 = 133

rf_grid <- expand.grid(
  ntree = seq(500, 1000, 1000),
  mtry = seq(10, 100, 10)
)

#Generate formula, actually same code from PLM earlier

rf_formula <- function(panel){
  #Remove the first 3 colNames, as these correspond to the return, time and stock id
  panel_colnames <- colnames(panel)[-c(1:3)]
  f <- as.formula(c("rt ~", paste(panel_colnames, collapse = "+")))
  return(f)
}

f <- rf_formula(pooled_panel)

#Initialize Loss Function Statistics

RF_stats <- data.frame(
  set = c(1:3),
  #Training
  train_MAE = c(1:3),
  train_MSE = c(1:3),
  train_RMSE = c(1:3),
  train_R2 = c(1:3),
  #Validation
  valid_MAE = c(1:3),
  valid_MSE = c(1:3),
  valid_RMSE = c(1:3),
  valid_R2 = c(1:3),
  #Test
  test_MAE = c(1:3),
  test_MSE = c(1:3),
  test_RMSE = c(1:3),
  test_R2 = c(1:3)
)

#Initialize Forecasts

RF_forecasts <- rep(list(0), 3)

#Initialize Models

RF_models <- rep(list(0), 3)

#Load training, validation and test sets

train <- pooled_panel %>%
  filter(time %in% timeSlices[[set]]$train)
validation <- pooled_panel %>%
  filter(time %in% timeSlices[[set]]$validation)
test<- pooled_panel %>%
  filter(time %in% timeSlices[[set]]$test)

#Train Model on Training Set

rf <- ranger(f, data = train, num.trees = rf_grid$ntree[1], mtry = rf_grid$mtry[1],
             write.forest = TRUE, min.node.size = 5, splitrule = "variance")

RF_models[[1]] <- 


for (set in 1:3) {
  
  #Training Set Statistics
    RF_stats$train_MAE[set] <- mae(train$return, predict(rf))
    RF_stats$train_MSE[set] <- mse(train$return, predict(rf))
    RF_stats$train_RMSE[set] <- rmse(train$return, predict(rf))
    #R squared still pending, haven't figured out how to do it yet fully
    
  #Validation Set Statistics
    RF_stats$valid_MAE[set] <- mae(validation$return, predict(rf, newdata = validation))
    RF_stats$valid_MSE[set] <- mse(validation$return, predict(rf, newdata = validation))
    RF_stats$valid_RMSE[set] <- rmse(validation$return, predict(rf, newdata = validation))
    #R squared still pending, haven't figured out how to do it yet fully
  
  #Test Set Statistics
    RF_stats$test_MAE[set] <- mae(test$return, predict(rf, newdata = test))
    RF_stats$test_MSE[set] <- mse(test$return, predict(rf, newdata = test))
    RF_stats$test_RMSE[set] <- rmse(test$return, predict(rf, newdata = test))
    #R squared still pending, haven't figured out how to do it yet fully
    
  #Forecasts
    RF_forecasts[[set]]$forecast <- predict(rf, newdata = test)
    RF_forecasts[[set]]$forecast_error <- test$return - predict(rf, newdata = test)
    
}

```

```{r}
#Train RF Model + tune
rf <- ranger(f, data = train, 
             #Hyperparameters
             
             #Tuning parameters
              num.trees = 10,
              mtry = 10,
             
             #No Tuning
              #Default Min node size
              min.node.size = 5,
             
             #Other
             write.forest = TRUE,
             importance = "impurity"
             )

importance(rf)

```


```{r neural_networks}
#Neural Network

```

