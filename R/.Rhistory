hyperparameters = 0,
variable_importance = 0)
#Model
model <- ELN_model_grid[[best_model_params$list_index]]$model
ELN_stats[[set]]$model <- model
#Hyperparameters
ELN_stats[[set]]$hyperparameters <- best_model_params
#Loss Stats Dataframe
#Train
train_predict <- predict(model, train_x, alpha = best_model_params$alpha, lambda = best_model_params$lambda)
ELN_stats[[set]]$loss_stats$train_MAE <- mae(train_y, train_predict)
ELN_stats[[set]]$loss_stats$train_MSE <- mse(train_y, train_predict)
ELN_stats[[set]]$loss_stats$train_RMSE <- rmse(train_y, train_predict)
ELN_stats[[set]]$loss_stats$train_RSquare <- (1 - sse(train_y, train_predict) / sum((train_y - mean(train_y))^2))
#Validation
valid_predict <- predict(model, validation_x, alpha = best_model_params$alpha, lambda = best_model_params$lambda)
ELN_stats[[set]]$loss_stats$validation_MAE <- mae(validation_y, valid_predict)
ELN_stats[[set]]$loss_stats$validation_MSE <- mse(validation_y, valid_predict)
ELN_stats[[set]]$loss_stats$validation_RMSE <- rmse(validation_y, valid_predict)
ELN_stats[[set]]$loss_stats$validation_RSquare <- (1 - sse(validation_y, valid_predict) / sum((validation_y - mean(validation_y))^2))
#Test
test_predict <- predict(model, test_x, alpha = best_model_params$alpha, lambda = best_model_params$lambda)
ELN_stats[[set]]$loss_stats$test_MAE <- mae(test_y, test_predict)
ELN_stats[[set]]$loss_stats$test_MSE <- mse(test_y, test_predict)
ELN_stats[[set]]$loss_stats$test_RMSE <- rmse(test_y, test_predict)
ELN_stats[[set]]$loss_stats$test_RSquare <- (1 - sse(test_y, test_predict) / sum((test_y - mean(test_y))^2))
#Forecasts
ELN_stats[[set]]$forecasts <- predict(model, test_x, alpha = best_model_params$alpha, lambda = best_model_params$lambda)
#Forecast residuals
ELN_stats[[set]]$forecast_resids <- test_y - predict(model, test_x, alpha = best_model_params$alpha, lambda = best_model_params$lambda)
#Variable Importance
ELN_stats[[set]]$variable_importance <- ELN_variable_importance(test, model, alpha = best_model_params$alpha, lambda = best_model_params$lambda)
}
return(ELN_stats)
}
#Testing if function works
# ELN_stats_mae <- ELN_fit_stats(alpha_grid, nlamb = 100, timeSlices, pooled_panel, loss_function = "mae")
#
# ELN_stats_mse <- ELN_fit_stats(alpha_grid, nlamb = 100, timeSlices, pooled_panel, loss_function = "mse")
#
# ELN_stats_mse[[1]]$loss_stats
# ELN_stats_mse[[1]]$model$alpha
#
# ELN_stats_mse[[2]]$loss_stats
# ELN_stats_mse[[2]]$model$alpha
#
# ELN_stats_mse[[3]]$loss_stats
# ELN_stats_mse[[3]]$model$alpha
#
# ELN_stats_mse[[1]]$variable_importance %>% arrange(desc(importance))
# ELN_stats_mse[[2]]$variable_importance %>% arrange(desc(importance))
# ELN_stats_mse[[3]]$variable_importance %>% arrange(desc(importance))
#
# ELN_stats_mae[[1]]$loss_stats
# ELN_stats_mae[[1]]$model$alpha
#
# ELN_stats_mae[[2]]$loss_stats
# ELN_stats_mae[[2]]$model$alpha
#
# ELN_stats_mae[[3]]$loss_stats
# ELN_stats_mae[[3]]$model$alpha
#
# ELN_stats_mae[[1]]$variable_importance %>% arrange(desc(importance))
# ELN_stats_mae[[2]]$variable_importance %>% arrange(desc(importance))
# ELN_stats_mae[[3]]$variable_importance %>% arrange(desc(importance))
# Notes
# Mostly consistent results with the previous implementation, very minor differences in optimal alpha values chosen
# Similarly, optimal alpha values are highly unstable
# Does not seem to particularly different between MAE and MSE versions
# Moving from A1 to A2
# LM struggles greatly with more complex A matrix specifications
# However, ELN actually does quite well in spite of this
# It actuallyhas higher R squared values than the true r squared, presumbaly indicating that it is somehow capturing the SV error structure
# In general, it seems that ELN is choosing very parsimonious representations (look at variable importance metrics that are at 0)
NNet_fit_stats <- function(pooled_panel, timeSlices, hidden_layers, loss_function, batch_size, patience) {
#Initialize
NNet_stats <- rep(list(0), 3)
for (set in 1:3) {
NNet_stats[[set]] <- list(loss_stats = data.frame(train_MAE = 0, train_MSE = 0, train_RMSE = 0, train_RSquare = 0,
validation_MAE = 0, validation_MSE = 0, validation_RMSE = 0, validation_RSquare = 0,
test_MAE = 0, test_MSE = 0, test_RMSE = 0, test_RSquare = 0),
#Other useful things
forecasts = 0,
forecast_resids = 0,
model = 0,
variable_importance = 0)
#Load Training, validation and test sets
train <- pooled_panel %>%
filter(time %in% timeSlices[[set]]$train)
validation <- pooled_panel %>%
filter(time %in% timeSlices[[set]]$validation)
test<- pooled_panel %>%
filter(time %in% timeSlices[[set]]$test)
train_x <- train[4:ncol(train)]
train_y <- train$rt
validation_x <- validation[4:ncol(validation)]
validation_y <- validation$rt
test_x <- test[4:ncol(test)]
test_y <- test$rt
# Fit the model
# The patience parameter is the amount of epochs to check for improvement.
# Gu et al don't say what their early stopping parameter p is
early_stop <- callback_early_stopping(monitor = "val_loss", patience = patience, restore_best_weights = TRUE)
print_dot_callback <- callback_lambda(
on_epoch_end = function(epoch, logs) {
if (epoch %% 50 == 0) cat("\n")
cat(".")
}
)
l1_penalty <- 0.1
build_NN <- function(hidden_layers, loss_function) {
if (hidden_layers == 1) {
model <- keras_model_sequential() %>%
# Layer 1
layer_dense(units = 32, input_shape = ncol(train_x)) %>%
layer_batch_normalization() %>%
layer_activation("relu") %>%
layer_activity_regularization(l1 = l1_penalty) %>%
layer_batch_normalization() %>%
# Output Layer
layer_dense(units = 1, activation = "linear")
} else if (hidden_layers == 2) {
model <- keras_model_sequential() %>%
# Layer 1
layer_dense(units = 32, input_shape = ncol(train_x)) %>%
layer_batch_normalization() %>%
layer_activation("relu") %>%
layer_activity_regularization(l1 = l1_penalty) %>%
layer_batch_normalization() %>%
# Layer 2
layer_dense(units = 16) %>%
layer_activation("relu") %>%
layer_activity_regularization(l1 = l1_penalty) %>%
layer_batch_normalization() %>%
# Output Layer
layer_dense(units = 1, activation = "linear")
} else if (hidden_layers == 3) {
model <- keras_model_sequential() %>%
# Layer 1
layer_dense(units = 32, input_shape = ncol(train_x)) %>%
layer_batch_normalization() %>%
layer_activation("relu") %>%
layer_activity_regularization(l1 = l1_penalty) %>%
layer_batch_normalization() %>%
# Layer 2
layer_dense(units = 16) %>%
layer_activation("relu") %>%
layer_activity_regularization(l1 = l1_penalty) %>%
layer_batch_normalization() %>%
#Layer 3
layer_dense(units = 8) %>%
layer_activation("relu") %>%
layer_activity_regularization(l1 = l1_penalty) %>%
layer_batch_normalization() %>%
# Output Layer
layer_dense(units = 1, activation = "linear")
} else if (hidden_layers == 4) {
model <- keras_model_sequential() %>%
# Layer 1
layer_dense(units = 32, input_shape = ncol(train_x)) %>%
layer_batch_normalization() %>%
layer_activation("relu") %>%
layer_activity_regularization(l1 = l1_penalty) %>%
layer_batch_normalization() %>%
# Layer 2
layer_dense(units = 16) %>%
layer_activation("relu") %>%
layer_activity_regularization(l1 = l1_penalty) %>%
layer_batch_normalization() %>%
#Layer 3
layer_dense(units = 8) %>%
layer_activation("relu") %>%
layer_activity_regularization(l1 = l1_penalty) %>%
layer_batch_normalization() %>%
#Layer 4
layer_dense(units = 4) %>%
layer_activation("relu") %>%
layer_activity_regularization(l1 = l1_penalty) %>%
layer_batch_normalization() %>%
# Output Layer
layer_dense(units = 1, activation = "linear")
} else {
model <- keras_model_sequential() %>%
# Layer 1
layer_dense(units = 32, input_shape = ncol(train_x)) %>%
layer_batch_normalization() %>%
layer_activation("relu") %>%
layer_activity_regularization(l1 = l1_penalty) %>%
layer_batch_normalization() %>%
# Layer 2
layer_dense(units = 16) %>%
layer_activation("relu") %>%
layer_activity_regularization(l1 = l1_penalty) %>%
layer_batch_normalization() %>%
#Layer 3
layer_dense(units = 8) %>%
layer_activation("relu") %>%
layer_activity_regularization(l1 = l1_penalty) %>%
layer_batch_normalization() %>%
#Layer 4
layer_dense(units = 4) %>%
layer_activation("relu") %>%
layer_activity_regularization(l1 = l1_penalty) %>%
layer_batch_normalization() %>%
#Layer 5
layer_dense(units = 2) %>%
layer_activation("relu") %>%
layer_activity_regularization(l1 = l1_penalty) %>%
layer_batch_normalization() %>%
# Output Layer
layer_dense(units = 1, activation = "linear")
}
model %>% compile(
loss = loss_function,
optimizer = "adam",
metrics = list("mae", "mse")
)
model
}
neural_network <- build_NN(hidden_layers, loss_function)
# Other options used throughout the neural network fitting process are specified here
# Namely, batch size
# In general, larger batch sizes result in faster progress in training, but don't always converge as fast. Smaller batch sizes train slower, but can converge faster.
# Default batch size is 32
neural_network %>% fit(as.matrix(train_x), as.matrix(train_y),
batch_size = batch_size, epochs = 500, verbose = 1,
validation_data = list(as.matrix(validation_x), as.matrix(validation_y)),
callbacks = list(early_stop, print_dot_callback))
#Model
NNet_stats[[set]]$model <- neural_network
#Train
train_predict <- neural_network %>% predict(as.matrix(train_x))
NNet_stats[[set]]$loss_stats$train_MAE <- mae(train$rt, train_predict)
NNet_stats[[set]]$loss_stats$train_MSE <- mse(train$rt, train_predict)
NNet_stats[[set]]$loss_stats$train_RMSE <- rmse(train$rt, train_predict)
NNet_stats[[set]]$loss_stats$train_RSquare <- R2(train_predict, train$rt, form = "traditional")
#Validation
validation_predict <- neural_network %>% predict(as.matrix(validation_x))
NNet_stats[[set]]$loss_stats$validation_MAE <- mae(validation$rt, validation_predict)
NNet_stats[[set]]$loss_stats$validation_MSE <- mse(validation$rt, validation_predict)
NNet_stats[[set]]$loss_stats$validation_RMSE <- rmse(validation$rt, validation_predict)
NNet_stats[[set]]$loss_stats$validation_RSquare <- R2(validation_predict, validation$rt, form = "traditional")
#Test
test_predict <- neural_network %>% predict(as.matrix(test_x))
NNet_stats[[set]]$loss_stats$test_MAE <- mae(test$rt, test_predict)
NNet_stats[[set]]$loss_stats$test_MSE <- mse(test$rt, test_predict)
NNet_stats[[set]]$loss_stats$test_RMSE <- rmse(test$rt, test_predict)
NNet_stats[[set]]$loss_stats$test_RSquare <- R2(test_predict, test$rt, form = "traditional")
#Forecasts
NNet_stats[[set]]$forecasts <- test_predict
#Forecast residuals
NNet_stats[[set]]$forecast_resids <- test$rt - test_predict
#Variable Importance
NNet_stats[[set]]$variable_importance <- NNet_variable_importance(test, neural_network)
}
NNet_stats
}
####################################################
# Load RAW Welch Goyal Data
####################################################
# Note that Gu et al only used:
# Dividend price ratio dp, earnings price ratio ep, book to market ratio bm,
# net equity expansion ntis, Treasury bill rate tbl, term spread tms,
# Default spread dfy, stock variance svar
PredictorData2017_RAW <- read_excel("data/factors/PredictorData2017.xlsx",
# Specify quarterly as we are now working with quarterly data
sheet = "Quarterly",
na = "NaN")
PredictorData2017 <- PredictorData2017_RAW %>%
mutate(time = as.yearqtr(as.character(yyyyq), "%Y%q")) %>%
select(-yyyyq) %>%
# Dividend price ratio is the difference between the log of dividends and the log of prices
# Note that log(D12) - log(Index) is negative which doesn't make sense as a ratio
mutate(dp = abs(log(D12) - log(Index))) %>%
# earnings price ratio is the difference between the log of earnings and the log of prices
# Note that log(E12) - log(Index) is negative which doesn't make sense as a ratio
mutate(ep = abs(log(E12) - log(Index))) %>%
# Term Spread is the difference between long term yield on gov bonds and treasury bills
mutate(tms = lty - tbl) %>%
# Default spread is the difference between BAA and AAA-rated corporate bond yields
mutate(dfy = abs(BAA - AAA)) %>%
# Rename b/m to bm
rename(bm = `b/m`) %>%
# Reorder
select(time, everything())
# Subset so that we have the predictors we actually care about
# Note that some of these by default have the same names as those in the individual factor set
# Therefore, prefix them with macro_ to make it clearer
# Also filter out all the entries that are not in the datashare dataset as they can't be used (Note that this also takes care of all missing values in this dataset)
# Filter it so that we only have the years we are interested in (same filtering as datashare)
macro_predictors <- PredictorData2017 %>%
select(time, dp, ep, bm, ntis, tbl, tms, dfy, svar) %>%
rename_at(vars(-time), function(x) paste0("macro_", x)) %>%
# Create constant
mutate(macro_constant = 1) %>%
# Reorder
select(time, macro_constant, everything()) %>%
filter(time >= 1993.5)
macro_factor_names <- colnames(select(macro_predictors, -time))
individual_factor_names <- colnames(select(datashare_imputed, -permno, -time, -RET_M, -PRC))
interaction_terms <- rep(list(0), length(macro_factor_names))
for (i in 1:length(macro_factor_names)) {
interaction_terms[[i]] <- paste(macro_factor_names[i], individual_factor_names, sep = ":", collapse = " + ")
}
f <- as.formula(c("~ ", paste(interaction_terms, collapse = "+")))
combined <- full_join(macro_predictors, datashare, by = "time")
dmy <- dummyVars(f, data = combined)
# This is pretty damn large at ~1.5GB. Interaction terms are brutal
final_dataset_x <- (predict(dmy, combined))
final_dataset <- data.frame(combined$permno, combined$time, combined$RET_M, final_dataset_x) %>%
# Rename some columns
mutate(permno = combined.permno) %>%
mutate(time = combined.time) %>%
mutate(rt = combined.RET_M) %>%
select(-combined.permno, -combined.time, -combined.RET_M) %>%
select(time, permno, rt, everything())
rm(final_dataset_x)
# Double Check
colnames(final_dataset)
## time slices function
## Different function needed due to different time format (inclusion of quarters)
## Makes it so that it works with all of our model fitting function from earlier
realdata_custom_timeslices <- function(start, initialWindow, horizon, validation_size, test_size, set_no) {
time_slice <- list(train = 0, validation = 0, test = 0)
time_slices <- rep(list(time_slice), set_no)
for (t in 1:set_no) {
time_slice$train <- seq(start,
(start + initialWindow + (t-1) * horizon + 3/4),
0.25)
time_slice$validation <- seq(
(start + initialWindow + (t-1) * horizon + 1),
(start + initialWindow + (t-1) * horizon + validation_size + 3/4),
0.25
)
time_slice$test <- seq(
(start + initialWindow + (t-1) * horizon + validation_size + 1),
(start + (initialWindow + (t-1) * horizon) + validation_size + test_size + 3/4),
0.25
)
time_slices[[t]] <- time_slice
}
time_slices
}
# These are the settings working with a dataset going from 1993 - 2016
realdata_timeSlices <- realdata_custom_timeslices(start = 1993, initialWindow = 15, horizon = 1, validation_size = 3, test_size = 3, set_no = 3)
real_panel_formula <- function(panel){
#Remove the first 3 colNames, as these correspond to the return, time and stock id
panel_colnames <- colnames(panel)[-c(1:3)]
f <- as.formula(c("rt ~", paste(panel_colnames, collapse = "+")))
return(f)
}
## Function to fit all models for real data
## Slightly different from simulation model fitting function
fit_all_models_real_data <- function(pooled_panel,
# Logical arguments specifying which models you want to fit
# This is useful if you don't want to fit some of the most intensive methods such as RF and Neural Networks
LM, ELN, RF, NNet) {
# Initialize List
real_data_results_list <- list(0)
real_data_results_list <- list(
# Panel Statistics
pooled_panel_stats = 0,
# Y Values (used for Diebold Mariano Tests later)
returns = 0,
# Models
LM_MSE = 0, LM_MAE = 0,
ELN_MSE = 0, ELN_MAE = 0,
RF_MSE = 0, RF_MAE = 0,
# Neural Networks
NN1_MSE = 0, NN1_MAE = 0,
NN2_MSE = 0, NN2_MAE = 0,
NN3_MSE = 0, NN3_MAE = 0,
NN4_MSE = 0, NN4_MAE = 0,
NN5_MSE = 0, NN5_MAE = 0
)
# Load pooled_panel
real_data_results_list$returns <- pooled_panel$RET_M
realdata_timeSlices <- realdata_custom_timeslices(start = 1993, initialWindow = 15, horizon = 1, validation_size = 3, test_size = 3, set_no = 3)
f <- real_panel_formula(pooled_panel)
if (LM == 1) {
# Linear Models
real_data_results_list$LM_MSE <- LM_fit(pooled_panel, realdata_timeSlices, "mse", f)
real_data_results_list$LM_MAE <- LM_fit(pooled_panel, realdata_timeSlices, "mae", f)
}
if (ELN == 1) {
# Penalized Linear Models
alpha_grid <- seq(0, 1, 0.01)
real_data_results_list$ELN_MAE <- ELN_fit_stats(alpha_grid, nlamb = 100, realdata_timeSlices, pooled_panel, loss_function = "mae")
real_data_results_list$ELN_MSE <- ELN_fit_stats(alpha_grid, nlamb = 100, realdata_timeSlices, pooled_panel, loss_function = "mse")
}
if (RF == 1) {
# Random Forests
RF_grid <- expand.grid(
#ntree usually isn't tuned. Just set to max of computationally feasible
ntree = 100,
mtry = seq(10, round(ncol(pooled_panel[4:ncol(pooled_panel)])/3), 20)
# nodesize = seq(2, 14, 2)
# nodedepth recommended not to be changed
#nodedepth = 1
)
real_data_results_list$RF_MSE <- RF_fit_stats(pooled_panel, RF_grid, realdata_timeSlices, "mse")
real_data_results_list$RF_MAE <- RF_fit_stats(pooled_panel, RF_grid, realdata_timeSlices, "mae")
}
if (NNet == 1) {
# Neural Networks
# Commented for now because honours lab computers don't have keras/tensorflow
batch_size <- 32
patience <- 20
real_data_results_list$NN1_MSE <- NNet_fit_stats(pooled_panel, realdata_timeSlices, 1, "mse", batch_size = batch_size, patience = patience)
real_data_results_list$NN1_MAE <- NNet_fit_stats(pooled_panel, realdata_timeSlices, 1, "mae", batch_size = batch_size, patience = patience)
real_data_results_list$NN2_MSE <- NNet_fit_stats(pooled_panel, realdata_timeSlices, 2, "mse", batch_size = batch_size, patience = patience)
real_data_results_list$NN2_MAE <- NNet_fit_stats(pooled_panel, realdata_timeSlices, 2, "mae", batch_size = batch_size, patience = patience)
real_data_results_list$NN3_MSE <- NNet_fit_stats(pooled_panel, realdata_timeSlices, 3, "mse", batch_size = batch_size, patience = patience)
real_data_results_list$NN3_MAE <- NNet_fit_stats(pooled_panel, realdata_timeSlices, 3, "mae", batch_size = batch_size, patience = patience)
real_data_results_list$NN4_MSE <- NNet_fit_stats(pooled_panel, realdata_timeSlices, 4, "mse", batch_size = batch_size, patience = patience)
real_data_results_list$NN4_MAE <- NNet_fit_stats(pooled_panel, realdata_timeSlices, 4, "mae", batch_size = batch_size, patience = patience)
real_data_results_list$NN5_MSE <- NNet_fit_stats(pooled_panel, realdata_timeSlices, 5, "mse", batch_size = batch_size, patience = patience)
real_data_results_list$NN5_MAE <- NNet_fit_stats(pooled_panel, realdata_timeSlices, 5, "mae", batch_size = batch_size, patience = patience)
}
real_data_results_list
}
save.image("~/GitHub/Evaluation-of-Machine-Learning-in-Asset-Pricing/R/.RData")
# Test on fitting actual data
# Lots of memory issues, need to change fit functions so that they are more memory efficient
options(future.globals.maxSize = 1e+9)
# test
NNet_1_mse_stats <- NNet_fit_stats(final_dataset, realdata_timeSlices, 1, "mse", batch_size = 32, 10)
# test
LM_stats_mse <- LM_fit(final_dataset, realdata_timeSlices, "mse")
# Test on fitting actual data
# Lots of memory issues, need to change fit functions so that they are more memory efficient
options(future.globals.maxSize = 1.5e+9)
real_data_results <- fit_all_models_real_data(final_dataset,
LM = 1, ELN = 0, RF = 0, NNet = 0)
LM_fit <- function(pooled_panel, timeSlices, loss_function, f) {
#Initialize Loss Function Statistics
LM_stats <- rep(list(0), 3)
for (set in 1:3) {
LM_stats[[set]] <- list(loss_stats = data.frame(train_MAE = 0, train_MSE = 0, train_RMSE = 0, train_RSquare = 0,
validation_MAE = 0, validation_MSE = 0, validation_RMSE = 0, validation_RSquare = 0,
test_MAE = 0, test_MSE = 0, test_RMSE = 0, test_RSquare = 0),
#Other useful things
forecasts = 0,
forecast_resids = 0,
model = 0,
#Variable Importance
variable_importance = 0)
#Load Training, validation and test sets
train <- pooled_panel %>%
filter(time %in% timeSlices[[set]]$train)
validation <- pooled_panel %>%
filter(time %in% timeSlices[[set]]$validation)
test <- pooled_panel %>%
filter(time %in% timeSlices[[set]]$test)
#Train Model on training set
# Set model = FALSE so that it won't save a copy of the training data in the model object
# Doen for memory efficiency
#MSE case
if (loss_function == "mse") {
lm <- lm(f, data = train, model = FALSE, y = FALSE)
} else {
# Use pfn as method here for much faster computation
lm <- rq(f, data = train, tau = 0.5, method = "pfn")
}
LM_stats[[set]]$model <- lm
#No Tuning Needed
#Statistics
#Training Set
train_predict <- predict(lm, newdata = train)
LM_stats[[set]]$loss_stats$train_MAE <- mae(train$rt, train_predict)
LM_stats[[set]]$loss_stats$train_MSE <- mse(train$rt, train_predict)
LM_stats[[set]]$loss_stats$train_RMSE <- rmse(train$rt, train_predict)
LM_stats[[set]]$loss_stats$train_RSquare <- R2(train_predict, train$rt, form = "traditional")
#Validation Set Statistics
validation_predict <- predict(lm, newdata = validation)
LM_stats[[set]]$loss_stats$validation_MAE <- mae(validation$rt, validation_predict)
LM_stats[[set]]$loss_stats$validation_MSE <- mse(validation$rt, validation_predict)
LM_stats[[set]]$loss_stats$validation_RMSE <- rmse(validation$rt, validation_predict)
LM_stats[[set]]$loss_stats$validation_RSquare <- R2(validation_predict, validation$rt, form = "traditional")
#Test Set Statistics
test_predict <- predict(lm, newdata = test)
LM_stats[[set]]$loss_stats$test_MAE <- mae(test$rt, test_predict)
LM_stats[[set]]$loss_stats$test_MSE <- mse(test$rt, test_predict)
LM_stats[[set]]$loss_stats$test_RMSE <- rmse(test$rt, test_predict)
LM_stats[[set]]$loss_stats$test_RSquare <- R2(test_predict, test$rt, form = "traditional")
#Forecasts
LM_stats[[set]]$forecasts <- test_predict
#Forecast Residuals
LM_stats[[set]]$forecast_resids <- test$rt - test_predict
#Variable Importance
LM_stats[[set]]$variable_importance <- LM_variable_importance(test, lm)
}
return(LM_stats)
}
# test
LM_stats_mse <- LM_fit(final_dataset, realdata_timeSlices, "mse")
real_panel_formula <- function(panel){
#Remove the first 3 colNames, as these correspond to the return, time and stock id
panel_colnames <- colnames(panel)[-c(1:3)]
f <- as.formula(c("rt ~", paste(panel_colnames, collapse = "+")))
return(f)
}
f <- real_panel_formula(pooled_panel)
f <- real_panel_formula(final_dataset)
# test
LM_stats_mse <- LM_fit(final_dataset, realdata_timeSlices, "mse")
# test
LM_stats_mse <- LM_fit(final_dataset, realdata_timeSlices, "mse", f)
LM_stats_mse[[1]]$loss_stats
LM_stats_mse[[1]]$forecasts
LM_stats_mse[[1]]$variable_importance
g1_A1_panel[[1]]$panel
